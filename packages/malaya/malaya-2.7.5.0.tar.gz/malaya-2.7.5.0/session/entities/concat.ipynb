{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train_X', 'test_X', 'train_Y', 'test_Y'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('session-entities.pkl', 'rb') as fopen:\n",
    "    data = pickle.load(fopen)\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = data['train_X']\n",
    "test_X = data['test_X']\n",
    "train_Y = data['train_Y']\n",
    "test_Y = data['test_Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['word2idx', 'idx2word', 'tag2idx', 'idx2tag', 'char2idx'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('dictionary-entities.json') as fopen:\n",
    "    dictionary = json.load(fopen)\n",
    "dictionary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = dictionary['word2idx']\n",
    "idx2word = {int(k): v for k, v in dictionary['idx2word'].items()}\n",
    "tag2idx = dictionary['tag2idx']\n",
    "idx2tag = {int(k): v for k, v in dictionary['idx2tag'].items()}\n",
    "char2idx = dictionary['char2idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('harahap', 'person'),\n",
       " (',', 'OTHER'),\n",
       " ('pedagang', 'OTHER'),\n",
       " ('ikan', 'OTHER'),\n",
       " ('kering', 'OTHER'),\n",
       " (',', 'OTHER'),\n",
       " ('kering', 'OTHER'),\n",
       " (',', 'OTHER'),\n",
       " ('para', 'OTHER'),\n",
       " ('pembeli', 'OTHER'),\n",
       " ('mayoritas', 'OTHER'),\n",
       " ('datang', 'OTHER'),\n",
       " ('dari', 'OTHER'),\n",
       " ('luar', 'OTHER'),\n",
       " ('kota', 'OTHER'),\n",
       " ('seperti', 'OTHER'),\n",
       " ('meranti', 'location'),\n",
       " (',', 'OTHER'),\n",
       " ('riau', 'location'),\n",
       " (',', 'OTHER'),\n",
       " ('jambi', 'location'),\n",
       " (',', 'OTHER'),\n",
       " ('dan', 'OTHER'),\n",
       " ('batam', 'location'),\n",
       " ('.', 'OTHER'),\n",
       " ('cabai', 'OTHER'),\n",
       " ('kering', 'OTHER'),\n",
       " (',', 'OTHER'),\n",
       " ('para', 'OTHER'),\n",
       " ('pembeli', 'OTHER'),\n",
       " ('mayoritas', 'OTHER'),\n",
       " ('datang', 'OTHER'),\n",
       " ('dari', 'OTHER'),\n",
       " ('luar', 'OTHER'),\n",
       " ('kota', 'OTHER'),\n",
       " ('seperti', 'OTHER'),\n",
       " ('chuah', 'location'),\n",
       " (',', 'OTHER'),\n",
       " ('riau', 'location'),\n",
       " (',', 'OTHER'),\n",
       " ('jambi', 'location'),\n",
       " (',', 'OTHER'),\n",
       " ('dan', 'OTHER'),\n",
       " ('batam', 'location'),\n",
       " ('.', 'OTHER'),\n",
       " ('cabai', 'OTHER'),\n",
       " ('para', 'OTHER'),\n",
       " ('pembeli', 'OTHER'),\n",
       " ('mayoritas', 'OTHER'),\n",
       " ('datang', 'OTHER')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip([idx2word[d] for d in train_X[-1]], [idx2tag[d] for d in train_Y[-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('politik', 'OTHER'),\n",
       " ('dari', 'OTHER'),\n",
       " ('Universitas', 'organization'),\n",
       " ('Gadjah', 'organization'),\n",
       " ('Mada', 'organization'),\n",
       " (',', 'OTHER'),\n",
       " ('Arie', 'person'),\n",
       " ('Sudjito', 'person'),\n",
       " (',', 'OTHER'),\n",
       " ('menilai,', 'OTHER'),\n",
       " ('keinginan', 'OTHER'),\n",
       " ('Ketua', 'OTHER'),\n",
       " ('Umum', 'OTHER'),\n",
       " ('Partai', 'organization'),\n",
       " ('Golkar', 'organization'),\n",
       " ('Aburizal', 'person'),\n",
       " ('Bakrie', 'person'),\n",
       " ('untuk', 'OTHER'),\n",
       " ('maju', 'OTHER'),\n",
       " ('kembali', 'OTHER'),\n",
       " ('sebagai', 'OTHER'),\n",
       " ('ketua', 'OTHER'),\n",
       " ('umum', 'OTHER'),\n",
       " ('merupakan', 'OTHER'),\n",
       " ('pemaksaan', 'OTHER'),\n",
       " ('kehendak.', 'OTHER'),\n",
       " ('Menurut', 'OTHER'),\n",
       " ('dia,', 'OTHER'),\n",
       " ('ada', 'OTHER'),\n",
       " ('kesan', 'OTHER'),\n",
       " ('bahwa', 'OTHER'),\n",
       " ('Aburizal', 'person'),\n",
       " ('menggunakan', 'OTHER'),\n",
       " ('segala', 'OTHER'),\n",
       " ('cara', 'OTHER'),\n",
       " ('untuk', 'OTHER'),\n",
       " ('memuluskan', 'OTHER'),\n",
       " ('jalannya', 'OTHER'),\n",
       " ('kembali', 'OTHER'),\n",
       " ('menduduki', 'OTHER'),\n",
       " ('Golkar', 'organization'),\n",
       " ('1.', 'OTHER'),\n",
       " ('Hal', 'OTHER'),\n",
       " ('ini,', 'OTHER'),\n",
       " ('kata', 'OTHER'),\n",
       " ('Arie', 'person'),\n",
       " (',', 'OTHER'),\n",
       " ('berpotensi', 'OTHER'),\n",
       " ('menimbukan', 'OTHER'),\n",
       " ('perpecahan', 'OTHER')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip([idx2word[d] for d in train_X[1]], [idx2tag[d] for d in train_Y[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_char_seq(batch):\n",
    "    x = [[len(idx2word[i]) for i in k] for k in batch]\n",
    "    maxlen = max([j for i in x for j in i])\n",
    "    temp = np.zeros((batch.shape[0],batch.shape[1],maxlen),dtype=np.int32)\n",
    "    for i in range(batch.shape[0]):\n",
    "        for k in range(batch.shape[1]):\n",
    "            for no, c in enumerate(idx2word[batch[i,k]]):\n",
    "                temp[i,k,-1-no] = char2idx[c]\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 50, 11)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_char_seq(data['train_X'][:10]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_word,\n",
    "        dim_char,\n",
    "        dropout,\n",
    "        learning_rate,\n",
    "        hidden_size_char,\n",
    "        hidden_size_word,\n",
    "        num_layers,\n",
    "    ):\n",
    "        def cells(size, reuse = False):\n",
    "            return tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.nn.rnn_cell.LSTMCell(\n",
    "                    size,\n",
    "                    initializer = tf.orthogonal_initializer(),\n",
    "                    reuse = reuse,\n",
    "                ),\n",
    "                output_keep_prob = dropout,\n",
    "            )\n",
    "\n",
    "        self.word_ids = tf.placeholder(tf.int32, shape = [None, None])\n",
    "        self.char_ids = tf.placeholder(tf.int32, shape = [None, None, None])\n",
    "        self.labels = tf.placeholder(tf.int32, shape = [None, None])\n",
    "        self.maxlen = tf.shape(self.word_ids)[1]\n",
    "        self.lengths = tf.count_nonzero(self.word_ids, 1)\n",
    "        \n",
    "        self.word_embeddings = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [len(word2idx), dim_word], stddev = 1.0 / np.sqrt(dim_word)\n",
    "            )\n",
    "        )\n",
    "        self.char_embeddings = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [len(char2idx), dim_char], stddev = 1.0 / np.sqrt(dim_char)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        word_embedded = tf.nn.embedding_lookup(\n",
    "            self.word_embeddings, self.word_ids\n",
    "        )\n",
    "        char_embedded = tf.nn.embedding_lookup(\n",
    "            self.char_embeddings, self.char_ids\n",
    "        )\n",
    "        s = tf.shape(char_embedded)\n",
    "        char_embedded = tf.reshape(\n",
    "            char_embedded, shape = [s[0] * s[1], s[-2], dim_char]\n",
    "        )\n",
    "        \n",
    "        for n in range(num_layers):\n",
    "            (out_fw, out_bw), (\n",
    "                state_fw,\n",
    "                state_bw,\n",
    "            ) = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw = cells(hidden_size_char),\n",
    "                cell_bw = cells(hidden_size_char),\n",
    "                inputs = char_embedded,\n",
    "                dtype = tf.float32,\n",
    "                scope = 'bidirectional_rnn_char_%d' % (n),\n",
    "            )\n",
    "            char_embedded = tf.concat((out_fw, out_bw), 2)\n",
    "        output = tf.reshape(\n",
    "            char_embedded[:, -1], shape = [s[0], s[1], 2 * hidden_size_char]\n",
    "        )\n",
    "        word_embedded = tf.concat([word_embedded, output], axis = -1)\n",
    "\n",
    "        for n in range(num_layers):\n",
    "            (out_fw, out_bw), (\n",
    "                state_fw,\n",
    "                state_bw,\n",
    "            ) = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw = cells(hidden_size_word),\n",
    "                cell_bw = cells(hidden_size_word),\n",
    "                inputs = word_embedded,\n",
    "                dtype = tf.float32,\n",
    "                scope = 'bidirectional_rnn_word_%d' % (n),\n",
    "            )\n",
    "            word_embedded = tf.concat((out_fw, out_bw), 2)\n",
    "\n",
    "        logits = tf.layers.dense(word_embedded, len(idx2tag))\n",
    "        y_t = self.labels\n",
    "        log_likelihood, transition_params = tf.contrib.crf.crf_log_likelihood(\n",
    "            logits, y_t, self.lengths\n",
    "        )\n",
    "        self.cost = tf.reduce_mean(-log_likelihood)\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate = learning_rate\n",
    "        ).minimize(self.cost)\n",
    "        mask = tf.sequence_mask(self.lengths, maxlen = self.maxlen)\n",
    "        self.tags_seq, tags_score = tf.contrib.crf.crf_decode(\n",
    "            logits, transition_params, self.lengths\n",
    "        )\n",
    "        self.tags_seq = tf.identity(self.tags_seq, name = 'logits')\n",
    "\n",
    "        y_t = tf.cast(y_t, tf.int32)\n",
    "        self.prediction = tf.boolean_mask(self.tags_seq, mask)\n",
    "        mask_label = tf.boolean_mask(y_t, mask)\n",
    "        correct_pred = tf.equal(self.prediction, mask_label)\n",
    "        correct_index = tf.cast(correct_pred, tf.float32)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0805 00:21:47.352130 140268250425152 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py:507: calling count_nonzero (from tensorflow.python.ops.math_ops) with axis is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "reduction_indices is deprecated, use axis instead\n",
      "W0805 00:21:47.857108 140268250425152 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0805 00:21:47.858026 140268250425152 deprecation.py:323] From <ipython-input-10-9add79807405>:17: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W0805 00:21:47.863429 140268250425152 deprecation.py:323] From <ipython-input-10-9add79807405>:59: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "W0805 00:21:47.864029 140268250425152 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "W0805 00:21:47.923970 140268250425152 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0805 00:21:49.060250 140268250425152 deprecation.py:323] From <ipython-input-10-9add79807405>:80: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0805 00:21:49.064700 140268250425152 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0805 00:21:49.358782 140268250425152 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/crf/python/ops/crf.py:99: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "dim_word = 128\n",
    "dim_char = 256\n",
    "dropout = 0.8\n",
    "learning_rate = 1e-3\n",
    "hidden_size_char = 128\n",
    "hidden_size_word = 128\n",
    "num_layers = 2\n",
    "batch_size = 64\n",
    "\n",
    "model = Model(dim_word,dim_char,dropout,learning_rate,hidden_size_char,hidden_size_word,num_layers)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = 'KUALA LUMPUR: Sempena sambutan Aidilfitri minggu depan, Perdana Menteri Tun Dr Mahathir Mohamad dan Menteri Pengangkutan Anthony Loke Siew Fook menitipkan pesanan khas kepada orang ramai yang mahu pulang ke kampung halaman masing-masing. Dalam video pendek terbitan Jabatan Keselamatan Jalan Raya (JKJR) itu, Dr Mahathir menasihati mereka supaya berhenti berehat dan tidur sebentar  sekiranya mengantuk ketika memandu.'\n",
    "\n",
    "import re\n",
    "\n",
    "def entities_textcleaning(string, lowering = False):\n",
    "    \"\"\"\n",
    "    use by entities recognition, pos recognition and dependency parsing\n",
    "    \"\"\"\n",
    "    string = re.sub('[^A-Za-z0-9\\-\\/() ]+', ' ', string)\n",
    "    string = re.sub(r'[ ]+', ' ', string).strip()\n",
    "    original_string = string.split()\n",
    "    if lowering:\n",
    "        string = string.lower()\n",
    "    string = [\n",
    "        (original_string[no], word.title() if word.isupper() else word)\n",
    "        for no, word in enumerate(string.split())\n",
    "        if len(word)\n",
    "    ]\n",
    "    return [s[0] for s in string], [s[1] for s in string]\n",
    "\n",
    "def char_str_idx(corpus, dic, UNK = 0):\n",
    "    maxlen = max([len(i) for i in corpus])\n",
    "    X = np.zeros((len(corpus), maxlen))\n",
    "    for i in range(len(corpus)):\n",
    "        for no, k in enumerate(corpus[i][:maxlen][::-1]):\n",
    "            val = dic[k] if k in dic else UNK\n",
    "            X[i, -1 - no] = val\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 9191/9191 [1:12:15<00:00,  2.35it/s, accuracy=0.999, cost=0.235]\n",
      "test minibatch loop: 100%|██████████| 2298/2298 [08:35<00:00,  4.46it/s, accuracy=0.676, cost=52.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 4850.963254451752\n",
      "epoch: 0, training loss: 8.659316, training acc: 0.939020, valid loss: 25.182621, valid acc: 0.859950\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop:   0%|          | 0/9191 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kuala location\n",
      "Lumpur location\n",
      "Sempena OTHER\n",
      "sambutan OTHER\n",
      "Aidilfitri OTHER\n",
      "minggu time\n",
      "depan OTHER\n",
      "Perdana OTHER\n",
      "Menteri OTHER\n",
      "Tun person\n",
      "Dr person\n",
      "Mahathir person\n",
      "Mohamad OTHER\n",
      "dan OTHER\n",
      "Menteri OTHER\n",
      "Pengangkutan OTHER\n",
      "Anthony OTHER\n",
      "Loke OTHER\n",
      "Siew person\n",
      "Fook person\n",
      "menitipkan OTHER\n",
      "pesanan OTHER\n",
      "khas OTHER\n",
      "kepada OTHER\n",
      "orang quantity\n",
      "ramai quantity\n",
      "yang OTHER\n",
      "mahu OTHER\n",
      "pulang OTHER\n",
      "ke OTHER\n",
      "kampung location\n",
      "halaman location\n",
      "masing-masing OTHER\n",
      "Dalam OTHER\n",
      "video OTHER\n",
      "pendek OTHER\n",
      "terbitan OTHER\n",
      "Jabatan organization\n",
      "Keselamatan organization\n",
      "Jalan OTHER\n",
      "Raya OTHER\n",
      "(Jkjr) OTHER\n",
      "itu OTHER\n",
      "Dr OTHER\n",
      "Mahathir person\n",
      "menasihati OTHER\n",
      "mereka OTHER\n",
      "supaya OTHER\n",
      "berhenti OTHER\n",
      "berehat OTHER\n",
      "dan OTHER\n",
      "tidur OTHER\n",
      "sebentar OTHER\n",
      "sekiranya OTHER\n",
      "mengantuk quantity\n",
      "ketika OTHER\n",
      "memandu OTHER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 9191/9191 [1:12:14<00:00,  2.73it/s, accuracy=1, cost=0.0409]    \n",
      "test minibatch loop: 100%|██████████| 2298/2298 [08:36<00:00,  4.45it/s, accuracy=0.9, cost=26]     \n",
      "train minibatch loop:   0%|          | 0/9191 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 4850.45486998558\n",
      "epoch: 1, training loss: 2.052917, training acc: 0.984684, valid loss: 20.512893, valid acc: 0.898136\n",
      "\n",
      "Kuala location\n",
      "Lumpur location\n",
      "Sempena OTHER\n",
      "sambutan OTHER\n",
      "Aidilfitri time\n",
      "minggu time\n",
      "depan OTHER\n",
      "Perdana OTHER\n",
      "Menteri person\n",
      "Tun person\n",
      "Dr person\n",
      "Mahathir person\n",
      "Mohamad person\n",
      "dan OTHER\n",
      "Menteri person\n",
      "Pengangkutan person\n",
      "Anthony person\n",
      "Loke person\n",
      "Siew person\n",
      "Fook person\n",
      "menitipkan person\n",
      "pesanan OTHER\n",
      "khas OTHER\n",
      "kepada OTHER\n",
      "orang quantity\n",
      "ramai quantity\n",
      "yang OTHER\n",
      "mahu OTHER\n",
      "pulang OTHER\n",
      "ke OTHER\n",
      "kampung location\n",
      "halaman location\n",
      "masing-masing OTHER\n",
      "Dalam OTHER\n",
      "video OTHER\n",
      "pendek OTHER\n",
      "terbitan OTHER\n",
      "Jabatan organization\n",
      "Keselamatan organization\n",
      "Jalan organization\n",
      "Raya organization\n",
      "(Jkjr) location\n",
      "itu OTHER\n",
      "Dr person\n",
      "Mahathir person\n",
      "menasihati OTHER\n",
      "mereka OTHER\n",
      "supaya OTHER\n",
      "berhenti OTHER\n",
      "berehat OTHER\n",
      "dan OTHER\n",
      "tidur OTHER\n",
      "sebentar OTHER\n",
      "sekiranya OTHER\n",
      "mengantuk location\n",
      "ketika OTHER\n",
      "memandu OTHER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 9191/9191 [1:12:38<00:00,  2.92it/s, accuracy=1, cost=0.00787]   \n",
      "test minibatch loop: 100%|██████████| 2298/2298 [08:22<00:00,  4.58it/s, accuracy=0.94, cost=21.3]  \n",
      "train minibatch loop:   0%|          | 0/9191 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 4860.948942899704\n",
      "epoch: 2, training loss: 0.703229, training acc: 0.994912, valid loss: 14.662600, valid acc: 0.930679\n",
      "\n",
      "Kuala location\n",
      "Lumpur location\n",
      "Sempena OTHER\n",
      "sambutan OTHER\n",
      "Aidilfitri time\n",
      "minggu time\n",
      "depan OTHER\n",
      "Perdana person\n",
      "Menteri person\n",
      "Tun person\n",
      "Dr person\n",
      "Mahathir person\n",
      "Mohamad person\n",
      "dan OTHER\n",
      "Menteri person\n",
      "Pengangkutan person\n",
      "Anthony person\n",
      "Loke person\n",
      "Siew person\n",
      "Fook person\n",
      "menitipkan person\n",
      "pesanan OTHER\n",
      "khas OTHER\n",
      "kepada OTHER\n",
      "orang quantity\n",
      "ramai quantity\n",
      "yang OTHER\n",
      "mahu OTHER\n",
      "pulang OTHER\n",
      "ke OTHER\n",
      "kampung location\n",
      "halaman location\n",
      "masing-masing OTHER\n",
      "Dalam OTHER\n",
      "video OTHER\n",
      "pendek OTHER\n",
      "terbitan OTHER\n",
      "Jabatan organization\n",
      "Keselamatan organization\n",
      "Jalan organization\n",
      "Raya organization\n",
      "(Jkjr) location\n",
      "itu OTHER\n",
      "Dr person\n",
      "Mahathir person\n",
      "menasihati OTHER\n",
      "mereka OTHER\n",
      "supaya OTHER\n",
      "berhenti OTHER\n",
      "berehat OTHER\n",
      "dan OTHER\n",
      "tidur OTHER\n",
      "sebentar OTHER\n",
      "sekiranya OTHER\n",
      "mengantuk location\n",
      "ketika OTHER\n",
      "memandu OTHER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 9191/9191 [1:12:51<00:00,  2.69it/s, accuracy=1, cost=0.0424]    \n",
      "test minibatch loop: 100%|██████████| 2298/2298 [08:45<00:00,  4.37it/s, accuracy=0.936, cost=22.5] \n",
      "train minibatch loop:   0%|          | 0/9191 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 4896.710217475891\n",
      "epoch: 3, training loss: 0.304583, training acc: 0.997820, valid loss: 14.302636, valid acc: 0.943620\n",
      "\n",
      "Kuala location\n",
      "Lumpur location\n",
      "Sempena OTHER\n",
      "sambutan OTHER\n",
      "Aidilfitri event\n",
      "minggu time\n",
      "depan time\n",
      "Perdana person\n",
      "Menteri person\n",
      "Tun person\n",
      "Dr person\n",
      "Mahathir person\n",
      "Mohamad person\n",
      "dan OTHER\n",
      "Menteri person\n",
      "Pengangkutan person\n",
      "Anthony person\n",
      "Loke person\n",
      "Siew person\n",
      "Fook person\n",
      "menitipkan person\n",
      "pesanan OTHER\n",
      "khas OTHER\n",
      "kepada OTHER\n",
      "orang OTHER\n",
      "ramai OTHER\n",
      "yang OTHER\n",
      "mahu OTHER\n",
      "pulang OTHER\n",
      "ke OTHER\n",
      "kampung location\n",
      "halaman location\n",
      "masing-masing OTHER\n",
      "Dalam OTHER\n",
      "video OTHER\n",
      "pendek OTHER\n",
      "terbitan OTHER\n",
      "Jabatan organization\n",
      "Keselamatan organization\n",
      "Jalan organization\n",
      "Raya organization\n",
      "(Jkjr) location\n",
      "itu OTHER\n",
      "Dr person\n",
      "Mahathir person\n",
      "menasihati OTHER\n",
      "mereka OTHER\n",
      "supaya OTHER\n",
      "berhenti OTHER\n",
      "berehat organization\n",
      "dan OTHER\n",
      "tidur OTHER\n",
      "sebentar OTHER\n",
      "sekiranya OTHER\n",
      "mengantuk location\n",
      "ketika OTHER\n",
      "memandu OTHER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 9191/9191 [1:12:49<00:00,  2.64it/s, accuracy=1, cost=0.000336]  \n",
      "test minibatch loop: 100%|██████████| 2298/2298 [08:38<00:00,  4.43it/s, accuracy=0.94, cost=24.1]  \n",
      "train minibatch loop:   0%|          | 0/9191 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 4888.042459726334\n",
      "epoch: 4, training loss: 0.149868, training acc: 0.998946, valid loss: 15.276245, valid acc: 0.947501\n",
      "\n",
      "Kuala location\n",
      "Lumpur location\n",
      "Sempena OTHER\n",
      "sambutan OTHER\n",
      "Aidilfitri event\n",
      "minggu time\n",
      "depan time\n",
      "Perdana person\n",
      "Menteri person\n",
      "Tun person\n",
      "Dr person\n",
      "Mahathir person\n",
      "Mohamad person\n",
      "dan OTHER\n",
      "Menteri person\n",
      "Pengangkutan person\n",
      "Anthony person\n",
      "Loke person\n",
      "Siew person\n",
      "Fook person\n",
      "menitipkan person\n",
      "pesanan OTHER\n",
      "khas OTHER\n",
      "kepada OTHER\n",
      "orang OTHER\n",
      "ramai OTHER\n",
      "yang OTHER\n",
      "mahu OTHER\n",
      "pulang OTHER\n",
      "ke OTHER\n",
      "kampung location\n",
      "halaman location\n",
      "masing-masing OTHER\n",
      "Dalam OTHER\n",
      "video OTHER\n",
      "pendek OTHER\n",
      "terbitan OTHER\n",
      "Jabatan organization\n",
      "Keselamatan organization\n",
      "Jalan organization\n",
      "Raya organization\n",
      "(Jkjr) location\n",
      "itu OTHER\n",
      "Dr person\n",
      "Mahathir person\n",
      "menasihati OTHER\n",
      "mereka OTHER\n",
      "supaya OTHER\n",
      "berhenti OTHER\n",
      "berehat OTHER\n",
      "dan OTHER\n",
      "tidur OTHER\n",
      "sebentar OTHER\n",
      "sekiranya OTHER\n",
      "mengantuk location\n",
      "ketika OTHER\n",
      "memandu OTHER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 9191/9191 [1:12:40<00:00,  2.62it/s, accuracy=1, cost=0.00242]   \n",
      "test minibatch loop: 100%|██████████| 2298/2298 [08:39<00:00,  4.42it/s, accuracy=0.944, cost=13.5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 4880.330358505249\n",
      "epoch: 5, training loss: 0.096976, training acc: 0.999330, valid loss: 13.203072, valid acc: 0.951456\n",
      "\n",
      "Kuala location\n",
      "Lumpur location\n",
      "Sempena OTHER\n",
      "sambutan OTHER\n",
      "Aidilfitri event\n",
      "minggu time\n",
      "depan time\n",
      "Perdana person\n",
      "Menteri person\n",
      "Tun person\n",
      "Dr person\n",
      "Mahathir person\n",
      "Mohamad person\n",
      "dan OTHER\n",
      "Menteri person\n",
      "Pengangkutan person\n",
      "Anthony person\n",
      "Loke person\n",
      "Siew person\n",
      "Fook person\n",
      "menitipkan person\n",
      "pesanan OTHER\n",
      "khas OTHER\n",
      "kepada OTHER\n",
      "orang OTHER\n",
      "ramai OTHER\n",
      "yang OTHER\n",
      "mahu OTHER\n",
      "pulang OTHER\n",
      "ke OTHER\n",
      "kampung location\n",
      "halaman location\n",
      "masing-masing OTHER\n",
      "Dalam OTHER\n",
      "video OTHER\n",
      "pendek OTHER\n",
      "terbitan OTHER\n",
      "Jabatan organization\n",
      "Keselamatan organization\n",
      "Jalan organization\n",
      "Raya organization\n",
      "(Jkjr) location\n",
      "itu OTHER\n",
      "Dr person\n",
      "Mahathir person\n",
      "menasihati OTHER\n",
      "mereka OTHER\n",
      "supaya OTHER\n",
      "berhenti OTHER\n",
      "berehat OTHER\n",
      "dan OTHER\n",
      "tidur OTHER\n",
      "sebentar OTHER\n",
      "sekiranya OTHER\n",
      "mengantuk location\n",
      "ketika OTHER\n",
      "memandu OTHER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "for e in range(6):\n",
    "    lasttime = time.time()\n",
    "    train_acc, train_loss, test_acc, test_loss = 0, 0, 0, 0\n",
    "    pbar = tqdm(\n",
    "        range(0, train_X.shape[0], batch_size), desc = 'train minibatch loop'\n",
    "    )\n",
    "    for i in pbar:\n",
    "        index = min(i + batch_size, train_X.shape[0])\n",
    "        batch_x = train_X[i : index]\n",
    "        batch_char = generate_char_seq(batch_x)\n",
    "        batch_y = train_Y[i : index]\n",
    "        acc, cost, _ = sess.run(\n",
    "            [model.accuracy, model.cost, model.optimizer],\n",
    "            feed_dict = {\n",
    "                model.word_ids: batch_x,\n",
    "                model.char_ids: batch_char,\n",
    "                model.labels: batch_y\n",
    "            },\n",
    "        )\n",
    "        assert not np.isnan(cost)\n",
    "        train_loss += cost\n",
    "        train_acc += acc\n",
    "        pbar.set_postfix(cost = cost, accuracy = acc)\n",
    "        \n",
    "    pbar = tqdm(\n",
    "        range(0, test_X.shape[0], batch_size), desc = 'test minibatch loop'\n",
    "    )\n",
    "    for i in pbar:\n",
    "        index = min(i + batch_size, test_X.shape[0])\n",
    "        batch_x = test_X[i : index]\n",
    "        batch_char = generate_char_seq(batch_x)\n",
    "        batch_y = test_Y[i : index]\n",
    "        acc, cost = sess.run(\n",
    "            [model.accuracy, model.cost],\n",
    "            feed_dict = {\n",
    "                model.word_ids: batch_x,\n",
    "                model.char_ids: batch_char,\n",
    "                model.labels: batch_y\n",
    "            },\n",
    "        )\n",
    "        assert not np.isnan(cost)\n",
    "        test_loss += cost\n",
    "        test_acc += acc\n",
    "        pbar.set_postfix(cost = cost, accuracy = acc)\n",
    "    \n",
    "    train_loss /= len(train_X) / batch_size\n",
    "    train_acc /= len(train_X) / batch_size\n",
    "    test_loss /= len(test_X) / batch_size\n",
    "    test_acc /= len(test_X) / batch_size\n",
    "\n",
    "    print('time taken:', time.time() - lasttime)\n",
    "    print(\n",
    "        'epoch: %d, training loss: %f, training acc: %f, valid loss: %f, valid acc: %f\\n'\n",
    "        % (e, train_loss, train_acc, test_loss, test_acc)\n",
    "    )\n",
    "    \n",
    "    sequence = entities_textcleaning(string)[1]\n",
    "    X_seq = char_str_idx([sequence], word2idx, 2)\n",
    "    X_char_seq = generate_char_seq(X_seq)\n",
    "\n",
    "    predicted = sess.run(model.tags_seq,\n",
    "                feed_dict = {\n",
    "                    model.word_ids: X_seq,\n",
    "                    model.char_ids: X_char_seq,\n",
    "                },\n",
    "        )[0]\n",
    "\n",
    "    for i in range(len(predicted)):\n",
    "        print(sequence[i],idx2tag[predicted[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mahathir person\n",
      "suka OTHER\n",
      "akta law\n",
      "19977 law\n"
     ]
    }
   ],
   "source": [
    "sequence = entities_textcleaning('mahathir suka akta 19977')[1]\n",
    "X_seq = char_str_idx([sequence], word2idx, 2)\n",
    "X_char_seq = generate_char_seq(X_seq)\n",
    "\n",
    "predicted = sess.run(model.tags_seq,\n",
    "            feed_dict = {\n",
    "                model.word_ids: X_seq,\n",
    "                model.char_ids: X_char_seq,\n",
    "            },\n",
    "    )[0]\n",
    "\n",
    "for i in range(len(predicted)):\n",
    "    print(sequence[i],idx2tag[predicted[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred2label(pred):\n",
    "    out = []\n",
    "    for pred_i in pred:\n",
    "        out_i = []\n",
    "        for p in pred_i:\n",
    "            out_i.append(idx2tag[p])\n",
    "        out.append(out_i)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation minibatch loop: 100%|██████████| 2298/2298 [08:19<00:00,  4.60it/s]\n"
     ]
    }
   ],
   "source": [
    "real_Y, predict_Y = [], []\n",
    "\n",
    "pbar = tqdm(\n",
    "    range(0, len(test_X), batch_size), desc = 'validation minibatch loop'\n",
    ")\n",
    "for i in pbar:\n",
    "    batch_x = test_X[i : min(i + batch_size, test_X.shape[0])]\n",
    "    batch_char = generate_char_seq(batch_x)\n",
    "    batch_y = test_Y[i : min(i + batch_size, test_X.shape[0])]\n",
    "    predicted = pred2label(sess.run(model.tags_seq,\n",
    "            feed_dict = {\n",
    "                model.word_ids: batch_x,\n",
    "                model.char_ids: batch_char,\n",
    "            },\n",
    "    ))\n",
    "    real = pred2label(batch_y)\n",
    "    predict_Y.extend(predicted)\n",
    "    real_Y.extend(real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       OTHER   0.971261  0.995777  0.983367   5160854\n",
      "       event   0.991603  0.181505  0.306844    143787\n",
      "         law   0.989329  0.879490  0.931181    146950\n",
      "    location   0.848477  0.960757  0.901133    428869\n",
      "organization   0.960967  0.761301  0.849560    694150\n",
      "      person   0.850705  0.969984  0.906437    507960\n",
      "    quantity   0.996606  0.972120  0.984211     88200\n",
      "        time   0.879509  0.986763  0.930054    179880\n",
      "\n",
      "    accuracy                       0.951052   7350650\n",
      "   macro avg   0.936057  0.838462  0.849098   7350650\n",
      "weighted avg   0.953613  0.951052  0.945046   7350650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(np.array(real_Y).ravel(), np.array(predict_Y).ravel(),\n",
    "                           digits = 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Placeholder',\n",
       " 'Placeholder_1',\n",
       " 'Placeholder_2',\n",
       " 'Variable',\n",
       " 'Variable_1',\n",
       " 'bidirectional_rnn_char_0/fw/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_char_0/fw/lstm_cell/bias',\n",
       " 'bidirectional_rnn_char_0/bw/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_char_0/bw/lstm_cell/bias',\n",
       " 'bidirectional_rnn_char_1/fw/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_char_1/fw/lstm_cell/bias',\n",
       " 'bidirectional_rnn_char_1/bw/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_char_1/bw/lstm_cell/bias',\n",
       " 'bidirectional_rnn_word_0/fw/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_word_0/fw/lstm_cell/bias',\n",
       " 'bidirectional_rnn_word_0/bw/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_word_0/bw/lstm_cell/bias',\n",
       " 'bidirectional_rnn_word_1/fw/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_word_1/fw/lstm_cell/bias',\n",
       " 'bidirectional_rnn_word_1/bw/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_word_1/bw/lstm_cell/bias',\n",
       " 'dense/kernel',\n",
       " 'dense/bias',\n",
       " 'transitions',\n",
       " 'logits']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver(tf.trainable_variables())\n",
    "saver.save(sess, 'concat/model.ckpt')\n",
    "\n",
    "strings = ','.join(\n",
    "    [\n",
    "        n.name\n",
    "        for n in tf.get_default_graph().as_graph_def().node\n",
    "        if ('Variable' in n.op\n",
    "        or 'Placeholder' in n.name\n",
    "        or 'logits' in n.name\n",
    "        or 'alphas' in n.name)\n",
    "        and 'Adam' not in n.name\n",
    "        and 'beta' not in n.name\n",
    "        and 'OptimizeLoss' not in n.name\n",
    "        and 'Global_Step' not in n.name\n",
    "    ]\n",
    ")\n",
    "strings.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_graph(model_dir, output_node_names):\n",
    "\n",
    "    if not tf.gfile.Exists(model_dir):\n",
    "        raise AssertionError(\n",
    "            \"Export directory doesn't exists. Please specify an export \"\n",
    "            'directory: %s' % model_dir\n",
    "        )\n",
    "\n",
    "    checkpoint = tf.train.get_checkpoint_state(model_dir)\n",
    "    input_checkpoint = checkpoint.model_checkpoint_path\n",
    "\n",
    "    absolute_model_dir = '/'.join(input_checkpoint.split('/')[:-1])\n",
    "    output_graph = absolute_model_dir + '/frozen_model.pb'\n",
    "    clear_devices = True\n",
    "    with tf.Session(graph = tf.Graph()) as sess:\n",
    "        saver = tf.train.import_meta_graph(\n",
    "            input_checkpoint + '.meta', clear_devices = clear_devices\n",
    "        )\n",
    "        saver.restore(sess, input_checkpoint)\n",
    "        output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "            sess,\n",
    "            tf.get_default_graph().as_graph_def(),\n",
    "            output_node_names.split(','),\n",
    "        )\n",
    "        with tf.gfile.GFile(output_graph, 'wb') as f:\n",
    "            f.write(output_graph_def.SerializeToString())\n",
    "        print('%d ops in the final graph.' % len(output_graph_def.node))\n",
    "        \n",
    "def load_graph(frozen_graph_filename):\n",
    "    with tf.gfile.GFile(frozen_graph_filename, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def)\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0805 08:38:52.328392 140268250425152 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "W0805 08:38:52.601029 140268250425152 deprecation.py:323] From <ipython-input-19-3d8392da7830>:23: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "W0805 08:38:52.601755 140268250425152 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1532 ops in the final graph.\n"
     ]
    }
   ],
   "source": [
    "freeze_graph('concat', strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = load_graph('concat/frozen_model.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "word_ids = g.get_tensor_by_name('import/Placeholder:0')\n",
    "char_ids = g.get_tensor_by_name('import/Placeholder_1:0')\n",
    "tags_seq = g.get_tensor_by_name('import/logits:0')\n",
    "tags_state_fw = g.get_tensor_by_name('import/transitions:0')\n",
    "tags_state_bw = g.get_tensor_by_name('import/Variable:0')\n",
    "test_sess = tf.InteractiveSession(graph = g)\n",
    "predicted = test_sess.run([tags_seq, tags_state_fw, tags_state_bw],\n",
    "            feed_dict = {\n",
    "                word_ids: X_seq,\n",
    "                char_ids: X_char_seq,\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[4, 2, 8, 8]], dtype=int32),\n",
       " array([[ 0.47884172, -0.16190596, -2.4069993 , -1.3115252 , -1.9645609 ,\n",
       "         -0.20467333, -1.109368  , -0.4051585 , -0.73241717, -0.57735586],\n",
       "        [-0.41838917, -0.40878856, -3.450508  , -1.0079795 , -1.126499  ,\n",
       "         -0.8411751 , -0.4628218 , -0.53364605, -0.6538642 , -0.98395264],\n",
       "        [-1.4484956 , -2.4792776 ,  1.0506705 , -1.0934587 , -0.9977262 ,\n",
       "         -1.2000083 , -0.6880191 , -1.6287059 , -1.5211865 , -1.2802426 ],\n",
       "        [-1.041553  , -1.6351454 , -1.4314326 ,  1.9525024 , -1.3669676 ,\n",
       "         -1.4555649 , -0.41423967, -1.5732112 , -3.9457858 , -3.0300593 ],\n",
       "        [-1.3480742 , -2.1231194 , -1.2117796 , -2.1732378 ,  1.8638    ,\n",
       "         -0.6669971 , -1.952644  , -1.663421  , -3.3421414 , -1.807233  ],\n",
       "        [-0.3863962 , -0.47493416, -1.2358948 , -1.161847  , -0.49586853,\n",
       "          1.8206847 , -0.7389852 , -3.117508  , -1.6827074 , -1.134648  ],\n",
       "        [-0.9731228 , -1.271768  , -0.81208587, -2.388461  , -0.62136734,\n",
       "         -0.6048617 ,  1.931444  , -0.9558625 , -2.0121994 , -1.6540738 ],\n",
       "        [-0.55710596, -0.27782583, -1.4717171 , -1.4897999 , -1.3805573 ,\n",
       "         -3.2082398 , -0.7797185 ,  1.2860484 , -3.3349702 , -5.251648  ],\n",
       "        [-0.35837752, -0.8974596 , -1.1639369 , -2.9073708 , -1.6890558 ,\n",
       "         -3.5949929 , -0.6940302 , -2.9629626 ,  1.8027016 , -3.0579152 ],\n",
       "        [-0.259302  , -0.7547574 , -1.6125801 , -2.436959  , -1.7212354 ,\n",
       "         -1.0727485 , -2.3732288 , -4.494331  , -4.297628  ,  1.5011649 ]],\n",
       "       dtype=float32),\n",
       " array([[ 0.08437788,  0.01848716, -0.04109306, ...,  0.04482306,\n",
       "         -0.15521765, -0.00892066],\n",
       "        [ 0.05400806,  0.01416366,  0.07723003, ..., -0.01741494,\n",
       "         -0.00507902, -0.00514337],\n",
       "        [-0.04632119, -0.08211993, -0.08111479, ...,  0.129999  ,\n",
       "         -0.06037236,  0.11595264],\n",
       "        ...,\n",
       "        [ 0.04757647, -0.00883408, -0.02435873, ...,  0.04110677,\n",
       "          0.08302181,  0.0989907 ],\n",
       "        [-0.00770224, -0.16607328, -0.01935261, ..., -0.00255077,\n",
       "          0.04009438, -0.02876427],\n",
       "        [-0.06948477, -0.01568228,  0.04083067, ..., -0.0289785 ,\n",
       "          0.02559927, -0.03527535]], dtype=float32)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
