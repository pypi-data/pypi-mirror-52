{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import json\nwith open('../input/toxicity.json') as fopen:\n    x = json.load(fopen)\ntexts = x['x']\nlabels = x['y']","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip3 install bert-tensorflow sentencepiece","execution_count":2,"outputs":[{"output_type":"stream","text":"Collecting bert-tensorflow\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n\u001b[K     |████████████████████████████████| 71kB 8.3MB/s eta 0:00:011\n\u001b[?25hRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (0.1.82)\nRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from bert-tensorflow) (1.12.0)\nInstalling collected packages: bert-tensorflow\nSuccessfully installed bert-tensorflow-1.0.1\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nimport json\nimport bert\nfrom bert import run_classifier\nfrom bert import optimization\nfrom bert import tokenization\nfrom bert import modeling\nimport numpy as np\nimport tensorflow as tf","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_SEQ_LENGTH = 100","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget http://s3-ap-southeast-1.amazonaws.com/huseinhouse-storage/bert-bahasa/bert-bahasa-small.tar.gz\n!tar -zxf bert-bahasa-small.tar.gz","execution_count":5,"outputs":[{"output_type":"stream","text":"--2019-08-04 06:36:47--  http://s3-ap-southeast-1.amazonaws.com/huseinhouse-storage/bert-bahasa/bert-bahasa-small.tar.gz\nResolving s3-ap-southeast-1.amazonaws.com (s3-ap-southeast-1.amazonaws.com)... 52.219.32.41\nConnecting to s3-ap-southeast-1.amazonaws.com (s3-ap-southeast-1.amazonaws.com)|52.219.32.41|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 171855372 (164M) [binary/octet-stream]\nSaving to: ‘bert-bahasa-small.tar.gz’\n\nbert-bahasa-small.t 100%[===================>] 163.89M  10.6MB/s    in 18s     \n\n2019-08-04 06:37:05 (9.27 MB/s) - ‘bert-bahasa-small.tar.gz’ saved [171855372/171855372]\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import unicodedata\nimport six\nfrom functools import partial\n\nSPIECE_UNDERLINE = '▁'\n\ndef preprocess_text(inputs, lower=False, remove_space=True, keep_accents=False):\n  if remove_space:\n    outputs = ' '.join(inputs.strip().split())\n  else:\n    outputs = inputs\n  outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\n\n  if six.PY2 and isinstance(outputs, str):\n    outputs = outputs.decode('utf-8')\n\n  if not keep_accents:\n    outputs = unicodedata.normalize('NFKD', outputs)\n    outputs = ''.join([c for c in outputs if not unicodedata.combining(c)])\n  if lower:\n    outputs = outputs.lower()\n\n  return outputs\n\n\ndef encode_pieces(sp_model, text, return_unicode=True, sample=False):\n  # return_unicode is used only for py2\n\n  # note(zhiliny): in some systems, sentencepiece only accepts str for py2\n  if six.PY2 and isinstance(text, unicode):\n    text = text.encode('utf-8')\n\n  if not sample:\n    pieces = sp_model.EncodeAsPieces(text)\n  else:\n    pieces = sp_model.SampleEncodeAsPieces(text, 64, 0.1)\n  new_pieces = []\n  for piece in pieces:\n    if len(piece) > 1 and piece[-1] == ',' and piece[-2].isdigit():\n      cur_pieces = sp_model.EncodeAsPieces(\n          piece[:-1].replace(SPIECE_UNDERLINE, ''))\n      if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n        if len(cur_pieces[0]) == 1:\n          cur_pieces = cur_pieces[1:]\n        else:\n          cur_pieces[0] = cur_pieces[0][1:]\n      cur_pieces.append(piece[-1])\n      new_pieces.extend(cur_pieces)\n    else:\n      new_pieces.append(piece)\n\n  # note(zhiliny): convert back to unicode for py2\n  if six.PY2 and return_unicode:\n    ret_pieces = []\n    for piece in new_pieces:\n      if isinstance(piece, str):\n        piece = piece.decode('utf-8')\n      ret_pieces.append(piece)\n    new_pieces = ret_pieces\n\n  return new_pieces\n\n\ndef encode_ids(sp_model, text, sample=False):\n  pieces = encode_pieces(sp_model, text, return_unicode=False, sample=sample)\n  ids = [sp_model.PieceToId(piece) for piece in pieces]\n  return ids","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sentencepiece as spm\n\nsp_model = spm.SentencePieceProcessor()\nsp_model.Load('bert-bahasa-small/sp10m.cased.v4.model')\n\nwith open('bert-bahasa-small/sp10m.cased.v4.vocab') as fopen:\n    v = fopen.read().split('\\n')[:-1]\nv = [i.split('\\t') for i in v]\nv = {i[0]: i[1] for i in v}\n\nclass Tokenizer:\n    def __init__(self, v):\n        self.vocab = v\n        pass\n    \n    def tokenize(self, string):\n        return encode_pieces(sp_model, string, return_unicode=False, sample=False)\n    \n    def convert_tokens_to_ids(self, tokens):\n        return [sp_model.PieceToId(piece) for piece in tokens]\n    \n    def convert_ids_to_tokens(self, ids):\n        return [sp_model.IdToPiece(i) for i in ids]\n    \ntokenizer = Tokenizer(v)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BERT_INIT_CHKPNT = 'bert-bahasa-small/model.ckpt'\nBERT_CONFIG = 'bert-bahasa-small/bert_config.json'","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_ids, input_masks, segment_ids = [], [], []\n\nfor text in tqdm(texts):\n    tokens_a = tokenizer.tokenize(text)\n    if len(tokens_a) > MAX_SEQ_LENGTH - 2:\n        tokens_a = tokens_a[:(MAX_SEQ_LENGTH - 2)]\n    tokens = [\"<cls>\"] + tokens_a + [\"<sep>\"]\n    segment_id = [0] * len(tokens)\n    input_id = tokenizer.convert_tokens_to_ids(tokens)\n    input_mask = [1] * len(input_id)\n    padding = [0] * (MAX_SEQ_LENGTH - len(input_id))\n    input_id += padding\n    input_mask += padding\n    segment_id += padding\n    \n    input_ids.append(input_id)\n    input_masks.append(input_mask)\n    segment_ids.append(segment_id)","execution_count":9,"outputs":[{"output_type":"stream","text":"100%|██████████| 192029/192029 [00:53<00:00, 3576.78it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_config = modeling.BertConfig.from_json_file(BERT_CONFIG)\n\nepoch = 10\nbatch_size = 60\nwarmup_proportion = 0.1\nnum_train_steps = int(len(texts) / batch_size * epoch)\nnum_warmup_steps = int(num_train_steps * warmup_proportion)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model:\n    def __init__(\n        self,\n        dimension_output,\n        learning_rate = 2e-5,\n    ):\n        self.X = tf.placeholder(tf.int32, [None, None])\n        self.Y = tf.placeholder(tf.float32, [None, dimension_output])\n        \n        model = modeling.BertModel(\n            config=bert_config,\n            is_training=False,\n            input_ids=self.X,\n            use_one_hot_embeddings=False)\n        \n        output_layer = model.get_sequence_output()\n        self.logits_seq = tf.layers.dense(output_layer, dimension_output)\n        self.logits_seq = tf.identity(self.logits_seq, name = 'logits_seq')\n        self.logits = self.logits_seq[:, 0]\n        self.logits = tf.identity(self.logits, name = 'logits')\n        \n        self.cost = tf.reduce_mean(\n            tf.nn.sigmoid_cross_entropy_with_logits(\n                logits = self.logits, labels = self.Y\n            )\n        )\n        \n        self.optimizer = optimization.create_optimizer(self.cost, learning_rate, \n                                                       num_train_steps, num_warmup_steps, False)\n        \n        correct_prediction = tf.equal(tf.round(tf.nn.sigmoid(self.logits)), tf.round(self.Y))\n        all_labels_true = tf.reduce_min(tf.cast(correct_prediction, tf.float32), 1)\n        self.accuracy = tf.reduce_mean(all_labels_true)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dimension_output = 6\nlearning_rate = 2e-5\n\ntf.reset_default_graph()\nsess = tf.InteractiveSession()\nmodel = Model(\n    dimension_output,\n    learning_rate\n)\n\nsess.run(tf.global_variables_initializer())\nvar_lists = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = 'bert')\nsaver = tf.train.Saver(var_list = var_lists)\nsaver.restore(sess, BERT_INIT_CHKPNT)","execution_count":14,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n  warnings.warn('An interactive session is already active. This can '\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_input_ids, test_input_ids, train_Y, test_Y = train_test_split(\n    input_ids, labels, test_size = 0.2\n)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\n\nEARLY_STOPPING, CURRENT_CHECKPOINT, CURRENT_ACC, EPOCH = 2, 0, 0, 0\n\nwhile True:\n    lasttime = time.time()\n    if CURRENT_CHECKPOINT == EARLY_STOPPING:\n        print('break epoch:%d\\n' % (EPOCH))\n        break\n\n    train_acc, train_loss, test_acc, test_loss = 0, 0, 0, 0\n    pbar = tqdm(\n        range(0, len(train_input_ids), batch_size), desc = 'train minibatch loop'\n    )\n    for i in pbar:\n        index = min(i + batch_size, len(train_input_ids))\n        batch_x = train_input_ids[i: index]\n        batch_y = train_Y[i: index]\n        acc, cost, _ = sess.run(\n            [model.accuracy, model.cost, model.optimizer],\n            feed_dict = {\n                model.Y: batch_y,\n                model.X: batch_x,\n            },\n        )\n        assert not np.isnan(cost)\n        train_loss += cost\n        train_acc += acc\n        pbar.set_postfix(cost = cost, accuracy = acc)\n        \n    pbar = tqdm(range(0, len(test_input_ids), batch_size), desc = 'test minibatch loop')\n    for i in pbar:\n        index = min(i + batch_size, len(test_input_ids))\n        batch_x = test_input_ids[i: index]\n        batch_y = test_Y[i: index]\n        acc, cost = sess.run(\n            [model.accuracy, model.cost],\n            feed_dict = {\n                model.Y: batch_y,\n                model.X: batch_x,\n            },\n        )\n        test_loss += cost\n        test_acc += acc\n        pbar.set_postfix(cost = cost, accuracy = acc)\n\n    train_loss /= len(train_input_ids) / batch_size\n    train_acc /= len(train_input_ids) / batch_size\n    test_loss /= len(test_input_ids) / batch_size\n    test_acc /= len(test_input_ids) / batch_size\n\n    if test_acc > CURRENT_ACC:\n        print(\n            'epoch: %d, pass acc: %f, current acc: %f'\n            % (EPOCH, CURRENT_ACC, test_acc)\n        )\n        CURRENT_ACC = test_acc\n        CURRENT_CHECKPOINT = 0\n    else:\n        CURRENT_CHECKPOINT += 1\n        \n    print('time taken:', time.time() - lasttime)\n    print(\n        'epoch: %d, training loss: %f, training acc: %f, valid loss: %f, valid acc: %f\\n'\n        % (EPOCH, train_loss, train_acc, test_loss, test_acc)\n    )\n    EPOCH += 1","execution_count":16,"outputs":[{"output_type":"stream","text":"train minibatch loop: 100%|██████████| 2561/2561 [09:58<00:00,  5.13it/s, accuracy=0.913, cost=0.0299] \ntest minibatch loop: 100%|██████████| 641/641 [00:51<00:00, 12.56it/s, accuracy=1, cost=0.00708]   \ntrain minibatch loop:   0%|          | 0/2561 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"epoch: 0, pass acc: 0.000000, current acc: 0.920637\ntime taken: 649.4259285926819\nepoch: 0, training loss: 0.087843, training acc: 0.891734, valid loss: 0.057133, valid acc: 0.920637\n\n","name":"stdout"},{"output_type":"stream","text":"train minibatch loop: 100%|██████████| 2561/2561 [09:56<00:00,  5.12it/s, accuracy=0.957, cost=0.0223] \ntest minibatch loop: 100%|██████████| 641/641 [00:50<00:00, 12.61it/s, accuracy=1, cost=0.00308]    \ntrain minibatch loop:   0%|          | 0/2561 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"epoch: 1, pass acc: 0.920637, current acc: 0.921913\ntime taken: 647.0207903385162\nepoch: 1, training loss: 0.053778, training acc: 0.918472, valid loss: 0.055203, valid acc: 0.921913\n\n","name":"stdout"},{"output_type":"stream","text":"train minibatch loop: 100%|██████████| 2561/2561 [09:55<00:00,  5.14it/s, accuracy=1, cost=0.0118]     \ntest minibatch loop: 100%|██████████| 641/641 [00:50<00:00, 12.60it/s, accuracy=1, cost=0.000368]   \ntrain minibatch loop:   0%|          | 0/2561 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"time taken: 646.7220537662506\nepoch: 2, training loss: 0.041131, training acc: 0.933747, valid loss: 0.062951, valid acc: 0.920429\n\n","name":"stdout"},{"output_type":"stream","text":"train minibatch loop: 100%|██████████| 2561/2561 [09:55<00:00,  5.13it/s, accuracy=0.957, cost=0.0107] \ntest minibatch loop: 100%|██████████| 641/641 [00:50<00:00, 12.61it/s, accuracy=1, cost=0.000208]   ","name":"stderr"},{"output_type":"stream","text":"time taken: 646.6734571456909\nepoch: 3, training loss: 0.028792, training acc: 0.952555, valid loss: 0.073648, valid acc: 0.920846\n\nbreak epoch:4\n\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"stack = []\n\npbar = tqdm(\n    range(0, len(test_input_ids), batch_size), desc = 'validation minibatch loop'\n)\nfor i in pbar:\n    index = min(i + batch_size, len(test_input_ids))\n    batch_x = test_input_ids[i: index]\n    stack.append(sess.run(tf.nn.sigmoid(model.logits),\n            feed_dict = {\n                model.Y: batch_y,\n                model.X: batch_x,\n            },\n    ))","execution_count":17,"outputs":[{"output_type":"stream","text":"validation minibatch loop: 100%|██████████| 641/641 [03:16<00:00,  3.32it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\nprint(metrics.classification_report(np.array(test_Y),np.around(np.concatenate(stack,axis=0)),\n                                    target_names=[\"toxic\", \"severe_toxic\", \"obscene\", \n                                            \"threat\", \"insult\", \"identity_hate\"],\n                                   digits=6))","execution_count":18,"outputs":[{"output_type":"stream","text":"               precision    recall  f1-score   support\n\n        toxic   0.844565  0.676247  0.751092      3688\n severe_toxic   0.661538  0.219949  0.330134       391\n      obscene   0.813200  0.672515  0.736196      2052\n       threat   0.592593  0.285714  0.385542       112\n       insult   0.801894  0.580696  0.673600      1896\nidentity_hate   0.664671  0.331343  0.442231       335\n\n    micro avg   0.816442  0.614114  0.700970      8474\n    macro avg   0.729743  0.461077  0.553133      8474\n weighted avg   0.808535  0.614114  0.693682      8474\n  samples avg   0.061611  0.055738  0.056286      8474\n\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n  'precision', 'predicted', average, warn_for)\n/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n  'recall', 'true', average, warn_for)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"strings = ','.join(\n    [\n        n.name\n        for n in tf.get_default_graph().as_graph_def().node\n        if ('Variable' in n.op\n        or 'Placeholder' in n.name\n        or 'logits' in n.name\n        or 'alphas' in n.name\n        or 'self/Softmax' in n.name)\n        and 'adam' not in n.name\n        and 'beta' not in n.name\n        and 'global_step' not in n.name\n    ]\n)\nstrings.split(',')","execution_count":20,"outputs":[{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"['Placeholder',\n 'Placeholder_1',\n 'bert/embeddings/word_embeddings',\n 'bert/embeddings/token_type_embeddings',\n 'bert/embeddings/position_embeddings',\n 'bert/embeddings/LayerNorm/gamma',\n 'bert/encoder/layer_0/attention/self/query/kernel',\n 'bert/encoder/layer_0/attention/self/query/bias',\n 'bert/encoder/layer_0/attention/self/key/kernel',\n 'bert/encoder/layer_0/attention/self/key/bias',\n 'bert/encoder/layer_0/attention/self/value/kernel',\n 'bert/encoder/layer_0/attention/self/value/bias',\n 'bert/encoder/layer_0/attention/self/Softmax',\n 'bert/encoder/layer_0/attention/output/dense/kernel',\n 'bert/encoder/layer_0/attention/output/dense/bias',\n 'bert/encoder/layer_0/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_0/intermediate/dense/kernel',\n 'bert/encoder/layer_0/intermediate/dense/bias',\n 'bert/encoder/layer_0/output/dense/kernel',\n 'bert/encoder/layer_0/output/dense/bias',\n 'bert/encoder/layer_0/output/LayerNorm/gamma',\n 'bert/encoder/layer_1/attention/self/query/kernel',\n 'bert/encoder/layer_1/attention/self/query/bias',\n 'bert/encoder/layer_1/attention/self/key/kernel',\n 'bert/encoder/layer_1/attention/self/key/bias',\n 'bert/encoder/layer_1/attention/self/value/kernel',\n 'bert/encoder/layer_1/attention/self/value/bias',\n 'bert/encoder/layer_1/attention/self/Softmax',\n 'bert/encoder/layer_1/attention/output/dense/kernel',\n 'bert/encoder/layer_1/attention/output/dense/bias',\n 'bert/encoder/layer_1/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_1/intermediate/dense/kernel',\n 'bert/encoder/layer_1/intermediate/dense/bias',\n 'bert/encoder/layer_1/output/dense/kernel',\n 'bert/encoder/layer_1/output/dense/bias',\n 'bert/encoder/layer_1/output/LayerNorm/gamma',\n 'bert/encoder/layer_2/attention/self/query/kernel',\n 'bert/encoder/layer_2/attention/self/query/bias',\n 'bert/encoder/layer_2/attention/self/key/kernel',\n 'bert/encoder/layer_2/attention/self/key/bias',\n 'bert/encoder/layer_2/attention/self/value/kernel',\n 'bert/encoder/layer_2/attention/self/value/bias',\n 'bert/encoder/layer_2/attention/self/Softmax',\n 'bert/encoder/layer_2/attention/output/dense/kernel',\n 'bert/encoder/layer_2/attention/output/dense/bias',\n 'bert/encoder/layer_2/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_2/intermediate/dense/kernel',\n 'bert/encoder/layer_2/intermediate/dense/bias',\n 'bert/encoder/layer_2/output/dense/kernel',\n 'bert/encoder/layer_2/output/dense/bias',\n 'bert/encoder/layer_2/output/LayerNorm/gamma',\n 'bert/encoder/layer_3/attention/self/query/kernel',\n 'bert/encoder/layer_3/attention/self/query/bias',\n 'bert/encoder/layer_3/attention/self/key/kernel',\n 'bert/encoder/layer_3/attention/self/key/bias',\n 'bert/encoder/layer_3/attention/self/value/kernel',\n 'bert/encoder/layer_3/attention/self/value/bias',\n 'bert/encoder/layer_3/attention/self/Softmax',\n 'bert/encoder/layer_3/attention/output/dense/kernel',\n 'bert/encoder/layer_3/attention/output/dense/bias',\n 'bert/encoder/layer_3/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_3/intermediate/dense/kernel',\n 'bert/encoder/layer_3/intermediate/dense/bias',\n 'bert/encoder/layer_3/output/dense/kernel',\n 'bert/encoder/layer_3/output/dense/bias',\n 'bert/encoder/layer_3/output/LayerNorm/gamma',\n 'bert/encoder/layer_4/attention/self/query/kernel',\n 'bert/encoder/layer_4/attention/self/query/bias',\n 'bert/encoder/layer_4/attention/self/key/kernel',\n 'bert/encoder/layer_4/attention/self/key/bias',\n 'bert/encoder/layer_4/attention/self/value/kernel',\n 'bert/encoder/layer_4/attention/self/value/bias',\n 'bert/encoder/layer_4/attention/self/Softmax',\n 'bert/encoder/layer_4/attention/output/dense/kernel',\n 'bert/encoder/layer_4/attention/output/dense/bias',\n 'bert/encoder/layer_4/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_4/intermediate/dense/kernel',\n 'bert/encoder/layer_4/intermediate/dense/bias',\n 'bert/encoder/layer_4/output/dense/kernel',\n 'bert/encoder/layer_4/output/dense/bias',\n 'bert/encoder/layer_4/output/LayerNorm/gamma',\n 'bert/encoder/layer_5/attention/self/query/kernel',\n 'bert/encoder/layer_5/attention/self/query/bias',\n 'bert/encoder/layer_5/attention/self/key/kernel',\n 'bert/encoder/layer_5/attention/self/key/bias',\n 'bert/encoder/layer_5/attention/self/value/kernel',\n 'bert/encoder/layer_5/attention/self/value/bias',\n 'bert/encoder/layer_5/attention/self/Softmax',\n 'bert/encoder/layer_5/attention/output/dense/kernel',\n 'bert/encoder/layer_5/attention/output/dense/bias',\n 'bert/encoder/layer_5/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_5/intermediate/dense/kernel',\n 'bert/encoder/layer_5/intermediate/dense/bias',\n 'bert/encoder/layer_5/output/dense/kernel',\n 'bert/encoder/layer_5/output/dense/bias',\n 'bert/encoder/layer_5/output/LayerNorm/gamma',\n 'bert/pooler/dense/kernel',\n 'bert/pooler/dense/bias',\n 'dense/kernel',\n 'dense/bias',\n 'logits_seq',\n 'logits',\n 'gradients/bert/encoder/layer_5/attention/self/Softmax_grad/mul',\n 'gradients/bert/encoder/layer_5/attention/self/Softmax_grad/Sum/reduction_indices',\n 'gradients/bert/encoder/layer_5/attention/self/Softmax_grad/Sum',\n 'gradients/bert/encoder/layer_5/attention/self/Softmax_grad/sub',\n 'gradients/bert/encoder/layer_5/attention/self/Softmax_grad/mul_1',\n 'gradients/bert/encoder/layer_4/attention/self/Softmax_grad/mul',\n 'gradients/bert/encoder/layer_4/attention/self/Softmax_grad/Sum/reduction_indices',\n 'gradients/bert/encoder/layer_4/attention/self/Softmax_grad/Sum',\n 'gradients/bert/encoder/layer_4/attention/self/Softmax_grad/sub',\n 'gradients/bert/encoder/layer_4/attention/self/Softmax_grad/mul_1',\n 'gradients/bert/encoder/layer_3/attention/self/Softmax_grad/mul',\n 'gradients/bert/encoder/layer_3/attention/self/Softmax_grad/Sum/reduction_indices',\n 'gradients/bert/encoder/layer_3/attention/self/Softmax_grad/Sum',\n 'gradients/bert/encoder/layer_3/attention/self/Softmax_grad/sub',\n 'gradients/bert/encoder/layer_3/attention/self/Softmax_grad/mul_1',\n 'gradients/bert/encoder/layer_2/attention/self/Softmax_grad/mul',\n 'gradients/bert/encoder/layer_2/attention/self/Softmax_grad/Sum/reduction_indices',\n 'gradients/bert/encoder/layer_2/attention/self/Softmax_grad/Sum',\n 'gradients/bert/encoder/layer_2/attention/self/Softmax_grad/sub',\n 'gradients/bert/encoder/layer_2/attention/self/Softmax_grad/mul_1',\n 'gradients/bert/encoder/layer_1/attention/self/Softmax_grad/mul',\n 'gradients/bert/encoder/layer_1/attention/self/Softmax_grad/Sum/reduction_indices',\n 'gradients/bert/encoder/layer_1/attention/self/Softmax_grad/Sum',\n 'gradients/bert/encoder/layer_1/attention/self/Softmax_grad/sub',\n 'gradients/bert/encoder/layer_1/attention/self/Softmax_grad/mul_1',\n 'gradients/bert/encoder/layer_0/attention/self/Softmax_grad/mul',\n 'gradients/bert/encoder/layer_0/attention/self/Softmax_grad/Sum/reduction_indices',\n 'gradients/bert/encoder/layer_0/attention/self/Softmax_grad/Sum',\n 'gradients/bert/encoder/layer_0/attention/self/Softmax_grad/sub',\n 'gradients/bert/encoder/layer_0/attention/self/Softmax_grad/mul_1']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def freeze_graph(model_dir, output_node_names):\n\n    if not tf.gfile.Exists(model_dir):\n        raise AssertionError(\n            \"Export directory doesn't exists. Please specify an export \"\n            'directory: %s' % model_dir\n        )\n\n    checkpoint = tf.train.get_checkpoint_state(model_dir)\n    input_checkpoint = checkpoint.model_checkpoint_path\n\n    absolute_model_dir = '/'.join(input_checkpoint.split('/')[:-1])\n    output_graph = absolute_model_dir + '/frozen_model.pb'\n    clear_devices = True\n    with tf.Session(graph = tf.Graph()) as sess:\n        saver = tf.train.import_meta_graph(\n            input_checkpoint + '.meta', clear_devices = clear_devices\n        )\n        saver.restore(sess, input_checkpoint)\n        output_graph_def = tf.graph_util.convert_variables_to_constants(\n            sess,\n            tf.get_default_graph().as_graph_def(),\n            output_node_names.split(','),\n        )\n        with tf.gfile.GFile(output_graph, 'wb') as f:\n            f.write(output_graph_def.SerializeToString())\n        print('%d ops in the final graph.' % len(output_graph_def.node))","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"saver = tf.train.Saver(tf.trainable_variables())\nsaver.save(sess, 'bert-small-toxicity/model.ckpt')","execution_count":22,"outputs":[{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"'bert-small-toxicity/model.ckpt'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"freeze_graph('bert-small-toxicity', strings)","execution_count":23,"outputs":[{"output_type":"stream","text":"3061 ops in the final graph.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import boto3\n\nbucketName = 'huseinhouse-storage'\nKey = 'bert-small-toxicity/frozen_model.pb'\noutPutname = \"v27/toxicicity/bert-small-toxicity.pb\"\n\ns3 = boto3.client('s3',\n                 aws_access_key_id='',\n                 aws_secret_access_key='')\ns3.upload_file(Key,bucketName,outPutname)","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}