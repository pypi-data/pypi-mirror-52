{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip3 install bert-tensorflow sentencepiece boto3","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting bert-tensorflow\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n\u001b[K     |████████████████████████████████| 71kB 8.9MB/s eta 0:00:011\n\u001b[?25hRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (0.1.82)\nRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (1.9.194)\nRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from bert-tensorflow) (1.12.0)\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3) (0.9.4)\nRequirement already satisfied: botocore<1.13.0,>=1.12.194 in /opt/conda/lib/python3.6/site-packages (from boto3) (1.12.194)\nRequirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from boto3) (0.2.1)\nRequirement already satisfied: urllib3<1.26,>=1.20; python_version >= \"3.4\" in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.194->boto3) (1.24.2)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.194->boto3) (2.8.0)\nRequirement already satisfied: docutils<0.15,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.194->boto3) (0.14)\nInstalling collected packages: bert-tensorflow\nSuccessfully installed bert-tensorflow-1.0.1\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nimport json\nimport bert\nfrom bert import run_classifier\nfrom bert import optimization\nfrom bert import tokenization\nfrom bert import modeling\nimport numpy as np\nimport tensorflow as tf","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_SEQ_LENGTH = 100","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget http://s3-ap-southeast-1.amazonaws.com/huseinhouse-storage/bert-bahasa/bert-bahasa-base.tar.gz\n!tar -zxf bert-bahasa-base.tar.gz","execution_count":4,"outputs":[{"output_type":"stream","text":"--2019-08-04 05:39:01--  http://s3-ap-southeast-1.amazonaws.com/huseinhouse-storage/bert-bahasa/bert-bahasa-base.tar.gz\nResolving s3-ap-southeast-1.amazonaws.com (s3-ap-southeast-1.amazonaws.com)... 52.219.32.230\nConnecting to s3-ap-southeast-1.amazonaws.com (s3-ap-southeast-1.amazonaws.com)|52.219.32.230|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 433961043 (414M) [binary/octet-stream]\nSaving to: ‘bert-bahasa-base.tar.gz’\n\nbert-bahasa-base.ta 100%[===================>] 413.86M  10.4MB/s    in 43s     \n\n2019-08-04 05:39:45 (9.67 MB/s) - ‘bert-bahasa-base.tar.gz’ saved [433961043/433961043]\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget https://raw.githubusercontent.com/huseinzol05/Malaya-Dataset/master/subjectivity/subjectivity-negative-bm.txt\n!wget https://raw.githubusercontent.com/huseinzol05/Malaya-Dataset/master/subjectivity/subjectivity-positive-bm.txt","execution_count":5,"outputs":[{"output_type":"stream","text":"--2019-08-04 05:39:52--  https://raw.githubusercontent.com/huseinzol05/Malaya-Dataset/master/subjectivity/subjectivity-negative-bm.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 700425 (684K) [text/plain]\nSaving to: ‘subjectivity-negative-bm.txt’\n\nsubjectivity-negati 100%[===================>] 684.01K  --.-KB/s    in 0.02s   \n\n2019-08-04 05:39:54 (35.2 MB/s) - ‘subjectivity-negative-bm.txt’ saved [700425/700425]\n\n--2019-08-04 05:39:54--  https://raw.githubusercontent.com/huseinzol05/Malaya-Dataset/master/subjectivity/subjectivity-positive-bm.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 651624 (636K) [text/plain]\nSaving to: ‘subjectivity-positive-bm.txt’\n\nsubjectivity-positi 100%[===================>] 636.35K  --.-KB/s    in 0.02s   \n\n2019-08-04 05:39:55 (32.0 MB/s) - ‘subjectivity-positive-bm.txt’ saved [651624/651624]\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('subjectivity-negative-bm.txt','r') as fopen:\n    texts = fopen.read().split('\\n')\nlabels = [0] * len(texts)\n\nwith open('subjectivity-positive-bm.txt','r') as fopen:\n    positive_texts = fopen.read().split('\\n')\nlabels += [1] * len(positive_texts)\ntexts += positive_texts\n\nassert len(labels) == len(texts)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import unicodedata\nimport six\nfrom functools import partial\n\nSPIECE_UNDERLINE = '▁'\n\ndef preprocess_text(inputs, lower=False, remove_space=True, keep_accents=False):\n  if remove_space:\n    outputs = ' '.join(inputs.strip().split())\n  else:\n    outputs = inputs\n  outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\n\n  if six.PY2 and isinstance(outputs, str):\n    outputs = outputs.decode('utf-8')\n\n  if not keep_accents:\n    outputs = unicodedata.normalize('NFKD', outputs)\n    outputs = ''.join([c for c in outputs if not unicodedata.combining(c)])\n  if lower:\n    outputs = outputs.lower()\n\n  return outputs\n\n\ndef encode_pieces(sp_model, text, return_unicode=True, sample=False):\n  # return_unicode is used only for py2\n\n  # note(zhiliny): in some systems, sentencepiece only accepts str for py2\n  if six.PY2 and isinstance(text, unicode):\n    text = text.encode('utf-8')\n\n  if not sample:\n    pieces = sp_model.EncodeAsPieces(text)\n  else:\n    pieces = sp_model.SampleEncodeAsPieces(text, 64, 0.1)\n  new_pieces = []\n  for piece in pieces:\n    if len(piece) > 1 and piece[-1] == ',' and piece[-2].isdigit():\n      cur_pieces = sp_model.EncodeAsPieces(\n          piece[:-1].replace(SPIECE_UNDERLINE, ''))\n      if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n        if len(cur_pieces[0]) == 1:\n          cur_pieces = cur_pieces[1:]\n        else:\n          cur_pieces[0] = cur_pieces[0][1:]\n      cur_pieces.append(piece[-1])\n      new_pieces.extend(cur_pieces)\n    else:\n      new_pieces.append(piece)\n\n  # note(zhiliny): convert back to unicode for py2\n  if six.PY2 and return_unicode:\n    ret_pieces = []\n    for piece in new_pieces:\n      if isinstance(piece, str):\n        piece = piece.decode('utf-8')\n      ret_pieces.append(piece)\n    new_pieces = ret_pieces\n\n  return new_pieces\n\n\ndef encode_ids(sp_model, text, sample=False):\n  pieces = encode_pieces(sp_model, text, return_unicode=False, sample=sample)\n  ids = [sp_model.PieceToId(piece) for piece in pieces]\n  return ids","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sentencepiece as spm\n\nsp_model = spm.SentencePieceProcessor()\nsp_model.Load('bert-bahasa-base/sp10m.cased.v4.model')\n\nwith open('bert-bahasa-base/sp10m.cased.v4.vocab') as fopen:\n    v = fopen.read().split('\\n')[:-1]\nv = [i.split('\\t') for i in v]\nv = {i[0]: i[1] for i in v}\n\nclass Tokenizer:\n    def __init__(self, v):\n        self.vocab = v\n        pass\n    \n    def tokenize(self, string):\n        return encode_pieces(sp_model, string, return_unicode=False, sample=False)\n    \n    def convert_tokens_to_ids(self, tokens):\n        return [sp_model.PieceToId(piece) for piece in tokens]\n    \n    def convert_ids_to_tokens(self, ids):\n        return [sp_model.IdToPiece(i) for i in ids]\n    \ntokenizer = Tokenizer(v)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BERT_INIT_CHKPNT = 'bert-bahasa-base/model.ckpt'\nBERT_CONFIG = 'bert-bahasa-base/bert_config.json'","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_ids, input_masks, segment_ids = [], [], []\n\nfor text in tqdm(texts):\n    tokens_a = tokenizer.tokenize(text)\n    if len(tokens_a) > MAX_SEQ_LENGTH - 2:\n        tokens_a = tokens_a[:(MAX_SEQ_LENGTH - 2)]\n    tokens = [\"<cls>\"] + tokens_a + [\"<sep>\"]\n    segment_id = [0] * len(tokens)\n    input_id = tokenizer.convert_tokens_to_ids(tokens)\n    input_mask = [1] * len(input_id)\n    padding = [0] * (MAX_SEQ_LENGTH - len(input_id))\n    input_id += padding\n    input_mask += padding\n    segment_id += padding\n    \n    input_ids.append(input_id)\n    input_masks.append(input_mask)\n    segment_ids.append(segment_id)","execution_count":10,"outputs":[{"output_type":"stream","text":"100%|██████████| 9962/9962 [00:01<00:00, 8847.29it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_config = modeling.BertConfig.from_json_file(BERT_CONFIG)\n\nepoch = 10\nbatch_size = 60\nwarmup_proportion = 0.1\nnum_train_steps = int(len(texts) / batch_size * epoch)\nnum_warmup_steps = int(num_train_steps * warmup_proportion)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model:\n    def __init__(\n        self,\n        dimension_output,\n        learning_rate = 2e-5,\n    ):\n        self.X = tf.placeholder(tf.int32, [None, None])\n        self.Y = tf.placeholder(tf.int32, [None])\n        \n        model = modeling.BertModel(\n            config=bert_config,\n            is_training=False,\n            input_ids=self.X,\n            use_one_hot_embeddings=False)\n        \n        output_layer = model.get_sequence_output()\n        self.logits_seq = tf.layers.dense(output_layer, dimension_output)\n        self.logits_seq = tf.identity(self.logits_seq, name = 'logits_seq')\n        self.logits = self.logits_seq[:, 0]\n        self.logits = tf.identity(self.logits, name = 'logits')\n        \n        self.cost = tf.reduce_mean(\n            tf.nn.sparse_softmax_cross_entropy_with_logits(\n                logits = self.logits, labels = self.Y\n            )\n        )\n        \n        self.optimizer = optimization.create_optimizer(self.cost, learning_rate, \n                                                       num_train_steps, num_warmup_steps, False)\n        correct_pred = tf.equal(\n            tf.argmax(self.logits, 1, output_type = tf.int32), self.Y\n        )\n        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dimension_output = 2\nlearning_rate = 2e-5\n\ntf.reset_default_graph()\nsess = tf.InteractiveSession()\nmodel = Model(\n    dimension_output,\n    learning_rate\n)\n\nsess.run(tf.global_variables_initializer())\nvar_lists = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = 'bert')\nsaver = tf.train.Saver(var_list = var_lists)\nsaver.restore(sess, BERT_INIT_CHKPNT)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_input_ids, test_input_ids, train_Y, test_Y = train_test_split(\n    input_ids, labels, test_size = 0.2\n)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\n\nEARLY_STOPPING, CURRENT_CHECKPOINT, CURRENT_ACC, EPOCH = 3, 0, 0, 0\n\nwhile True:\n    lasttime = time.time()\n    if CURRENT_CHECKPOINT == EARLY_STOPPING:\n        print('break epoch:%d\\n' % (EPOCH))\n        break\n\n    train_acc, train_loss, test_acc, test_loss = 0, 0, 0, 0\n    pbar = tqdm(\n        range(0, len(train_input_ids), batch_size), desc = 'train minibatch loop'\n    )\n    for i in pbar:\n        index = min(i + batch_size, len(train_input_ids))\n        batch_x = train_input_ids[i: index]\n        batch_y = train_Y[i: index]\n        acc, cost, _ = sess.run(\n            [model.accuracy, model.cost, model.optimizer],\n            feed_dict = {\n                model.Y: batch_y,\n                model.X: batch_x,\n            },\n        )\n        assert not np.isnan(cost)\n        train_loss += cost\n        train_acc += acc\n        pbar.set_postfix(cost = cost, accuracy = acc)\n        \n    pbar = tqdm(range(0, len(test_input_ids), batch_size), desc = 'test minibatch loop')\n    for i in pbar:\n        index = min(i + batch_size, len(test_input_ids))\n        batch_x = test_input_ids[i: index]\n        batch_y = test_Y[i: index]\n        acc, cost = sess.run(\n            [model.accuracy, model.cost],\n            feed_dict = {\n                model.Y: batch_y,\n                model.X: batch_x,\n            },\n        )\n        test_loss += cost\n        test_acc += acc\n        pbar.set_postfix(cost = cost, accuracy = acc)\n\n    train_loss /= len(train_input_ids) / batch_size\n    train_acc /= len(train_input_ids) / batch_size\n    test_loss /= len(test_input_ids) / batch_size\n    test_acc /= len(test_input_ids) / batch_size\n\n    if test_acc > CURRENT_ACC:\n        print(\n            'epoch: %d, pass acc: %f, current acc: %f'\n            % (EPOCH, CURRENT_ACC, test_acc)\n        )\n        CURRENT_ACC = test_acc\n        CURRENT_CHECKPOINT = 0\n    else:\n        CURRENT_CHECKPOINT += 1\n        \n    print('time taken:', time.time() - lasttime)\n    print(\n        'epoch: %d, training loss: %f, training acc: %f, valid loss: %f, valid acc: %f\\n'\n        % (EPOCH, train_loss, train_acc, test_loss, test_acc)\n    )\n    EPOCH += 1","execution_count":15,"outputs":[{"output_type":"stream","text":"train minibatch loop: 100%|██████████| 133/133 [01:33<00:00,  1.58it/s, accuracy=0.918, cost=0.191] \ntest minibatch loop: 100%|██████████| 34/34 [00:07<00:00,  4.36it/s, accuracy=0.923, cost=0.325]\ntrain minibatch loop:   0%|          | 0/133 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"epoch: 0, pass acc: 0.000000, current acc: 0.866726\ntime taken: 101.02019572257996\nepoch: 0, training loss: 0.461680, training acc: 0.793337, valid loss: 0.475811, valid acc: 0.866726\n\n","name":"stdout"},{"output_type":"stream","text":"train minibatch loop: 100%|██████████| 133/133 [01:28<00:00,  1.57it/s, accuracy=1, cost=0.00572]   \ntest minibatch loop: 100%|██████████| 34/34 [00:07<00:00,  4.58it/s, accuracy=1, cost=0.0563]    \ntrain minibatch loop:   0%|          | 0/133 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"epoch: 1, pass acc: 0.866726, current acc: 0.922730\ntime taken: 96.03491616249084\nepoch: 1, training loss: 0.201081, training acc: 0.925838, valid loss: 0.366465, valid acc: 0.922730\n\n","name":"stdout"},{"output_type":"stream","text":"train minibatch loop: 100%|██████████| 133/133 [01:28<00:00,  1.58it/s, accuracy=0.98, cost=0.0532] \ntest minibatch loop: 100%|██████████| 34/34 [00:07<00:00,  4.58it/s, accuracy=0.846, cost=0.184]\ntrain minibatch loop:   0%|          | 0/133 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"epoch: 2, pass acc: 0.922730, current acc: 0.931144\ntime taken: 96.01744174957275\nepoch: 2, training loss: 0.084891, training acc: 0.974373, valid loss: 0.378299, valid acc: 0.931144\n\n","name":"stdout"},{"output_type":"stream","text":"train minibatch loop: 100%|██████████| 133/133 [01:28<00:00,  1.58it/s, accuracy=1, cost=1.27e-7]   \ntest minibatch loop: 100%|██████████| 34/34 [00:07<00:00,  4.58it/s, accuracy=0.923, cost=0.275] \ntrain minibatch loop:   0%|          | 0/133 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"epoch: 3, pass acc: 0.931144, current acc: 0.940986\ntime taken: 95.95812511444092\nepoch: 3, training loss: 0.027227, training acc: 0.992345, valid loss: 0.584236, valid acc: 0.940986\n\n","name":"stdout"},{"output_type":"stream","text":"train minibatch loop: 100%|██████████| 133/133 [01:28<00:00,  1.57it/s, accuracy=1, cost=1.39e-7]   \ntest minibatch loop: 100%|██████████| 34/34 [00:07<00:00,  4.58it/s, accuracy=0.923, cost=0.534] \ntrain minibatch loop:   0%|          | 0/133 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"time taken: 95.98251962661743\nepoch: 4, training loss: 0.013319, training acc: 0.997490, valid loss: 0.671565, valid acc: 0.935968\n\n","name":"stdout"},{"output_type":"stream","text":"train minibatch loop: 100%|██████████| 133/133 [01:28<00:00,  1.58it/s, accuracy=1, cost=2.12e-5]   \ntest minibatch loop: 100%|██████████| 34/34 [00:07<00:00,  4.58it/s, accuracy=0.846, cost=2.13] \ntrain minibatch loop:   0%|          | 0/133 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"time taken: 95.92377805709839\nepoch: 5, training loss: 0.000959, training acc: 1.001129, valid loss: 0.884473, valid acc: 0.929638\n\n","name":"stdout"},{"output_type":"stream","text":"train minibatch loop: 100%|██████████| 133/133 [01:28<00:00,  1.57it/s, accuracy=1, cost=3.04e-7] \ntest minibatch loop: 100%|██████████| 34/34 [00:07<00:00,  4.57it/s, accuracy=0.846, cost=1.69]  ","name":"stderr"},{"output_type":"stream","text":"time taken: 95.95624423027039\nepoch: 6, training loss: 0.000587, training acc: 1.001255, valid loss: 0.745756, valid acc: 0.939673\n\nbreak epoch:7\n\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"real_Y, predict_Y = [], []\n\npbar = tqdm(\n    range(0, len(test_input_ids), batch_size), desc = 'validation minibatch loop'\n)\nfor i in pbar:\n    index = min(i + batch_size, len(test_input_ids))\n    batch_x = test_input_ids[i: index]\n    batch_y = test_Y[i: index]\n    predict_Y += np.argmax(sess.run(model.logits,\n            feed_dict = {\n                model.Y: batch_y,\n                model.X: batch_x,\n            },\n    ), 1, ).tolist()\n    real_Y += batch_y","execution_count":16,"outputs":[{"output_type":"stream","text":"validation minibatch loop: 100%|██████████| 34/34 [00:07<00:00,  4.39it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\nprint(\n    metrics.classification_report(\n        real_Y, predict_Y, target_names = ['negative', 'positive'], digits = 6\n    )\n)","execution_count":17,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n    negative   0.906977  0.936000  0.921260      1000\n    positive   0.933403  0.903323  0.918117       993\n\n    accuracy                       0.919719      1993\n   macro avg   0.920190  0.919662  0.919688      1993\nweighted avg   0.920143  0.919719  0.919694      1993\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"strings = ','.join(\n    [\n        n.name\n        for n in tf.get_default_graph().as_graph_def().node\n        if ('Variable' in n.op\n        or 'Placeholder' in n.name\n        or 'logits' in n.name\n        or 'alphas' in n.name\n        or 'self/Softmax' in n.name)\n        and 'adam' not in n.name\n        and 'beta' not in n.name\n        and 'global_step' not in n.name\n    ]\n)\nstrings.split(',')","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"['Placeholder',\n 'Placeholder_1',\n 'bert/embeddings/word_embeddings',\n 'bert/embeddings/token_type_embeddings',\n 'bert/embeddings/position_embeddings',\n 'bert/embeddings/LayerNorm/gamma',\n 'bert/encoder/layer_0/attention/self/query/kernel',\n 'bert/encoder/layer_0/attention/self/query/bias',\n 'bert/encoder/layer_0/attention/self/key/kernel',\n 'bert/encoder/layer_0/attention/self/key/bias',\n 'bert/encoder/layer_0/attention/self/value/kernel',\n 'bert/encoder/layer_0/attention/self/value/bias',\n 'bert/encoder/layer_0/attention/self/Softmax',\n 'bert/encoder/layer_0/attention/output/dense/kernel',\n 'bert/encoder/layer_0/attention/output/dense/bias',\n 'bert/encoder/layer_0/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_0/intermediate/dense/kernel',\n 'bert/encoder/layer_0/intermediate/dense/bias',\n 'bert/encoder/layer_0/output/dense/kernel',\n 'bert/encoder/layer_0/output/dense/bias',\n 'bert/encoder/layer_0/output/LayerNorm/gamma',\n 'bert/encoder/layer_1/attention/self/query/kernel',\n 'bert/encoder/layer_1/attention/self/query/bias',\n 'bert/encoder/layer_1/attention/self/key/kernel',\n 'bert/encoder/layer_1/attention/self/key/bias',\n 'bert/encoder/layer_1/attention/self/value/kernel',\n 'bert/encoder/layer_1/attention/self/value/bias',\n 'bert/encoder/layer_1/attention/self/Softmax',\n 'bert/encoder/layer_1/attention/output/dense/kernel',\n 'bert/encoder/layer_1/attention/output/dense/bias',\n 'bert/encoder/layer_1/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_1/intermediate/dense/kernel',\n 'bert/encoder/layer_1/intermediate/dense/bias',\n 'bert/encoder/layer_1/output/dense/kernel',\n 'bert/encoder/layer_1/output/dense/bias',\n 'bert/encoder/layer_1/output/LayerNorm/gamma',\n 'bert/encoder/layer_2/attention/self/query/kernel',\n 'bert/encoder/layer_2/attention/self/query/bias',\n 'bert/encoder/layer_2/attention/self/key/kernel',\n 'bert/encoder/layer_2/attention/self/key/bias',\n 'bert/encoder/layer_2/attention/self/value/kernel',\n 'bert/encoder/layer_2/attention/self/value/bias',\n 'bert/encoder/layer_2/attention/self/Softmax',\n 'bert/encoder/layer_2/attention/output/dense/kernel',\n 'bert/encoder/layer_2/attention/output/dense/bias',\n 'bert/encoder/layer_2/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_2/intermediate/dense/kernel',\n 'bert/encoder/layer_2/intermediate/dense/bias',\n 'bert/encoder/layer_2/output/dense/kernel',\n 'bert/encoder/layer_2/output/dense/bias',\n 'bert/encoder/layer_2/output/LayerNorm/gamma',\n 'bert/encoder/layer_3/attention/self/query/kernel',\n 'bert/encoder/layer_3/attention/self/query/bias',\n 'bert/encoder/layer_3/attention/self/key/kernel',\n 'bert/encoder/layer_3/attention/self/key/bias',\n 'bert/encoder/layer_3/attention/self/value/kernel',\n 'bert/encoder/layer_3/attention/self/value/bias',\n 'bert/encoder/layer_3/attention/self/Softmax',\n 'bert/encoder/layer_3/attention/output/dense/kernel',\n 'bert/encoder/layer_3/attention/output/dense/bias',\n 'bert/encoder/layer_3/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_3/intermediate/dense/kernel',\n 'bert/encoder/layer_3/intermediate/dense/bias',\n 'bert/encoder/layer_3/output/dense/kernel',\n 'bert/encoder/layer_3/output/dense/bias',\n 'bert/encoder/layer_3/output/LayerNorm/gamma',\n 'bert/encoder/layer_4/attention/self/query/kernel',\n 'bert/encoder/layer_4/attention/self/query/bias',\n 'bert/encoder/layer_4/attention/self/key/kernel',\n 'bert/encoder/layer_4/attention/self/key/bias',\n 'bert/encoder/layer_4/attention/self/value/kernel',\n 'bert/encoder/layer_4/attention/self/value/bias',\n 'bert/encoder/layer_4/attention/self/Softmax',\n 'bert/encoder/layer_4/attention/output/dense/kernel',\n 'bert/encoder/layer_4/attention/output/dense/bias',\n 'bert/encoder/layer_4/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_4/intermediate/dense/kernel',\n 'bert/encoder/layer_4/intermediate/dense/bias',\n 'bert/encoder/layer_4/output/dense/kernel',\n 'bert/encoder/layer_4/output/dense/bias',\n 'bert/encoder/layer_4/output/LayerNorm/gamma',\n 'bert/encoder/layer_5/attention/self/query/kernel',\n 'bert/encoder/layer_5/attention/self/query/bias',\n 'bert/encoder/layer_5/attention/self/key/kernel',\n 'bert/encoder/layer_5/attention/self/key/bias',\n 'bert/encoder/layer_5/attention/self/value/kernel',\n 'bert/encoder/layer_5/attention/self/value/bias',\n 'bert/encoder/layer_5/attention/self/Softmax',\n 'bert/encoder/layer_5/attention/output/dense/kernel',\n 'bert/encoder/layer_5/attention/output/dense/bias',\n 'bert/encoder/layer_5/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_5/intermediate/dense/kernel',\n 'bert/encoder/layer_5/intermediate/dense/bias',\n 'bert/encoder/layer_5/output/dense/kernel',\n 'bert/encoder/layer_5/output/dense/bias',\n 'bert/encoder/layer_5/output/LayerNorm/gamma',\n 'bert/encoder/layer_6/attention/self/query/kernel',\n 'bert/encoder/layer_6/attention/self/query/bias',\n 'bert/encoder/layer_6/attention/self/key/kernel',\n 'bert/encoder/layer_6/attention/self/key/bias',\n 'bert/encoder/layer_6/attention/self/value/kernel',\n 'bert/encoder/layer_6/attention/self/value/bias',\n 'bert/encoder/layer_6/attention/self/Softmax',\n 'bert/encoder/layer_6/attention/output/dense/kernel',\n 'bert/encoder/layer_6/attention/output/dense/bias',\n 'bert/encoder/layer_6/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_6/intermediate/dense/kernel',\n 'bert/encoder/layer_6/intermediate/dense/bias',\n 'bert/encoder/layer_6/output/dense/kernel',\n 'bert/encoder/layer_6/output/dense/bias',\n 'bert/encoder/layer_6/output/LayerNorm/gamma',\n 'bert/encoder/layer_7/attention/self/query/kernel',\n 'bert/encoder/layer_7/attention/self/query/bias',\n 'bert/encoder/layer_7/attention/self/key/kernel',\n 'bert/encoder/layer_7/attention/self/key/bias',\n 'bert/encoder/layer_7/attention/self/value/kernel',\n 'bert/encoder/layer_7/attention/self/value/bias',\n 'bert/encoder/layer_7/attention/self/Softmax',\n 'bert/encoder/layer_7/attention/output/dense/kernel',\n 'bert/encoder/layer_7/attention/output/dense/bias',\n 'bert/encoder/layer_7/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_7/intermediate/dense/kernel',\n 'bert/encoder/layer_7/intermediate/dense/bias',\n 'bert/encoder/layer_7/output/dense/kernel',\n 'bert/encoder/layer_7/output/dense/bias',\n 'bert/encoder/layer_7/output/LayerNorm/gamma',\n 'bert/encoder/layer_8/attention/self/query/kernel',\n 'bert/encoder/layer_8/attention/self/query/bias',\n 'bert/encoder/layer_8/attention/self/key/kernel',\n 'bert/encoder/layer_8/attention/self/key/bias',\n 'bert/encoder/layer_8/attention/self/value/kernel',\n 'bert/encoder/layer_8/attention/self/value/bias',\n 'bert/encoder/layer_8/attention/self/Softmax',\n 'bert/encoder/layer_8/attention/output/dense/kernel',\n 'bert/encoder/layer_8/attention/output/dense/bias',\n 'bert/encoder/layer_8/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_8/intermediate/dense/kernel',\n 'bert/encoder/layer_8/intermediate/dense/bias',\n 'bert/encoder/layer_8/output/dense/kernel',\n 'bert/encoder/layer_8/output/dense/bias',\n 'bert/encoder/layer_8/output/LayerNorm/gamma',\n 'bert/encoder/layer_9/attention/self/query/kernel',\n 'bert/encoder/layer_9/attention/self/query/bias',\n 'bert/encoder/layer_9/attention/self/key/kernel',\n 'bert/encoder/layer_9/attention/self/key/bias',\n 'bert/encoder/layer_9/attention/self/value/kernel',\n 'bert/encoder/layer_9/attention/self/value/bias',\n 'bert/encoder/layer_9/attention/self/Softmax',\n 'bert/encoder/layer_9/attention/output/dense/kernel',\n 'bert/encoder/layer_9/attention/output/dense/bias',\n 'bert/encoder/layer_9/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_9/intermediate/dense/kernel',\n 'bert/encoder/layer_9/intermediate/dense/bias',\n 'bert/encoder/layer_9/output/dense/kernel',\n 'bert/encoder/layer_9/output/dense/bias',\n 'bert/encoder/layer_9/output/LayerNorm/gamma',\n 'bert/encoder/layer_10/attention/self/query/kernel',\n 'bert/encoder/layer_10/attention/self/query/bias',\n 'bert/encoder/layer_10/attention/self/key/kernel',\n 'bert/encoder/layer_10/attention/self/key/bias',\n 'bert/encoder/layer_10/attention/self/value/kernel',\n 'bert/encoder/layer_10/attention/self/value/bias',\n 'bert/encoder/layer_10/attention/self/Softmax',\n 'bert/encoder/layer_10/attention/output/dense/kernel',\n 'bert/encoder/layer_10/attention/output/dense/bias',\n 'bert/encoder/layer_10/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_10/intermediate/dense/kernel',\n 'bert/encoder/layer_10/intermediate/dense/bias',\n 'bert/encoder/layer_10/output/dense/kernel',\n 'bert/encoder/layer_10/output/dense/bias',\n 'bert/encoder/layer_10/output/LayerNorm/gamma',\n 'bert/encoder/layer_11/attention/self/query/kernel',\n 'bert/encoder/layer_11/attention/self/query/bias',\n 'bert/encoder/layer_11/attention/self/key/kernel',\n 'bert/encoder/layer_11/attention/self/key/bias',\n 'bert/encoder/layer_11/attention/self/value/kernel',\n 'bert/encoder/layer_11/attention/self/value/bias',\n 'bert/encoder/layer_11/attention/self/Softmax',\n 'bert/encoder/layer_11/attention/output/dense/kernel',\n 'bert/encoder/layer_11/attention/output/dense/bias',\n 'bert/encoder/layer_11/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_11/intermediate/dense/kernel',\n 'bert/encoder/layer_11/intermediate/dense/bias',\n 'bert/encoder/layer_11/output/dense/kernel',\n 'bert/encoder/layer_11/output/dense/bias',\n 'bert/encoder/layer_11/output/LayerNorm/gamma',\n 'bert/pooler/dense/kernel',\n 'bert/pooler/dense/bias',\n 'dense/kernel',\n 'dense/bias',\n 'logits_seq',\n 'logits',\n 'gradients/bert/encoder/layer_11/attention/self/Softmax_grad/mul',\n 'gradients/bert/encoder/layer_11/attention/self/Softmax_grad/Sum/reduction_indices',\n 'gradients/bert/encoder/layer_11/attention/self/Softmax_grad/Sum',\n 'gradients/bert/encoder/layer_11/attention/self/Softmax_grad/sub',\n 'gradients/bert/encoder/layer_11/attention/self/Softmax_grad/mul_1',\n 'gradients/bert/encoder/layer_10/attention/self/Softmax_grad/mul',\n 'gradients/bert/encoder/layer_10/attention/self/Softmax_grad/Sum/reduction_indices',\n 'gradients/bert/encoder/layer_10/attention/self/Softmax_grad/Sum',\n 'gradients/bert/encoder/layer_10/attention/self/Softmax_grad/sub',\n 'gradients/bert/encoder/layer_10/attention/self/Softmax_grad/mul_1',\n 'gradients/bert/encoder/layer_9/attention/self/Softmax_grad/mul',\n 'gradients/bert/encoder/layer_9/attention/self/Softmax_grad/Sum/reduction_indices',\n 'gradients/bert/encoder/layer_9/attention/self/Softmax_grad/Sum',\n 'gradients/bert/encoder/layer_9/attention/self/Softmax_grad/sub',\n 'gradients/bert/encoder/layer_9/attention/self/Softmax_grad/mul_1',\n 'gradients/bert/encoder/layer_8/attention/self/Softmax_grad/mul',\n 'gradients/bert/encoder/layer_8/attention/self/Softmax_grad/Sum/reduction_indices',\n 'gradients/bert/encoder/layer_8/attention/self/Softmax_grad/Sum',\n 'gradients/bert/encoder/layer_8/attention/self/Softmax_grad/sub',\n 'gradients/bert/encoder/layer_8/attention/self/Softmax_grad/mul_1',\n 'gradients/bert/encoder/layer_7/attention/self/Softmax_grad/mul',\n 'gradients/bert/encoder/layer_7/attention/self/Softmax_grad/Sum/reduction_indices',\n 'gradients/bert/encoder/layer_7/attention/self/Softmax_grad/Sum',\n 'gradients/bert/encoder/layer_7/attention/self/Softmax_grad/sub',\n 'gradients/bert/encoder/layer_7/attention/self/Softmax_grad/mul_1',\n 'gradients/bert/encoder/layer_6/attention/self/Softmax_grad/mul',\n 'gradients/bert/encoder/layer_6/attention/self/Softmax_grad/Sum/reduction_indices',\n 'gradients/bert/encoder/layer_6/attention/self/Softmax_grad/Sum',\n 'gradients/bert/encoder/layer_6/attention/self/Softmax_grad/sub',\n 'gradients/bert/encoder/layer_6/attention/self/Softmax_grad/mul_1',\n 'gradients/bert/encoder/layer_5/attention/self/Softmax_grad/mul',\n 'gradients/bert/encoder/layer_5/attention/self/Softmax_grad/Sum/reduction_indices',\n 'gradients/bert/encoder/layer_5/attention/self/Softmax_grad/Sum',\n 'gradients/bert/encoder/layer_5/attention/self/Softmax_grad/sub',\n 'gradients/bert/encoder/layer_5/attention/self/Softmax_grad/mul_1',\n 'gradients/bert/encoder/layer_4/attention/self/Softmax_grad/mul',\n 'gradients/bert/encoder/layer_4/attention/self/Softmax_grad/Sum/reduction_indices',\n 'gradients/bert/encoder/layer_4/attention/self/Softmax_grad/Sum',\n 'gradients/bert/encoder/layer_4/attention/self/Softmax_grad/sub',\n 'gradients/bert/encoder/layer_4/attention/self/Softmax_grad/mul_1',\n 'gradients/bert/encoder/layer_3/attention/self/Softmax_grad/mul',\n 'gradients/bert/encoder/layer_3/attention/self/Softmax_grad/Sum/reduction_indices',\n 'gradients/bert/encoder/layer_3/attention/self/Softmax_grad/Sum',\n 'gradients/bert/encoder/layer_3/attention/self/Softmax_grad/sub',\n 'gradients/bert/encoder/layer_3/attention/self/Softmax_grad/mul_1',\n 'gradients/bert/encoder/layer_2/attention/self/Softmax_grad/mul',\n 'gradients/bert/encoder/layer_2/attention/self/Softmax_grad/Sum/reduction_indices',\n 'gradients/bert/encoder/layer_2/attention/self/Softmax_grad/Sum',\n 'gradients/bert/encoder/layer_2/attention/self/Softmax_grad/sub',\n 'gradients/bert/encoder/layer_2/attention/self/Softmax_grad/mul_1',\n 'gradients/bert/encoder/layer_1/attention/self/Softmax_grad/mul',\n 'gradients/bert/encoder/layer_1/attention/self/Softmax_grad/Sum/reduction_indices',\n 'gradients/bert/encoder/layer_1/attention/self/Softmax_grad/Sum',\n 'gradients/bert/encoder/layer_1/attention/self/Softmax_grad/sub',\n 'gradients/bert/encoder/layer_1/attention/self/Softmax_grad/mul_1',\n 'gradients/bert/encoder/layer_0/attention/self/Softmax_grad/mul',\n 'gradients/bert/encoder/layer_0/attention/self/Softmax_grad/Sum/reduction_indices',\n 'gradients/bert/encoder/layer_0/attention/self/Softmax_grad/Sum',\n 'gradients/bert/encoder/layer_0/attention/self/Softmax_grad/sub',\n 'gradients/bert/encoder/layer_0/attention/self/Softmax_grad/mul_1']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def freeze_graph(model_dir, output_node_names):\n\n    if not tf.gfile.Exists(model_dir):\n        raise AssertionError(\n            \"Export directory doesn't exists. Please specify an export \"\n            'directory: %s' % model_dir\n        )\n\n    checkpoint = tf.train.get_checkpoint_state(model_dir)\n    input_checkpoint = checkpoint.model_checkpoint_path\n\n    absolute_model_dir = '/'.join(input_checkpoint.split('/')[:-1])\n    output_graph = absolute_model_dir + '/frozen_model.pb'\n    clear_devices = True\n    with tf.Session(graph = tf.Graph()) as sess:\n        saver = tf.train.import_meta_graph(\n            input_checkpoint + '.meta', clear_devices = clear_devices\n        )\n        saver.restore(sess, input_checkpoint)\n        output_graph_def = tf.graph_util.convert_variables_to_constants(\n            sess,\n            tf.get_default_graph().as_graph_def(),\n            output_node_names.split(','),\n        )\n        with tf.gfile.GFile(output_graph, 'wb') as f:\n            f.write(output_graph_def.SerializeToString())\n        print('%d ops in the final graph.' % len(output_graph_def.node))","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"saver = tf.train.Saver(tf.trainable_variables())\nsaver.save(sess, 'bert-base-subjectivity/model.ckpt')","execution_count":20,"outputs":[{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"'bert-base-subjectivity/model.ckpt'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"freeze_graph('bert-base-subjectivity', strings)","execution_count":22,"outputs":[{"output_type":"stream","text":"5891 ops in the final graph.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_graph(frozen_graph_filename):\n    with tf.gfile.GFile(frozen_graph_filename, 'rb') as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n    with tf.Graph().as_default() as graph:\n        tf.import_graph_def(graph_def)\n    return graph","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = load_graph('bert-base-subjectivity/frozen_model.pb')\nx = g.get_tensor_by_name('import/Placeholder:0')\nlogits = g.get_tensor_by_name('import/logits:0')\ntest_sess = tf.InteractiveSession(graph = g)\nresult = test_sess.run(tf.nn.softmax(logits), feed_dict = {x: [input_id]})\nresult","execution_count":24,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n  warnings.warn('An interactive session is already active. This can '\n","name":"stderr"},{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"array([[1.919756e-09, 1.000000e+00]], dtype=float32)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import boto3\n\nbucketName = 'huseinhouse-storage'\nKey = 'bert-base-subjectivity/frozen_model.pb'\noutPutname = \"v27/subjective/bert-base-subjective.pb\"\n\ns3 = boto3.client('s3',\n                 aws_access_key_id='',\n                 aws_secret_access_key='')\n\ns3.upload_file(Key,bucketName,outPutname)","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}