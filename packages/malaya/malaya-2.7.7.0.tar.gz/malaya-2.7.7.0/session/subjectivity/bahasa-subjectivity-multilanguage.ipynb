{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip3 install bert-tensorflow sentencepiece boto3","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting bert-tensorflow\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n\u001b[K     |████████████████████████████████| 71kB 8.0MB/s eta 0:00:011\n\u001b[?25hRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (0.1.82)\nRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (1.9.194)\nRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from bert-tensorflow) (1.12.0)\nRequirement already satisfied: botocore<1.13.0,>=1.12.194 in /opt/conda/lib/python3.6/site-packages (from boto3) (1.12.194)\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3) (0.9.4)\nRequirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from boto3) (0.2.1)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.194->boto3) (2.8.0)\nRequirement already satisfied: docutils<0.15,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.194->boto3) (0.14)\nRequirement already satisfied: urllib3<1.26,>=1.20; python_version >= \"3.4\" in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.194->boto3) (1.24.2)\nInstalling collected packages: bert-tensorflow\nSuccessfully installed bert-tensorflow-1.0.1\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nimport json\nimport bert\nfrom bert import run_classifier\nfrom bert import optimization\nfrom bert import tokenization\nfrom bert import modeling\nimport numpy as np\nimport tensorflow as tf","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_SEQ_LENGTH = 100","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\n!unzip multi_cased_L-12_H-768_A-12.zip","execution_count":4,"outputs":[{"output_type":"stream","text":"--2019-08-04 06:30:54--  https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\nResolving storage.googleapis.com (storage.googleapis.com)... 173.194.79.128, 2a00:1450:4013:c05::80\nConnecting to storage.googleapis.com (storage.googleapis.com)|173.194.79.128|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 662903077 (632M) [application/zip]\nSaving to: ‘multi_cased_L-12_H-768_A-12.zip’\n\nmulti_cased_L-12_H- 100%[===================>] 632.19M   134MB/s    in 4.9s    \n\n2019-08-04 06:30:59 (130 MB/s) - ‘multi_cased_L-12_H-768_A-12.zip’ saved [662903077/662903077]\n\nArchive:  multi_cased_L-12_H-768_A-12.zip\n   creating: multi_cased_L-12_H-768_A-12/\n  inflating: multi_cased_L-12_H-768_A-12/bert_model.ckpt.meta  \n  inflating: multi_cased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n  inflating: multi_cased_L-12_H-768_A-12/vocab.txt  \n  inflating: multi_cased_L-12_H-768_A-12/bert_model.ckpt.index  \n  inflating: multi_cased_L-12_H-768_A-12/bert_config.json  \n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"BERT_VOCAB = 'multi_cased_L-12_H-768_A-12/vocab.txt'\nBERT_INIT_CHKPNT = 'multi_cased_L-12_H-768_A-12/bert_model.ckpt'\nBERT_CONFIG = 'multi_cased_L-12_H-768_A-12/bert_config.json'","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget https://raw.githubusercontent.com/huseinzol05/Malaya-Dataset/master/subjectivity/subjectivity-negative-bm.txt\n!wget https://raw.githubusercontent.com/huseinzol05/Malaya-Dataset/master/subjectivity/subjectivity-positive-bm.txt","execution_count":6,"outputs":[{"output_type":"stream","text":"--2019-08-04 06:31:15--  https://raw.githubusercontent.com/huseinzol05/Malaya-Dataset/master/subjectivity/subjectivity-negative-bm.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 700425 (684K) [text/plain]\nSaving to: ‘subjectivity-negative-bm.txt’\n\nsubjectivity-negati 100%[===================>] 684.01K  --.-KB/s    in 0.02s   \n\n2019-08-04 06:31:15 (33.8 MB/s) - ‘subjectivity-negative-bm.txt’ saved [700425/700425]\n\n--2019-08-04 06:31:16--  https://raw.githubusercontent.com/huseinzol05/Malaya-Dataset/master/subjectivity/subjectivity-positive-bm.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 651624 (636K) [text/plain]\nSaving to: ‘subjectivity-positive-bm.txt’\n\nsubjectivity-positi 100%[===================>] 636.35K  --.-KB/s    in 0.02s   \n\n2019-08-04 06:31:16 (30.4 MB/s) - ‘subjectivity-positive-bm.txt’ saved [651624/651624]\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('subjectivity-negative-bm.txt','r') as fopen:\n    texts = fopen.read().split('\\n')\nlabels = [0] * len(texts)\n\nwith open('subjectivity-positive-bm.txt','r') as fopen:\n    positive_texts = fopen.read().split('\\n')\nlabels += [1] * len(positive_texts)\ntexts += positive_texts\n\nassert len(labels) == len(texts)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = tokenization.FullTokenizer(\n      vocab_file=BERT_VOCAB, do_lower_case=False)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_ids, input_masks, segment_ids = [], [], []\n\nfor text in tqdm(texts):\n    tokens_a = tokenizer.tokenize(text)\n    if len(tokens_a) > MAX_SEQ_LENGTH - 2:\n        tokens_a = tokens_a[:(MAX_SEQ_LENGTH - 2)]\n    tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n    segment_id = [0] * len(tokens)\n    input_id = tokenizer.convert_tokens_to_ids(tokens)\n    input_mask = [1] * len(input_id)\n    padding = [0] * (MAX_SEQ_LENGTH - len(input_id))\n    input_id += padding\n    input_mask += padding\n    segment_id += padding\n    \n    input_ids.append(input_id)\n    input_masks.append(input_mask)\n    segment_ids.append(segment_id)","execution_count":9,"outputs":[{"output_type":"stream","text":"100%|██████████| 9962/9962 [00:05<00:00, 1968.96it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_config = modeling.BertConfig.from_json_file(BERT_CONFIG)\n\nepoch = 10\nbatch_size = 60\nwarmup_proportion = 0.1\nnum_train_steps = int(len(texts) / batch_size * epoch)\nnum_warmup_steps = int(num_train_steps * warmup_proportion)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model:\n    def __init__(\n        self,\n        dimension_output,\n        learning_rate = 2e-5,\n    ):\n        self.X = tf.placeholder(tf.int32, [None, None])\n        self.Y = tf.placeholder(tf.int32, [None])\n        \n        model = modeling.BertModel(\n            config=bert_config,\n            is_training=False,\n            input_ids=self.X,\n            use_one_hot_embeddings=False)\n        \n        output_layer = model.get_sequence_output()\n        self.logits_seq = tf.layers.dense(output_layer, dimension_output)\n        self.logits_seq = tf.identity(self.logits_seq, name = 'logits_seq')\n        self.logits = self.logits_seq[:, 0]\n        self.logits = tf.identity(self.logits, name = 'logits')\n        \n        self.cost = tf.reduce_mean(\n            tf.nn.sparse_softmax_cross_entropy_with_logits(\n                logits = self.logits, labels = self.Y\n            )\n        )\n        \n        self.optimizer = optimization.create_optimizer(self.cost, learning_rate, \n                                                       num_train_steps, num_warmup_steps, False)\n        correct_pred = tf.equal(\n            tf.argmax(self.logits, 1, output_type = tf.int32), self.Y\n        )\n        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dimension_output = 2\nlearning_rate = 2e-5\n\ntf.reset_default_graph()\nsess = tf.InteractiveSession()\nmodel = Model(\n    dimension_output,\n    learning_rate\n)\n\nsess.run(tf.global_variables_initializer())\nvar_lists = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = 'bert')\nsaver = tf.train.Saver(var_list = var_lists)\nsaver.restore(sess, BERT_INIT_CHKPNT)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_input_ids, test_input_ids, train_Y, test_Y = train_test_split(\n    input_ids, labels, test_size = 0.2\n)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\n\nEARLY_STOPPING, CURRENT_CHECKPOINT, CURRENT_ACC, EPOCH = 2, 0, 0, 0\n\nwhile True:\n    lasttime = time.time()\n    if CURRENT_CHECKPOINT == EARLY_STOPPING:\n        print('break epoch:%d\\n' % (EPOCH))\n        break\n\n    train_acc, train_loss, test_acc, test_loss = 0, 0, 0, 0\n    pbar = tqdm(\n        range(0, len(train_input_ids), batch_size), desc = 'train minibatch loop'\n    )\n    for i in pbar:\n        index = min(i + batch_size, len(train_input_ids))\n        batch_x = train_input_ids[i: index]\n        batch_y = train_Y[i: index]\n        acc, cost, _ = sess.run(\n            [model.accuracy, model.cost, model.optimizer],\n            feed_dict = {\n                model.Y: batch_y,\n                model.X: batch_x,\n            },\n        )\n        assert not np.isnan(cost)\n        train_loss += cost\n        train_acc += acc\n        pbar.set_postfix(cost = cost, accuracy = acc)\n        \n    pbar = tqdm(range(0, len(test_input_ids), batch_size), desc = 'test minibatch loop')\n    for i in pbar:\n        index = min(i + batch_size, len(test_input_ids))\n        batch_x = test_input_ids[i: index]\n        batch_y = test_Y[i: index]\n        acc, cost = sess.run(\n            [model.accuracy, model.cost],\n            feed_dict = {\n                model.Y: batch_y,\n                model.X: batch_x,\n            },\n        )\n        test_loss += cost\n        test_acc += acc\n        pbar.set_postfix(cost = cost, accuracy = acc)\n\n    train_loss /= len(train_input_ids) / batch_size\n    train_acc /= len(train_input_ids) / batch_size\n    test_loss /= len(test_input_ids) / batch_size\n    test_acc /= len(test_input_ids) / batch_size\n\n    if test_acc > CURRENT_ACC:\n        print(\n            'epoch: %d, pass acc: %f, current acc: %f'\n            % (EPOCH, CURRENT_ACC, test_acc)\n        )\n        CURRENT_ACC = test_acc\n        CURRENT_CHECKPOINT = 0\n    else:\n        CURRENT_CHECKPOINT += 1\n        \n    print('time taken:', time.time() - lasttime)\n    print(\n        'epoch: %d, training loss: %f, training acc: %f, valid loss: %f, valid acc: %f\\n'\n        % (EPOCH, train_loss, train_acc, test_loss, test_acc)\n    )\n    EPOCH += 1","execution_count":14,"outputs":[{"output_type":"stream","text":"train minibatch loop: 100%|██████████| 133/133 [01:35<00:00,  1.53it/s, accuracy=0.816, cost=0.549]\ntest minibatch loop: 100%|██████████| 34/34 [00:07<00:00,  4.37it/s, accuracy=0.769, cost=0.48] \ntrain minibatch loop:   0%|          | 0/133 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"epoch: 0, pass acc: 0.000000, current acc: 0.893203\ntime taken: 102.92259740829468\nepoch: 0, training loss: 0.365062, training acc: 0.835108, valid loss: 0.353573, valid acc: 0.893203\n\n","name":"stdout"},{"output_type":"stream","text":"train minibatch loop: 100%|██████████| 133/133 [01:31<00:00,  1.53it/s, accuracy=0.959, cost=0.128] \ntest minibatch loop: 100%|██████████| 34/34 [00:07<00:00,  4.58it/s, accuracy=0.923, cost=0.675]\ntrain minibatch loop:   0%|          | 0/133 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"epoch: 1, pass acc: 0.893203, current acc: 0.929445\ntime taken: 98.57189965248108\nepoch: 1, training loss: 0.215213, training acc: 0.923773, valid loss: 0.266983, valid acc: 0.929445\n\n","name":"stdout"},{"output_type":"stream","text":"train minibatch loop: 100%|██████████| 133/133 [01:31<00:00,  1.53it/s, accuracy=0.98, cost=0.0997] \ntest minibatch loop: 100%|██████████| 34/34 [00:07<00:00,  4.58it/s, accuracy=0.846, cost=0.834]\ntrain minibatch loop:   0%|          | 0/133 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"epoch: 2, pass acc: 0.929445, current acc: 0.946698\ntime taken: 98.50553750991821\nepoch: 2, training loss: 0.085043, training acc: 0.973745, valid loss: 0.341698, valid acc: 0.946698\n\n","name":"stdout"},{"output_type":"stream","text":"train minibatch loop: 100%|██████████| 133/133 [01:31<00:00,  1.53it/s, accuracy=1, cost=0.00466]   \ntest minibatch loop: 100%|██████████| 34/34 [00:07<00:00,  4.58it/s, accuracy=0.923, cost=0.518]\ntrain minibatch loop:   0%|          | 0/133 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"time taken: 98.65616345405579\nepoch: 3, training loss: 0.030697, training acc: 0.992471, valid loss: 0.468959, valid acc: 0.946003\n\n","name":"stdout"},{"output_type":"stream","text":"train minibatch loop: 100%|██████████| 133/133 [01:31<00:00,  1.53it/s, accuracy=1, cost=0.00444]   \ntest minibatch loop: 100%|██████████| 34/34 [00:07<00:00,  4.58it/s, accuracy=0.923, cost=0.795]","name":"stderr"},{"output_type":"stream","text":"time taken: 98.52804589271545\nepoch: 4, training loss: 0.014014, training acc: 0.997239, valid loss: 0.689737, valid acc: 0.933459\n\nbreak epoch:5\n\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"real_Y, predict_Y = [], []\n\npbar = tqdm(\n    range(0, len(test_input_ids), batch_size), desc = 'validation minibatch loop'\n)\nfor i in pbar:\n    index = min(i + batch_size, len(test_input_ids))\n    batch_x = test_input_ids[i: index]\n    batch_y = test_Y[i: index]\n    predict_Y += np.argmax(sess.run(model.logits,\n            feed_dict = {\n                model.Y: batch_y,\n                model.X: batch_x,\n            },\n    ), 1, ).tolist()\n    real_Y += batch_y","execution_count":15,"outputs":[{"output_type":"stream","text":"validation minibatch loop: 100%|██████████| 34/34 [00:07<00:00,  4.36it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\nprint(\n    metrics.classification_report(\n        real_Y, predict_Y, target_names = ['negative', 'positive'], digits = 6\n    )\n)","execution_count":16,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n    negative   0.958380  0.859738  0.906383       991\n    positive   0.874094  0.963074  0.916429      1002\n\n    accuracy                       0.911691      1993\n   macro avg   0.916237  0.911406  0.911406      1993\nweighted avg   0.916005  0.911691  0.911434      1993\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"strings = ','.join(\n    [\n        n.name\n        for n in tf.get_default_graph().as_graph_def().node\n        if ('Variable' in n.op\n        or 'Placeholder' in n.name\n        or 'logits' in n.name\n        or 'alphas' in n.name\n        or 'self/Softmax' in n.name)\n        and 'adam' not in n.name\n        and 'beta' not in n.name\n        and 'global_step' not in n.name\n    ]\n)\nstrings.split(',')","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"['Placeholder',\n 'Placeholder_1',\n 'bert/embeddings/word_embeddings',\n 'bert/embeddings/token_type_embeddings',\n 'bert/embeddings/position_embeddings',\n 'bert/embeddings/LayerNorm/gamma',\n 'bert/encoder/layer_0/attention/self/query/kernel',\n 'bert/encoder/layer_0/attention/self/query/bias',\n 'bert/encoder/layer_0/attention/self/key/kernel',\n 'bert/encoder/layer_0/attention/self/key/bias',\n 'bert/encoder/layer_0/attention/self/value/kernel',\n 'bert/encoder/layer_0/attention/self/value/bias',\n 'bert/encoder/layer_0/attention/self/Softmax',\n 'bert/encoder/layer_0/attention/output/dense/kernel',\n 'bert/encoder/layer_0/attention/output/dense/bias',\n 'bert/encoder/layer_0/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_0/intermediate/dense/kernel',\n 'bert/encoder/layer_0/intermediate/dense/bias',\n 'bert/encoder/layer_0/output/dense/kernel',\n 'bert/encoder/layer_0/output/dense/bias',\n 'bert/encoder/layer_0/output/LayerNorm/gamma',\n 'bert/encoder/layer_1/attention/self/query/kernel',\n 'bert/encoder/layer_1/attention/self/query/bias',\n 'bert/encoder/layer_1/attention/self/key/kernel',\n 'bert/encoder/layer_1/attention/self/key/bias',\n 'bert/encoder/layer_1/attention/self/value/kernel',\n 'bert/encoder/layer_1/attention/self/value/bias',\n 'bert/encoder/layer_1/attention/self/Softmax',\n 'bert/encoder/layer_1/attention/output/dense/kernel',\n 'bert/encoder/layer_1/attention/output/dense/bias',\n 'bert/encoder/layer_1/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_1/intermediate/dense/kernel',\n 'bert/encoder/layer_1/intermediate/dense/bias',\n 'bert/encoder/layer_1/output/dense/kernel',\n 'bert/encoder/layer_1/output/dense/bias',\n 'bert/encoder/layer_1/output/LayerNorm/gamma',\n 'bert/encoder/layer_2/attention/self/query/kernel',\n 'bert/encoder/layer_2/attention/self/query/bias',\n 'bert/encoder/layer_2/attention/self/key/kernel',\n 'bert/encoder/layer_2/attention/self/key/bias',\n 'bert/encoder/layer_2/attention/self/value/kernel',\n 'bert/encoder/layer_2/attention/self/value/bias',\n 'bert/encoder/layer_2/attention/self/Softmax',\n 'bert/encoder/layer_2/attention/output/dense/kernel',\n 'bert/encoder/layer_2/attention/output/dense/bias',\n 'bert/encoder/layer_2/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_2/intermediate/dense/kernel',\n 'bert/encoder/layer_2/intermediate/dense/bias',\n 'bert/encoder/layer_2/output/dense/kernel',\n 'bert/encoder/layer_2/output/dense/bias',\n 'bert/encoder/layer_2/output/LayerNorm/gamma',\n 'bert/encoder/layer_3/attention/self/query/kernel',\n 'bert/encoder/layer_3/attention/self/query/bias',\n 'bert/encoder/layer_3/attention/self/key/kernel',\n 'bert/encoder/layer_3/attention/self/key/bias',\n 'bert/encoder/layer_3/attention/self/value/kernel',\n 'bert/encoder/layer_3/attention/self/value/bias',\n 'bert/encoder/layer_3/attention/self/Softmax',\n 'bert/encoder/layer_3/attention/output/dense/kernel',\n 'bert/encoder/layer_3/attention/output/dense/bias',\n 'bert/encoder/layer_3/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_3/intermediate/dense/kernel',\n 'bert/encoder/layer_3/intermediate/dense/bias',\n 'bert/encoder/layer_3/output/dense/kernel',\n 'bert/encoder/layer_3/output/dense/bias',\n 'bert/encoder/layer_3/output/LayerNorm/gamma',\n 'bert/encoder/layer_4/attention/self/query/kernel',\n 'bert/encoder/layer_4/attention/self/query/bias',\n 'bert/encoder/layer_4/attention/self/key/kernel',\n 'bert/encoder/layer_4/attention/self/key/bias',\n 'bert/encoder/layer_4/attention/self/value/kernel',\n 'bert/encoder/layer_4/attention/self/value/bias',\n 'bert/encoder/layer_4/attention/self/Softmax',\n 'bert/encoder/layer_4/attention/output/dense/kernel',\n 'bert/encoder/layer_4/attention/output/dense/bias',\n 'bert/encoder/layer_4/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_4/intermediate/dense/kernel',\n 'bert/encoder/layer_4/intermediate/dense/bias',\n 'bert/encoder/layer_4/output/dense/kernel',\n 'bert/encoder/layer_4/output/dense/bias',\n 'bert/encoder/layer_4/output/LayerNorm/gamma',\n 'bert/encoder/layer_5/attention/self/query/kernel',\n 'bert/encoder/layer_5/attention/self/query/bias',\n 'bert/encoder/layer_5/attention/self/key/kernel',\n 'bert/encoder/layer_5/attention/self/key/bias',\n 'bert/encoder/layer_5/attention/self/value/kernel',\n 'bert/encoder/layer_5/attention/self/value/bias',\n 'bert/encoder/layer_5/attention/self/Softmax',\n 'bert/encoder/layer_5/attention/output/dense/kernel',\n 'bert/encoder/layer_5/attention/output/dense/bias',\n 'bert/encoder/layer_5/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_5/intermediate/dense/kernel',\n 'bert/encoder/layer_5/intermediate/dense/bias',\n 'bert/encoder/layer_5/output/dense/kernel',\n 'bert/encoder/layer_5/output/dense/bias',\n 'bert/encoder/layer_5/output/LayerNorm/gamma',\n 'bert/encoder/layer_6/attention/self/query/kernel',\n 'bert/encoder/layer_6/attention/self/query/bias',\n 'bert/encoder/layer_6/attention/self/key/kernel',\n 'bert/encoder/layer_6/attention/self/key/bias',\n 'bert/encoder/layer_6/attention/self/value/kernel',\n 'bert/encoder/layer_6/attention/self/value/bias',\n 'bert/encoder/layer_6/attention/self/Softmax',\n 'bert/encoder/layer_6/attention/output/dense/kernel',\n 'bert/encoder/layer_6/attention/output/dense/bias',\n 'bert/encoder/layer_6/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_6/intermediate/dense/kernel',\n 'bert/encoder/layer_6/intermediate/dense/bias',\n 'bert/encoder/layer_6/output/dense/kernel',\n 'bert/encoder/layer_6/output/dense/bias',\n 'bert/encoder/layer_6/output/LayerNorm/gamma',\n 'bert/encoder/layer_7/attention/self/query/kernel',\n 'bert/encoder/layer_7/attention/self/query/bias',\n 'bert/encoder/layer_7/attention/self/key/kernel',\n 'bert/encoder/layer_7/attention/self/key/bias',\n 'bert/encoder/layer_7/attention/self/value/kernel',\n 'bert/encoder/layer_7/attention/self/value/bias',\n 'bert/encoder/layer_7/attention/self/Softmax',\n 'bert/encoder/layer_7/attention/output/dense/kernel',\n 'bert/encoder/layer_7/attention/output/dense/bias',\n 'bert/encoder/layer_7/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_7/intermediate/dense/kernel',\n 'bert/encoder/layer_7/intermediate/dense/bias',\n 'bert/encoder/layer_7/output/dense/kernel',\n 'bert/encoder/layer_7/output/dense/bias',\n 'bert/encoder/layer_7/output/LayerNorm/gamma',\n 'bert/encoder/layer_8/attention/self/query/kernel',\n 'bert/encoder/layer_8/attention/self/query/bias',\n 'bert/encoder/layer_8/attention/self/key/kernel',\n 'bert/encoder/layer_8/attention/self/key/bias',\n 'bert/encoder/layer_8/attention/self/value/kernel',\n 'bert/encoder/layer_8/attention/self/value/bias',\n 'bert/encoder/layer_8/attention/self/Softmax',\n 'bert/encoder/layer_8/attention/output/dense/kernel',\n 'bert/encoder/layer_8/attention/output/dense/bias',\n 'bert/encoder/layer_8/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_8/intermediate/dense/kernel',\n 'bert/encoder/layer_8/intermediate/dense/bias',\n 'bert/encoder/layer_8/output/dense/kernel',\n 'bert/encoder/layer_8/output/dense/bias',\n 'bert/encoder/layer_8/output/LayerNorm/gamma',\n 'bert/encoder/layer_9/attention/self/query/kernel',\n 'bert/encoder/layer_9/attention/self/query/bias',\n 'bert/encoder/layer_9/attention/self/key/kernel',\n 'bert/encoder/layer_9/attention/self/key/bias',\n 'bert/encoder/layer_9/attention/self/value/kernel',\n 'bert/encoder/layer_9/attention/self/value/bias',\n 'bert/encoder/layer_9/attention/self/Softmax',\n 'bert/encoder/layer_9/attention/output/dense/kernel',\n 'bert/encoder/layer_9/attention/output/dense/bias',\n 'bert/encoder/layer_9/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_9/intermediate/dense/kernel',\n 'bert/encoder/layer_9/intermediate/dense/bias',\n 'bert/encoder/layer_9/output/dense/kernel',\n 'bert/encoder/layer_9/output/dense/bias',\n 'bert/encoder/layer_9/output/LayerNorm/gamma',\n 'bert/encoder/layer_10/attention/self/query/kernel',\n 'bert/encoder/layer_10/attention/self/query/bias',\n 'bert/encoder/layer_10/attention/self/key/kernel',\n 'bert/encoder/layer_10/attention/self/key/bias',\n 'bert/encoder/layer_10/attention/self/value/kernel',\n 'bert/encoder/layer_10/attention/self/value/bias',\n 'bert/encoder/layer_10/attention/self/Softmax',\n 'bert/encoder/layer_10/attention/output/dense/kernel',\n 'bert/encoder/layer_10/attention/output/dense/bias',\n 'bert/encoder/layer_10/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_10/intermediate/dense/kernel',\n 'bert/encoder/layer_10/intermediate/dense/bias',\n 'bert/encoder/layer_10/output/dense/kernel',\n 'bert/encoder/layer_10/output/dense/bias',\n 'bert/encoder/layer_10/output/LayerNorm/gamma',\n 'bert/encoder/layer_11/attention/self/query/kernel',\n 'bert/encoder/layer_11/attention/self/query/bias',\n 'bert/encoder/layer_11/attention/self/key/kernel',\n 'bert/encoder/layer_11/attention/self/key/bias',\n 'bert/encoder/layer_11/attention/self/value/kernel',\n 'bert/encoder/layer_11/attention/self/value/bias',\n 'bert/encoder/layer_11/attention/self/Softmax',\n 'bert/encoder/layer_11/attention/output/dense/kernel',\n 'bert/encoder/layer_11/attention/output/dense/bias',\n 'bert/encoder/layer_11/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_11/intermediate/dense/kernel',\n 'bert/encoder/layer_11/intermediate/dense/bias',\n 'bert/encoder/layer_11/output/dense/kernel',\n 'bert/encoder/layer_11/output/dense/bias',\n 'bert/encoder/layer_11/output/LayerNorm/gamma',\n 'bert/pooler/dense/kernel',\n 'bert/pooler/dense/bias',\n 'dense/kernel',\n 'dense/bias',\n 'logits_seq',\n 'logits',\n 'gradients/bert/encoder/layer_11/attention/self/Softmax_grad/mul',\n 'gradients/bert/encoder/layer_11/attention/self/Softmax_grad/Sum/reduction_indices',\n 'gradients/bert/encoder/layer_11/attention/self/Softmax_grad/Sum',\n 'gradients/bert/encoder/layer_11/attention/self/Softmax_grad/sub',\n 'gradients/bert/encoder/layer_11/attention/self/Softmax_grad/mul_1',\n 'gradients/bert/encoder/layer_10/attention/self/Softmax_grad/mul',\n 'gradients/bert/encoder/layer_10/attention/self/Softmax_grad/Sum/reduction_indices',\n 'gradients/bert/encoder/layer_10/attention/self/Softmax_grad/Sum',\n 'gradients/bert/encoder/layer_10/attention/self/Softmax_grad/sub',\n 'gradients/bert/encoder/layer_10/attention/self/Softmax_grad/mul_1',\n 'gradients/bert/encoder/layer_9/attention/self/Softmax_grad/mul',\n 'gradients/bert/encoder/layer_9/attention/self/Softmax_grad/Sum/reduction_indices',\n 'gradients/bert/encoder/layer_9/attention/self/Softmax_grad/Sum',\n 'gradients/bert/encoder/layer_9/attention/self/Softmax_grad/sub',\n 'gradients/bert/encoder/layer_9/attention/self/Softmax_grad/mul_1',\n 'gradients/bert/encoder/layer_8/attention/self/Softmax_grad/mul',\n 'gradients/bert/encoder/layer_8/attention/self/Softmax_grad/Sum/reduction_indices',\n 'gradients/bert/encoder/layer_8/attention/self/Softmax_grad/Sum',\n 'gradients/bert/encoder/layer_8/attention/self/Softmax_grad/sub',\n 'gradients/bert/encoder/layer_8/attention/self/Softmax_grad/mul_1',\n 'gradients/bert/encoder/layer_7/attention/self/Softmax_grad/mul',\n 'gradients/bert/encoder/layer_7/attention/self/Softmax_grad/Sum/reduction_indices',\n 'gradients/bert/encoder/layer_7/attention/self/Softmax_grad/Sum',\n 'gradients/bert/encoder/layer_7/attention/self/Softmax_grad/sub',\n 'gradients/bert/encoder/layer_7/attention/self/Softmax_grad/mul_1',\n 'gradients/bert/encoder/layer_6/attention/self/Softmax_grad/mul',\n 'gradients/bert/encoder/layer_6/attention/self/Softmax_grad/Sum/reduction_indices',\n 'gradients/bert/encoder/layer_6/attention/self/Softmax_grad/Sum',\n 'gradients/bert/encoder/layer_6/attention/self/Softmax_grad/sub',\n 'gradients/bert/encoder/layer_6/attention/self/Softmax_grad/mul_1',\n 'gradients/bert/encoder/layer_5/attention/self/Softmax_grad/mul',\n 'gradients/bert/encoder/layer_5/attention/self/Softmax_grad/Sum/reduction_indices',\n 'gradients/bert/encoder/layer_5/attention/self/Softmax_grad/Sum',\n 'gradients/bert/encoder/layer_5/attention/self/Softmax_grad/sub',\n 'gradients/bert/encoder/layer_5/attention/self/Softmax_grad/mul_1',\n 'gradients/bert/encoder/layer_4/attention/self/Softmax_grad/mul',\n 'gradients/bert/encoder/layer_4/attention/self/Softmax_grad/Sum/reduction_indices',\n 'gradients/bert/encoder/layer_4/attention/self/Softmax_grad/Sum',\n 'gradients/bert/encoder/layer_4/attention/self/Softmax_grad/sub',\n 'gradients/bert/encoder/layer_4/attention/self/Softmax_grad/mul_1',\n 'gradients/bert/encoder/layer_3/attention/self/Softmax_grad/mul',\n 'gradients/bert/encoder/layer_3/attention/self/Softmax_grad/Sum/reduction_indices',\n 'gradients/bert/encoder/layer_3/attention/self/Softmax_grad/Sum',\n 'gradients/bert/encoder/layer_3/attention/self/Softmax_grad/sub',\n 'gradients/bert/encoder/layer_3/attention/self/Softmax_grad/mul_1',\n 'gradients/bert/encoder/layer_2/attention/self/Softmax_grad/mul',\n 'gradients/bert/encoder/layer_2/attention/self/Softmax_grad/Sum/reduction_indices',\n 'gradients/bert/encoder/layer_2/attention/self/Softmax_grad/Sum',\n 'gradients/bert/encoder/layer_2/attention/self/Softmax_grad/sub',\n 'gradients/bert/encoder/layer_2/attention/self/Softmax_grad/mul_1',\n 'gradients/bert/encoder/layer_1/attention/self/Softmax_grad/mul',\n 'gradients/bert/encoder/layer_1/attention/self/Softmax_grad/Sum/reduction_indices',\n 'gradients/bert/encoder/layer_1/attention/self/Softmax_grad/Sum',\n 'gradients/bert/encoder/layer_1/attention/self/Softmax_grad/sub',\n 'gradients/bert/encoder/layer_1/attention/self/Softmax_grad/mul_1',\n 'gradients/bert/encoder/layer_0/attention/self/Softmax_grad/mul',\n 'gradients/bert/encoder/layer_0/attention/self/Softmax_grad/Sum/reduction_indices',\n 'gradients/bert/encoder/layer_0/attention/self/Softmax_grad/Sum',\n 'gradients/bert/encoder/layer_0/attention/self/Softmax_grad/sub',\n 'gradients/bert/encoder/layer_0/attention/self/Softmax_grad/mul_1']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def freeze_graph(model_dir, output_node_names):\n\n    if not tf.gfile.Exists(model_dir):\n        raise AssertionError(\n            \"Export directory doesn't exists. Please specify an export \"\n            'directory: %s' % model_dir\n        )\n\n    checkpoint = tf.train.get_checkpoint_state(model_dir)\n    input_checkpoint = checkpoint.model_checkpoint_path\n\n    absolute_model_dir = '/'.join(input_checkpoint.split('/')[:-1])\n    output_graph = absolute_model_dir + '/frozen_model.pb'\n    clear_devices = True\n    with tf.Session(graph = tf.Graph()) as sess:\n        saver = tf.train.import_meta_graph(\n            input_checkpoint + '.meta', clear_devices = clear_devices\n        )\n        saver.restore(sess, input_checkpoint)\n        output_graph_def = tf.graph_util.convert_variables_to_constants(\n            sess,\n            tf.get_default_graph().as_graph_def(),\n            output_node_names.split(','),\n        )\n        with tf.gfile.GFile(output_graph, 'wb') as f:\n            f.write(output_graph_def.SerializeToString())\n        print('%d ops in the final graph.' % len(output_graph_def.node))","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"saver = tf.train.Saver(tf.trainable_variables())\nsaver.save(sess, 'bert-multilanguage-subjectivity/model.ckpt')","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"'bert-multilanguage-subjectivity/model.ckpt'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"freeze_graph('bert-multilanguage-subjectivity', strings)","execution_count":20,"outputs":[{"output_type":"stream","text":"5891 ops in the final graph.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_graph(frozen_graph_filename):\n    with tf.gfile.GFile(frozen_graph_filename, 'rb') as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n    with tf.Graph().as_default() as graph:\n        tf.import_graph_def(graph_def)\n    return graph","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = load_graph('bert-multilanguage-subjectivity/frozen_model.pb')\nx = g.get_tensor_by_name('import/Placeholder:0')\nlogits = g.get_tensor_by_name('import/logits:0')\ntest_sess = tf.InteractiveSession(graph = g)\nresult = test_sess.run(tf.nn.softmax(logits), feed_dict = {x: [input_id]})\nresult","execution_count":22,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n  warnings.warn('An interactive session is already active. This can '\n","name":"stderr"},{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"array([[2.0942152e-05, 9.9997902e-01]], dtype=float32)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import boto3\n\nbucketName = 'huseinhouse-storage'\nKey = 'bert-multilanguage-subjectivity/frozen_model.pb'\noutPutname = \"v27/subjective/bert-multilanguage-subjective.pb\"\n\ns3 = boto3.client('s3',\n                 aws_access_key_id='',\n                 aws_secret_access_key='')\n\ns3.upload_file(Key,bucketName,outPutname)","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}