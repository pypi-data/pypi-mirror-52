{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!wget https://huseinhouse-storage.s3-ap-southeast-1.amazonaws.com/bert-bahasa/dictionary-pos.json\n!wget https://huseinhouse-storage.s3-ap-southeast-1.amazonaws.com/bert-bahasa/session-pos.pkl","execution_count":1,"outputs":[{"output_type":"stream","text":"--2019-08-04 16:23:07--  https://huseinhouse-storage.s3-ap-southeast-1.amazonaws.com/bert-bahasa/dictionary-pos.json\nResolving huseinhouse-storage.s3-ap-southeast-1.amazonaws.com (huseinhouse-storage.s3-ap-southeast-1.amazonaws.com)... 52.219.32.139\nConnecting to huseinhouse-storage.s3-ap-southeast-1.amazonaws.com (huseinhouse-storage.s3-ap-southeast-1.amazonaws.com)|52.219.32.139|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 825070 (806K) [binary/octet-stream]\nSaving to: ‘dictionary-pos.json’\n\ndictionary-pos.json 100%[===================>] 805.73K   138KB/s    in 5.8s    \n\n2019-08-04 16:23:14 (138 KB/s) - ‘dictionary-pos.json’ saved [825070/825070]\n\n--2019-08-04 16:23:14--  https://huseinhouse-storage.s3-ap-southeast-1.amazonaws.com/bert-bahasa/session-pos.pkl\nResolving huseinhouse-storage.s3-ap-southeast-1.amazonaws.com (huseinhouse-storage.s3-ap-southeast-1.amazonaws.com)... 52.219.32.147\nConnecting to huseinhouse-storage.s3-ap-southeast-1.amazonaws.com (huseinhouse-storage.s3-ap-southeast-1.amazonaws.com)|52.219.32.147|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 97458750 (93M) [binary/octet-stream]\nSaving to: ‘session-pos.pkl’\n\nsession-pos.pkl     100%[===================>]  92.94M  10.2MB/s    in 11s     \n\n2019-08-04 16:23:27 (8.35 MB/s) - ‘session-pos.pkl’ saved [97458750/97458750]\n\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pickle\nimport json\nimport tensorflow as tf\nimport numpy as np","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('session-pos.pkl', 'rb') as fopen:\n    data = pickle.load(fopen)\ndata.keys()","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"dict_keys(['train_X', 'test_X', 'train_Y', 'test_Y'])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X = data['train_X']\ntest_X = data['test_X']\ntrain_Y = data['train_Y']\ntest_Y = data['test_Y']","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('dictionary-pos.json') as fopen:\n    dictionary = json.load(fopen)\ndictionary.keys()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"dict_keys(['word2idx', 'idx2word', 'tag2idx', 'idx2tag', 'char2idx'])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"word2idx = dictionary['word2idx']\nidx2word = {int(k): v for k, v in dictionary['idx2word'].items()}\ntag2idx = dictionary['tag2idx']\nidx2tag = {int(k): v for k, v in dictionary['idx2tag'].items()}\nchar2idx = dictionary['char2idx']","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(zip([idx2word[d] for d in train_X[-1]], [idx2tag[d] for d in train_Y[-1]]))","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"[('-', 'PUNCT'),\n ('film', 'NOUN'),\n ('yang', 'PRON'),\n ('dibuatnya', 'VERB'),\n ('akan', 'ADV'),\n ('segera', 'ADV'),\n ('tayang', 'VERB'),\n ('.', 'PUNCT'),\n ('Jadi', 'ADV'),\n ('dicoba', 'VERB'),\n ('untuk', 'ADP'),\n ('menjawab', 'VERB'),\n ('pertanyaan-pertanyaan', 'NOUN'),\n ('seperti', 'ADP'),\n ('kebutuhan', 'NOUN'),\n ('apa', 'PRON'),\n ('yang', 'PRON'),\n ('dicoba', 'VERB'),\n ('dipuaskan', 'VERB'),\n ('oleh', 'ADP'),\n ('seseorang', 'NOUN'),\n ('?', 'PUNCT'),\n ('Kamu', 'PRON'),\n ('selalu', 'ADV'),\n ('bertanya', 'VERB'),\n ('apa', 'PRON'),\n ('itu', 'DET'),\n ('Pi', 'PROPN'),\n ('?', 'PUNCT'),\n ('Bagaimana', 'PRON'),\n ('di', 'ADP'),\n ('Indonesia', 'PROPN'),\n ('?', 'PUNCT'),\n ('Grimes', 'PROPN'),\n ('merupakan', 'VERB'),\n ('sebuah', 'DET'),\n ('di', 'ADP'),\n ('Dale', 'PROPN'),\n (',', 'PUNCT'),\n ('Alabama', 'PROPN'),\n (',', 'PUNCT'),\n ('Amerika', 'PROPN'),\n ('Serikat', 'PROPN'),\n ('.', 'PUNCT'),\n ('Sampul', 'NOUN'),\n ('dari', 'ADP'),\n ('dua', 'NUM'),\n ('singel', 'NOUN'),\n ('pertama', 'NUM'),\n ('difoto', 'VERB')]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_char_seq(batch):\n    x = [[len(idx2word[i]) for i in k] for k in batch]\n    maxlen = max([j for i in x for j in i])\n    temp = np.zeros((batch.shape[0],batch.shape[1],maxlen),dtype=np.int32)\n    for i in range(batch.shape[0]):\n        for k in range(batch.shape[1]):\n            for no, c in enumerate(idx2word[batch[i,k]]):\n                temp[i,k,-1-no] = char2idx[c]\n    return temp","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generate_char_seq(data['train_X'][:10]).shape","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"(10, 50, 12)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model:\n    def __init__(\n        self,\n        dim_word,\n        dim_char,\n        dropout,\n        learning_rate,\n        hidden_size_char,\n        hidden_size_word,\n        num_layers,\n    ):\n        def cells(size, reuse = False):\n            return tf.contrib.rnn.DropoutWrapper(\n                tf.nn.rnn_cell.LSTMCell(\n                    size,\n                    initializer = tf.orthogonal_initializer(),\n                    reuse = reuse,\n                ),\n                output_keep_prob = dropout,\n            )\n\n        self.word_ids = tf.placeholder(tf.int32, shape = [None, None])\n        self.char_ids = tf.placeholder(tf.int32, shape = [None, None, None])\n        self.labels = tf.placeholder(tf.int32, shape = [None, None])\n        self.maxlen = tf.shape(self.word_ids)[1]\n        self.lengths = tf.count_nonzero(self.word_ids, 1)\n        \n        self.word_embeddings = tf.Variable(\n            tf.truncated_normal(\n                [len(word2idx), dim_word], stddev = 1.0 / np.sqrt(dim_word)\n            )\n        )\n        self.char_embeddings = tf.Variable(\n            tf.truncated_normal(\n                [len(char2idx), dim_char], stddev = 1.0 / np.sqrt(dim_char)\n            )\n        )\n\n        word_embedded = tf.nn.embedding_lookup(\n            self.word_embeddings, self.word_ids\n        )\n        char_embedded = tf.nn.embedding_lookup(\n            self.char_embeddings, self.char_ids\n        )\n        s = tf.shape(char_embedded)\n        char_embedded = tf.reshape(\n            char_embedded, shape = [s[0] * s[1], s[-2], dim_char]\n        )\n        \n        for n in range(num_layers):\n            (out_fw, out_bw), (\n                state_fw,\n                state_bw,\n            ) = tf.nn.bidirectional_dynamic_rnn(\n                cell_fw = cells(hidden_size_char),\n                cell_bw = cells(hidden_size_char),\n                inputs = char_embedded,\n                dtype = tf.float32,\n                scope = 'bidirectional_rnn_char_%d' % (n),\n            )\n            char_embedded = tf.concat((out_fw, out_bw), 2)\n        output = tf.reshape(\n            char_embedded[:, -1], shape = [s[0], s[1], 2 * hidden_size_char]\n        )\n        word_embedded = tf.concat([word_embedded, output], axis = -1)\n\n        for n in range(num_layers):\n            (out_fw, out_bw), (\n                state_fw,\n                state_bw,\n            ) = tf.nn.bidirectional_dynamic_rnn(\n                cell_fw = cells(hidden_size_word),\n                cell_bw = cells(hidden_size_word),\n                inputs = word_embedded,\n                dtype = tf.float32,\n                scope = 'bidirectional_rnn_word_%d' % (n),\n            )\n            word_embedded = tf.concat((out_fw, out_bw), 2)\n\n        logits = tf.layers.dense(word_embedded, len(idx2tag))\n        y_t = self.labels\n        log_likelihood, transition_params = tf.contrib.crf.crf_log_likelihood(\n            logits, y_t, self.lengths\n        )\n        self.cost = tf.reduce_mean(-log_likelihood)\n        self.optimizer = tf.train.AdamOptimizer(\n            learning_rate = learning_rate\n        ).minimize(self.cost)\n        mask = tf.sequence_mask(self.lengths, maxlen = self.maxlen)\n        self.tags_seq, tags_score = tf.contrib.crf.crf_decode(\n            logits, transition_params, self.lengths\n        )\n        self.tags_seq = tf.identity(self.tags_seq, name = 'logits')\n\n        y_t = tf.cast(y_t, tf.int32)\n        self.prediction = tf.boolean_mask(self.tags_seq, mask)\n        mask_label = tf.boolean_mask(y_t, mask)\n        correct_pred = tf.equal(self.prediction, mask_label)\n        correct_index = tf.cast(correct_pred, tf.float32)\n        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.reset_default_graph()\nsess = tf.InteractiveSession()\n\ndim_word = 128\ndim_char = 256\ndropout = 0.8\nlearning_rate = 1e-3\nhidden_size_char = 128\nhidden_size_word = 128\nnum_layers = 2\nbatch_size = 64\n\nmodel = Model(dim_word,dim_char,dropout,learning_rate,hidden_size_char,hidden_size_word,num_layers)\nsess.run(tf.global_variables_initializer())","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"string = 'KUALA LUMPUR: Sempena sambutan Aidilfitri minggu depan, Perdana Menteri Tun Dr Mahathir Mohamad dan Menteri Pengangkutan Anthony Loke Siew Fook menitipkan pesanan khas kepada orang ramai yang mahu pulang ke kampung halaman masing-masing. Dalam video pendek terbitan Jabatan Keselamatan Jalan Raya (JKJR) itu, Dr Mahathir menasihati mereka supaya berhenti berehat dan tidur sebentar  sekiranya mengantuk ketika memandu.'\n\nimport re\n\ndef entities_textcleaning(string, lowering = False):\n    \"\"\"\n    use by entities recognition, pos recognition and dependency parsing\n    \"\"\"\n    string = re.sub('[^A-Za-z0-9\\-\\/() ]+', ' ', string)\n    string = re.sub(r'[ ]+', ' ', string).strip()\n    original_string = string.split()\n    if lowering:\n        string = string.lower()\n    string = [\n        (original_string[no], word.title() if word.isupper() else word)\n        for no, word in enumerate(string.split())\n        if len(word)\n    ]\n    return [s[0] for s in string], [s[1] for s in string]\n\ndef char_str_idx(corpus, dic, UNK = 0):\n    maxlen = max([len(i) for i in corpus])\n    X = np.zeros((len(corpus), maxlen))\n    for i in range(len(corpus)):\n        for no, k in enumerate(corpus[i][:maxlen][::-1]):\n            val = dic[k] if k in dic else UNK\n            X[i, -1 - no] = val\n    return X","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nimport time\n\nEARLY_STOPPING, CURRENT_CHECKPOINT, CURRENT_ACC, EPOCH = 2, 0, 0, 0\n\nwhile True:\n    lasttime = time.time()\n    if CURRENT_CHECKPOINT == EARLY_STOPPING:\n        print('break epoch:%d\\n' % (EPOCH))\n        break\n\n    lasttime = time.time()\n    train_acc, train_loss, test_acc, test_loss = 0, 0, 0, 0\n    pbar = tqdm(\n        range(0, train_X.shape[0], batch_size), desc = 'train minibatch loop'\n    )\n    for i in pbar:\n        index = min(i + batch_size, train_X.shape[0])\n        batch_x = train_X[i : index]\n        batch_char = generate_char_seq(batch_x)\n        batch_y = train_Y[i : index]\n        acc, cost, _ = sess.run(\n            [model.accuracy, model.cost, model.optimizer],\n            feed_dict = {\n                model.word_ids: batch_x,\n                model.char_ids: batch_char,\n                model.labels: batch_y\n            },\n        )\n        assert not np.isnan(cost)\n        train_loss += cost\n        train_acc += acc\n        pbar.set_postfix(cost = cost, accuracy = acc)\n        \n    pbar = tqdm(\n        range(0, test_X.shape[0], batch_size), desc = 'test minibatch loop'\n    )\n    for i in pbar:\n        index = min(i + batch_size, test_X.shape[0])\n        batch_x = test_X[i : index]\n        batch_char = generate_char_seq(batch_x)\n        batch_y = test_Y[i : index]\n        acc, cost = sess.run(\n            [model.accuracy, model.cost],\n            feed_dict = {\n                model.word_ids: batch_x,\n                model.char_ids: batch_char,\n                model.labels: batch_y\n            },\n        )\n        assert not np.isnan(cost)\n        test_loss += cost\n        test_acc += acc\n        pbar.set_postfix(cost = cost, accuracy = acc)\n    \n    train_loss /= len(train_X) / batch_size\n    train_acc /= len(train_X) / batch_size\n    test_loss /= len(test_X) / batch_size\n    test_acc /= len(test_X) / batch_size\n\n    print('time taken:', time.time() - lasttime)\n    print(\n        'epoch: %d, training loss: %f, training acc: %f, valid loss: %f, valid acc: %f\\n'\n        % (EPOCH, train_loss, train_acc, test_loss, test_acc)\n    )\n    \n    sequence = entities_textcleaning(string)[1]\n    X_seq = char_str_idx([sequence], word2idx, 2)\n    X_char_seq = generate_char_seq(X_seq)\n\n    predicted = sess.run(model.tags_seq,\n                feed_dict = {\n                    model.word_ids: X_seq,\n                    model.char_ids: X_char_seq,\n                },\n        )[0]\n\n    for i in range(len(predicted)):\n        print(sequence[i],idx2tag[predicted[i]])\n        \n    if test_acc > CURRENT_ACC:\n        print(\n            'epoch: %d, pass acc: %f, current acc: %f'\n            % (EPOCH, CURRENT_ACC, test_acc)\n        )\n        CURRENT_ACC = test_acc\n        CURRENT_CHECKPOINT = 0\n    else:\n        CURRENT_CHECKPOINT += 1\n    EPOCH += 1","execution_count":13,"outputs":[{"output_type":"stream","text":"train minibatch loop: 100%|██████████| 1524/1524 [09:20<00:00,  2.65it/s, accuracy=0.911, cost=14.4]\ntest minibatch loop: 100%|██████████| 381/381 [01:13<00:00,  5.49it/s, accuracy=0.943, cost=9.99]\n","name":"stderr"},{"output_type":"stream","text":"time taken: 633.8687188625336\nepoch: 0, training loss: 26.504379, training acc: 0.843851, valid loss: 14.007037, valid acc: 0.923529\n\n","name":"stdout"},{"output_type":"stream","text":"\rtrain minibatch loop:   0%|          | 0/1524 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"Kuala PROPN\nLumpur PROPN\nSempena PROPN\nsambutan NOUN\nAidilfitri PROPN\nminggu NOUN\ndepan ADJ\nPerdana PROPN\nMenteri PROPN\nTun PROPN\nDr PROPN\nMahathir PROPN\nMohamad PROPN\ndan CCONJ\nMenteri PROPN\nPengangkutan PROPN\nAnthony PROPN\nLoke PROPN\nSiew PROPN\nFook PROPN\nmenitipkan PROPN\npesanan PROPN\nkhas ADJ\nkepada ADP\norang NOUN\nramai NOUN\nyang PRON\nmahu ADV\npulang VERB\nke ADP\nkampung NOUN\nhalaman NOUN\nmasing-masing PROPN\nDalam ADP\nvideo NOUN\npendek ADJ\nterbitan NOUN\nJabatan PROPN\nKeselamatan PROPN\nJalan PROPN\nRaya PROPN\n(Jkjr) PROPN\nitu DET\nDr PROPN\nMahathir PROPN\nmenasihati PROPN\nmereka PRON\nsupaya SCONJ\nberhenti VERB\nberehat PROPN\ndan CCONJ\ntidur NOUN\nsebentar ADJ\nsekiranya PROPN\nmengantuk PROPN\nketika SCONJ\nmemandu VERB\nepoch: 0, pass acc: 0.000000, current acc: 0.923529\n","name":"stdout"},{"output_type":"stream","text":"train minibatch loop: 100%|██████████| 1524/1524 [09:13<00:00,  2.82it/s, accuracy=0.95, cost=8.75] \ntest minibatch loop: 100%|██████████| 381/381 [01:12<00:00,  5.39it/s, accuracy=0.927, cost=9.4] \ntrain minibatch loop:   0%|          | 0/1524 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"time taken: 625.8798580169678\nepoch: 1, training loss: 9.353703, training acc: 0.945277, valid loss: 14.173260, valid acc: 0.922680\n\nKuala PROPN\nLumpur PROPN\nSempena PROPN\nsambutan NOUN\nAidilfitri PROPN\nminggu NOUN\ndepan ADJ\nPerdana PROPN\nMenteri PROPN\nTun PROPN\nDr PROPN\nMahathir PROPN\nMohamad PROPN\ndan CCONJ\nMenteri PROPN\nPengangkutan PROPN\nAnthony PROPN\nLoke PROPN\nSiew PROPN\nFook PROPN\nmenitipkan PROPN\npesanan PROPN\nkhas NOUN\nkepada ADP\norang NOUN\nramai NOUN\nyang PRON\nmahu ADV\npulang VERB\nke ADP\nkampung NOUN\nhalaman NOUN\nmasing-masing PROPN\nDalam ADP\nvideo NOUN\npendek ADJ\nterbitan NOUN\nJabatan NOUN\nKeselamatan PROPN\nJalan PROPN\nRaya PROPN\n(Jkjr) PROPN\nitu DET\nDr PROPN\nMahathir PROPN\nmenasihati PROPN\nmereka PRON\nsupaya SCONJ\nberhenti VERB\nberehat PROPN\ndan CCONJ\ntidur NOUN\nsebentar ADV\nsekiranya PROPN\nmengantuk PROPN\nketika SCONJ\nmemandu VERB\n","name":"stdout"},{"output_type":"stream","text":"train minibatch loop: 100%|██████████| 1524/1524 [09:10<00:00,  2.77it/s, accuracy=0.967, cost=6.32] \ntest minibatch loop: 100%|██████████| 381/381 [01:12<00:00,  5.36it/s, accuracy=0.911, cost=10.1]\n","name":"stderr"},{"output_type":"stream","text":"time taken: 622.2254421710968\nepoch: 2, training loss: 6.416565, training acc: 0.961122, valid loss: 15.992839, valid acc: 0.914768\n\nKuala PROPN\nLumpur PROPN\nSempena PROPN\nsambutan NOUN\nAidilfitri NOUN\nminggu NOUN\ndepan ADJ\nPerdana PROPN\nMenteri PROPN\nTun PROPN\nDr PROPN\nMahathir PROPN\nMohamad PROPN\ndan CCONJ\nMenteri PROPN\nPengangkutan PROPN\nAnthony PROPN\nLoke PROPN\nSiew PROPN\nFook PROPN\nmenitipkan PROPN\npesanan PROPN\nkhas ADJ\nkepada ADP\norang NOUN\nramai NOUN\nyang PRON\nmahu ADV\npulang VERB\nke ADP\nkampung NOUN\nhalaman NOUN\nmasing-masing PROPN\nDalam ADP\nvideo NOUN\npendek ADJ\nterbitan NOUN\nJabatan NOUN\nKeselamatan NOUN\nJalan PROPN\nRaya PROPN\n(Jkjr) NOUN\nitu DET\nDr PROPN\nMahathir PROPN\nmenasihati PROPN\nmereka PRON\nsupaya SCONJ\nberhenti VERB\nberehat PROPN\ndan CCONJ\ntidur NOUN\nsebentar ADV\nsekiranya PROPN\nmengantuk PROPN\nketika SCONJ\nmemandu VERB\nbreak epoch:3\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequence = entities_textcleaning('mahathir suka Akta 19977')[1]\nX_seq = char_str_idx([sequence], word2idx, 2)\nX_char_seq = generate_char_seq(X_seq)\n\npredicted = sess.run(model.tags_seq,\n            feed_dict = {\n                model.word_ids: X_seq,\n                model.char_ids: X_char_seq,\n            },\n    )[0]\n\nfor i in range(len(predicted)):\n    print(sequence[i],idx2tag[predicted[i]])","execution_count":14,"outputs":[{"output_type":"stream","text":"mahathir PROPN\nsuka VERB\nAkta PROPN\n19977 PROPN\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pred2label(pred):\n    out = []\n    for pred_i in pred:\n        out_i = []\n        for p in pred_i:\n            out_i.append(idx2tag[p])\n        out.append(out_i)\n    return out","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"real_Y, predict_Y = [], []\n\npbar = tqdm(\n    range(0, len(test_X), batch_size), desc = 'validation minibatch loop'\n)\nfor i in pbar:\n    batch_x = test_X[i : min(i + batch_size, test_X.shape[0])]\n    batch_char = generate_char_seq(batch_x)\n    batch_y = test_Y[i : min(i + batch_size, test_X.shape[0])]\n    predicted = pred2label(sess.run(model.tags_seq,\n            feed_dict = {\n                model.word_ids: batch_x,\n                model.char_ids: batch_char,\n            },\n    ))\n    real = pred2label(batch_y)\n    predict_Y.extend(predicted)\n    real_Y.extend(real)","execution_count":16,"outputs":[{"output_type":"stream","text":"validation minibatch loop: 100%|██████████| 381/381 [01:07<00:00,  5.73it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(np.array(real_Y).ravel(), np.array(predict_Y).ravel(),\n                           digits = 6))","execution_count":17,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n","name":"stderr"},{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n         ADJ   0.817403  0.689528  0.748040     45666\n         ADP   0.946743  0.960281  0.953464    119589\n         ADV   0.862745  0.790976  0.825303     47760\n         AUX   0.992753  1.000000  0.996363     10000\n       CCONJ   0.980811  0.917167  0.947922     37171\n         DET   0.922089  0.927882  0.924976     38839\n        NOUN   0.825955  0.933548  0.876462    268329\n         NUM   0.917225  0.927107  0.922139     41211\n        PART   0.879739  0.881818  0.880777      5500\n        PRON   0.967885  0.938671  0.953054     48835\n       PROPN   0.941781  0.858129  0.898011    227608\n       PUNCT   0.999666  0.998906  0.999286    182824\n       SCONJ   0.710932  0.839604  0.769929     15150\n         SYM   0.989079  0.981111  0.985079      3600\n        VERB   0.943542  0.908367  0.925620    124518\n           X   0.000000  0.000000  0.000000       150\n\n    accuracy                       0.913031   1216750\n   macro avg   0.856147  0.847068  0.850402   1216750\nweighted avg   0.915901  0.913031  0.912964   1216750\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"saver = tf.train.Saver(tf.trainable_variables())\nsaver.save(sess, 'concat/model.ckpt')\n\nstrings = ','.join(\n    [\n        n.name\n        for n in tf.get_default_graph().as_graph_def().node\n        if ('Variable' in n.op\n        or 'Placeholder' in n.name\n        or 'logits' in n.name\n        or 'alphas' in n.name)\n        and 'Adam' not in n.name\n        and 'beta' not in n.name\n        and 'OptimizeLoss' not in n.name\n        and 'Global_Step' not in n.name\n    ]\n)\nstrings.split(',')","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"['Placeholder',\n 'Placeholder_1',\n 'Placeholder_2',\n 'Variable',\n 'Variable_1',\n 'bidirectional_rnn_char_0/fw/lstm_cell/kernel',\n 'bidirectional_rnn_char_0/fw/lstm_cell/bias',\n 'bidirectional_rnn_char_0/bw/lstm_cell/kernel',\n 'bidirectional_rnn_char_0/bw/lstm_cell/bias',\n 'bidirectional_rnn_char_1/fw/lstm_cell/kernel',\n 'bidirectional_rnn_char_1/fw/lstm_cell/bias',\n 'bidirectional_rnn_char_1/bw/lstm_cell/kernel',\n 'bidirectional_rnn_char_1/bw/lstm_cell/bias',\n 'bidirectional_rnn_word_0/fw/lstm_cell/kernel',\n 'bidirectional_rnn_word_0/fw/lstm_cell/bias',\n 'bidirectional_rnn_word_0/bw/lstm_cell/kernel',\n 'bidirectional_rnn_word_0/bw/lstm_cell/bias',\n 'bidirectional_rnn_word_1/fw/lstm_cell/kernel',\n 'bidirectional_rnn_word_1/fw/lstm_cell/bias',\n 'bidirectional_rnn_word_1/bw/lstm_cell/kernel',\n 'bidirectional_rnn_word_1/bw/lstm_cell/bias',\n 'dense/kernel',\n 'dense/bias',\n 'transitions',\n 'logits']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def freeze_graph(model_dir, output_node_names):\n\n    if not tf.gfile.Exists(model_dir):\n        raise AssertionError(\n            \"Export directory doesn't exists. Please specify an export \"\n            'directory: %s' % model_dir\n        )\n\n    checkpoint = tf.train.get_checkpoint_state(model_dir)\n    input_checkpoint = checkpoint.model_checkpoint_path\n\n    absolute_model_dir = '/'.join(input_checkpoint.split('/')[:-1])\n    output_graph = absolute_model_dir + '/frozen_model.pb'\n    clear_devices = True\n    with tf.Session(graph = tf.Graph()) as sess:\n        saver = tf.train.import_meta_graph(\n            input_checkpoint + '.meta', clear_devices = clear_devices\n        )\n        saver.restore(sess, input_checkpoint)\n        output_graph_def = tf.graph_util.convert_variables_to_constants(\n            sess,\n            tf.get_default_graph().as_graph_def(),\n            output_node_names.split(','),\n        )\n        with tf.gfile.GFile(output_graph, 'wb') as f:\n            f.write(output_graph_def.SerializeToString())\n        print('%d ops in the final graph.' % len(output_graph_def.node))\n        \ndef load_graph(frozen_graph_filename):\n    with tf.gfile.GFile(frozen_graph_filename, 'rb') as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n    with tf.Graph().as_default() as graph:\n        tf.import_graph_def(graph_def)\n    return graph\n","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freeze_graph('concat', strings)","execution_count":20,"outputs":[{"output_type":"stream","text":"1532 ops in the final graph.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import boto3\n\nbucketName = 'huseinhouse-storage'\nKey = 'concat/frozen_model.pb'\noutPutname = \"v27/pos/concat-pos.pb\"\n\ns3 = boto3.client('s3',\n                 aws_access_key_id='',\n                 aws_secret_access_key='')\ns3.upload_file(Key,bucketName,outPutname)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}