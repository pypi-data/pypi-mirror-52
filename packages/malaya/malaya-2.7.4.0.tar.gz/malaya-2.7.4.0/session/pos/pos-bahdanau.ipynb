{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-08-04 13:55:49--  https://huseinhouse-storage.s3-ap-southeast-1.amazonaws.com/bert-bahasa/dictionary-pos.json\n",
      "Resolving huseinhouse-storage.s3-ap-southeast-1.amazonaws.com (huseinhouse-storage.s3-ap-southeast-1.amazonaws.com)... 52.219.32.175\n",
      "Connecting to huseinhouse-storage.s3-ap-southeast-1.amazonaws.com (huseinhouse-storage.s3-ap-southeast-1.amazonaws.com)|52.219.32.175|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 825070 (806K) [binary/octet-stream]\n",
      "Saving to: ‘dictionary-pos.json’\n",
      "\n",
      "dictionary-pos.json 100%[===================>] 805.73K   554KB/s    in 1.5s    \n",
      "\n",
      "2019-08-04 13:55:52 (554 KB/s) - ‘dictionary-pos.json’ saved [825070/825070]\n",
      "\n",
      "--2019-08-04 13:55:53--  https://huseinhouse-storage.s3-ap-southeast-1.amazonaws.com/bert-bahasa/session-pos.pkl\n",
      "Resolving huseinhouse-storage.s3-ap-southeast-1.amazonaws.com (huseinhouse-storage.s3-ap-southeast-1.amazonaws.com)... 52.219.40.27\n",
      "Connecting to huseinhouse-storage.s3-ap-southeast-1.amazonaws.com (huseinhouse-storage.s3-ap-southeast-1.amazonaws.com)|52.219.40.27|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 97458750 (93M) [binary/octet-stream]\n",
      "Saving to: ‘session-pos.pkl’\n",
      "\n",
      "session-pos.pkl     100%[===================>]  92.94M  10.2MB/s    in 11s     \n",
      "\n",
      "2019-08-04 13:56:05 (8.20 MB/s) - ‘session-pos.pkl’ saved [97458750/97458750]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://huseinhouse-storage.s3-ap-southeast-1.amazonaws.com/bert-bahasa/dictionary-pos.json\n",
    "!wget https://huseinhouse-storage.s3-ap-southeast-1.amazonaws.com/bert-bahasa/session-pos.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train_X', 'test_X', 'train_Y', 'test_Y'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('session-pos.pkl', 'rb') as fopen:\n",
    "    data = pickle.load(fopen)\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = data['train_X']\n",
    "test_X = data['test_X']\n",
    "train_Y = data['train_Y']\n",
    "test_Y = data['test_Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['word2idx', 'idx2word', 'tag2idx', 'idx2tag', 'char2idx'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('dictionary-pos.json') as fopen:\n",
    "    dictionary = json.load(fopen)\n",
    "dictionary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = dictionary['word2idx']\n",
    "idx2word = {int(k): v for k, v in dictionary['idx2word'].items()}\n",
    "tag2idx = dictionary['tag2idx']\n",
    "idx2tag = {int(k): v for k, v in dictionary['idx2tag'].items()}\n",
    "char2idx = dictionary['char2idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('-', 'PUNCT'),\n",
       " ('film', 'NOUN'),\n",
       " ('yang', 'PRON'),\n",
       " ('dibuatnya', 'VERB'),\n",
       " ('akan', 'ADV'),\n",
       " ('segera', 'ADV'),\n",
       " ('tayang', 'VERB'),\n",
       " ('.', 'PUNCT'),\n",
       " ('Jadi', 'ADV'),\n",
       " ('dicoba', 'VERB'),\n",
       " ('untuk', 'ADP'),\n",
       " ('menjawab', 'VERB'),\n",
       " ('pertanyaan-pertanyaan', 'NOUN'),\n",
       " ('seperti', 'ADP'),\n",
       " ('kebutuhan', 'NOUN'),\n",
       " ('apa', 'PRON'),\n",
       " ('yang', 'PRON'),\n",
       " ('dicoba', 'VERB'),\n",
       " ('dipuaskan', 'VERB'),\n",
       " ('oleh', 'ADP'),\n",
       " ('seseorang', 'NOUN'),\n",
       " ('?', 'PUNCT'),\n",
       " ('Kamu', 'PRON'),\n",
       " ('selalu', 'ADV'),\n",
       " ('bertanya', 'VERB'),\n",
       " ('apa', 'PRON'),\n",
       " ('itu', 'DET'),\n",
       " ('Pi', 'PROPN'),\n",
       " ('?', 'PUNCT'),\n",
       " ('Bagaimana', 'PRON'),\n",
       " ('di', 'ADP'),\n",
       " ('Indonesia', 'PROPN'),\n",
       " ('?', 'PUNCT'),\n",
       " ('Grimes', 'PROPN'),\n",
       " ('merupakan', 'VERB'),\n",
       " ('sebuah', 'DET'),\n",
       " ('di', 'ADP'),\n",
       " ('Dale', 'PROPN'),\n",
       " (',', 'PUNCT'),\n",
       " ('Alabama', 'PROPN'),\n",
       " (',', 'PUNCT'),\n",
       " ('Amerika', 'PROPN'),\n",
       " ('Serikat', 'PROPN'),\n",
       " ('.', 'PUNCT'),\n",
       " ('Sampul', 'NOUN'),\n",
       " ('dari', 'ADP'),\n",
       " ('dua', 'NUM'),\n",
       " ('singel', 'NOUN'),\n",
       " ('pertama', 'NUM'),\n",
       " ('difoto', 'VERB')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip([idx2word[d] for d in train_X[-1]], [idx2tag[d] for d in train_Y[-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_char_seq(batch):\n",
    "    x = [[len(idx2word[i]) for i in k] for k in batch]\n",
    "    maxlen = max([j for i in x for j in i])\n",
    "    temp = np.zeros((batch.shape[0],batch.shape[1],maxlen),dtype=np.int32)\n",
    "    for i in range(batch.shape[0]):\n",
    "        for k in range(batch.shape[1]):\n",
    "            for no, c in enumerate(idx2word[batch[i,k]]):\n",
    "                temp[i,k,-1-no] = char2idx[c]\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 50, 12)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_char_seq(data['train_X'][:10]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_word,\n",
    "        dim_char,\n",
    "        dropout,\n",
    "        learning_rate,\n",
    "        hidden_size_char,\n",
    "        hidden_size_word,\n",
    "        num_layers,\n",
    "    ):\n",
    "        def cells(size, reuse = False):\n",
    "            return tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.nn.rnn_cell.LSTMCell(\n",
    "                    size,\n",
    "                    initializer = tf.orthogonal_initializer(),\n",
    "                    reuse = reuse,\n",
    "                ),\n",
    "                output_keep_prob = dropout,\n",
    "            )\n",
    "\n",
    "        def bahdanau(embedded, size):\n",
    "            attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                num_units = hidden_size_word, memory = embedded\n",
    "            )\n",
    "            return tf.contrib.seq2seq.AttentionWrapper(\n",
    "                cell = cells(hidden_size_word),\n",
    "                attention_mechanism = attention_mechanism,\n",
    "                attention_layer_size = hidden_size_word,\n",
    "            )\n",
    "\n",
    "        self.word_ids = tf.placeholder(tf.int32, shape = [None, None])\n",
    "        self.char_ids = tf.placeholder(tf.int32, shape = [None, None, None])\n",
    "        self.labels = tf.placeholder(tf.int32, shape = [None, None])\n",
    "        self.maxlen = tf.shape(self.word_ids)[1]\n",
    "        self.lengths = tf.count_nonzero(self.word_ids, 1)\n",
    "        \n",
    "        self.word_embeddings = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [len(word2idx), dim_word], stddev = 1.0 / np.sqrt(dim_word)\n",
    "            )\n",
    "        )\n",
    "        self.char_embeddings = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [len(char2idx), dim_char], stddev = 1.0 / np.sqrt(dim_char)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        word_embedded = tf.nn.embedding_lookup(\n",
    "            self.word_embeddings, self.word_ids\n",
    "        )\n",
    "        char_embedded = tf.nn.embedding_lookup(\n",
    "            self.char_embeddings, self.char_ids\n",
    "        )\n",
    "        s = tf.shape(char_embedded)\n",
    "        char_embedded = tf.reshape(\n",
    "            char_embedded, shape = [s[0] * s[1], s[-2], dim_char]\n",
    "        )\n",
    "        \n",
    "        for n in range(num_layers):\n",
    "            (out_fw, out_bw), (\n",
    "                state_fw,\n",
    "                state_bw,\n",
    "            ) = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw = cells(hidden_size_char),\n",
    "                cell_bw = cells(hidden_size_char),\n",
    "                inputs = char_embedded,\n",
    "                dtype = tf.float32,\n",
    "                scope = 'bidirectional_rnn_char_%d' % (n),\n",
    "            )\n",
    "            char_embedded = tf.concat((out_fw, out_bw), 2)\n",
    "        output = tf.reshape(\n",
    "            char_embedded[:, -1], shape = [s[0], s[1], 2 * hidden_size_char]\n",
    "        )\n",
    "        word_embedded = tf.concat([word_embedded, output], axis = -1)\n",
    "\n",
    "        for n in range(num_layers):\n",
    "            (out_fw, out_bw), (\n",
    "                state_fw,\n",
    "                state_bw,\n",
    "            ) = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw = bahdanau(word_embedded, hidden_size_word),\n",
    "                cell_bw = bahdanau(word_embedded, hidden_size_word),\n",
    "                inputs = word_embedded,\n",
    "                dtype = tf.float32,\n",
    "                scope = 'bidirectional_rnn_word_%d' % (n),\n",
    "            )\n",
    "            word_embedded = tf.concat((out_fw, out_bw), 2)\n",
    "\n",
    "        logits = tf.layers.dense(word_embedded, len(idx2tag))\n",
    "        y_t = self.labels\n",
    "        log_likelihood, transition_params = tf.contrib.crf.crf_log_likelihood(\n",
    "            logits, y_t, self.lengths\n",
    "        )\n",
    "        self.cost = tf.reduce_mean(-log_likelihood)\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate = learning_rate\n",
    "        ).minimize(self.cost)\n",
    "        mask = tf.sequence_mask(self.lengths, maxlen = self.maxlen)\n",
    "        self.tags_seq, tags_score = tf.contrib.crf.crf_decode(\n",
    "            logits, transition_params, self.lengths\n",
    "        )\n",
    "        self.tags_seq = tf.identity(self.tags_seq, name = 'logits')\n",
    "\n",
    "        y_t = tf.cast(y_t, tf.int32)\n",
    "        self.prediction = tf.boolean_mask(self.tags_seq, mask)\n",
    "        mask_label = tf.boolean_mask(y_t, mask)\n",
    "        correct_pred = tf.equal(self.prediction, mask_label)\n",
    "        correct_index = tf.cast(correct_pred, tf.float32)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "dim_word = 128\n",
    "dim_char = 256\n",
    "dropout = 0.8\n",
    "learning_rate = 1e-3\n",
    "hidden_size_char = 128\n",
    "hidden_size_word = 128\n",
    "num_layers = 2\n",
    "batch_size = 64\n",
    "\n",
    "model = Model(dim_word,dim_char,dropout,learning_rate,hidden_size_char,hidden_size_word,num_layers)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = 'KUALA LUMPUR: Sempena sambutan Aidilfitri minggu depan, Perdana Menteri Tun Dr Mahathir Mohamad dan Menteri Pengangkutan Anthony Loke Siew Fook menitipkan pesanan khas kepada orang ramai yang mahu pulang ke kampung halaman masing-masing. Dalam video pendek terbitan Jabatan Keselamatan Jalan Raya (JKJR) itu, Dr Mahathir menasihati mereka supaya berhenti berehat dan tidur sebentar  sekiranya mengantuk ketika memandu.'\n",
    "\n",
    "import re\n",
    "\n",
    "def entities_textcleaning(string, lowering = False):\n",
    "    \"\"\"\n",
    "    use by entities recognition, pos recognition and dependency parsing\n",
    "    \"\"\"\n",
    "    string = re.sub('[^A-Za-z0-9\\-\\/() ]+', ' ', string)\n",
    "    string = re.sub(r'[ ]+', ' ', string).strip()\n",
    "    original_string = string.split()\n",
    "    if lowering:\n",
    "        string = string.lower()\n",
    "    string = [\n",
    "        (original_string[no], word.title() if word.isupper() else word)\n",
    "        for no, word in enumerate(string.split())\n",
    "        if len(word)\n",
    "    ]\n",
    "    return [s[0] for s in string], [s[1] for s in string]\n",
    "\n",
    "def char_str_idx(corpus, dic, UNK = 0):\n",
    "    maxlen = max([len(i) for i in corpus])\n",
    "    X = np.zeros((len(corpus), maxlen))\n",
    "    for i in range(len(corpus)):\n",
    "        for no, k in enumerate(corpus[i][:maxlen][::-1]):\n",
    "            val = dic[k] if k in dic else UNK\n",
    "            X[i, -1 - no] = val\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 1524/1524 [13:39<00:00,  1.88it/s, accuracy=0.896, cost=16]  \n",
      "test minibatch loop: 100%|██████████| 381/381 [01:36<00:00,  3.94it/s, accuracy=0.94, cost=9.56] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 916.4935417175293\n",
      "epoch: 0, training loss: 30.129804, training acc: 0.816989, valid loss: 15.117491, valid acc: 0.918162\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "train minibatch loop:   0%|          | 0/1524 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kuala PROPN\n",
      "Lumpur PROPN\n",
      "Sempena PROPN\n",
      "sambutan NOUN\n",
      "Aidilfitri PROPN\n",
      "minggu VERB\n",
      "depan ADJ\n",
      "Perdana PROPN\n",
      "Menteri PROPN\n",
      "Tun PROPN\n",
      "Dr PROPN\n",
      "Mahathir PROPN\n",
      "Mohamad PROPN\n",
      "dan CCONJ\n",
      "Menteri PROPN\n",
      "Pengangkutan PROPN\n",
      "Anthony PROPN\n",
      "Loke PROPN\n",
      "Siew PROPN\n",
      "Fook PROPN\n",
      "menitipkan PROPN\n",
      "pesanan PROPN\n",
      "khas NOUN\n",
      "kepada ADP\n",
      "orang NOUN\n",
      "ramai NOUN\n",
      "yang PRON\n",
      "mahu ADJ\n",
      "pulang VERB\n",
      "ke ADP\n",
      "kampung NOUN\n",
      "halaman NOUN\n",
      "masing-masing PROPN\n",
      "Dalam ADP\n",
      "video NOUN\n",
      "pendek ADJ\n",
      "terbitan NOUN\n",
      "Jabatan PROPN\n",
      "Keselamatan PROPN\n",
      "Jalan PROPN\n",
      "Raya PROPN\n",
      "(Jkjr) PROPN\n",
      "itu DET\n",
      "Dr PROPN\n",
      "Mahathir PROPN\n",
      "menasihati PROPN\n",
      "mereka PRON\n",
      "supaya PART\n",
      "berhenti VERB\n",
      "berehat PROPN\n",
      "dan CCONJ\n",
      "tidur NOUN\n",
      "sebentar ADJ\n",
      "sekiranya PROPN\n",
      "mengantuk PROPN\n",
      "ketika SCONJ\n",
      "memandu VERB\n",
      "epoch: 0, pass acc: 0.000000, current acc: 0.918162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 1524/1524 [13:30<00:00,  1.90it/s, accuracy=0.95, cost=10.8] \n",
      "test minibatch loop: 100%|██████████| 381/381 [01:35<00:00,  4.01it/s, accuracy=0.948, cost=10]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 906.6713445186615\n",
      "epoch: 1, training loss: 10.626078, training acc: 0.939507, valid loss: 15.288042, valid acc: 0.921442\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "train minibatch loop:   0%|          | 0/1524 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kuala PROPN\n",
      "Lumpur PROPN\n",
      "Sempena PROPN\n",
      "sambutan NOUN\n",
      "Aidilfitri PROPN\n",
      "minggu NOUN\n",
      "depan ADJ\n",
      "Perdana PROPN\n",
      "Menteri PROPN\n",
      "Tun PROPN\n",
      "Dr PROPN\n",
      "Mahathir PROPN\n",
      "Mohamad PROPN\n",
      "dan CCONJ\n",
      "Menteri PROPN\n",
      "Pengangkutan PROPN\n",
      "Anthony PROPN\n",
      "Loke PROPN\n",
      "Siew PROPN\n",
      "Fook PROPN\n",
      "menitipkan PROPN\n",
      "pesanan PROPN\n",
      "khas VERB\n",
      "kepada ADP\n",
      "orang NOUN\n",
      "ramai NOUN\n",
      "yang PRON\n",
      "mahu DET\n",
      "pulang VERB\n",
      "ke ADP\n",
      "kampung NOUN\n",
      "halaman NOUN\n",
      "masing-masing PROPN\n",
      "Dalam ADP\n",
      "video NOUN\n",
      "pendek ADJ\n",
      "terbitan NOUN\n",
      "Jabatan NOUN\n",
      "Keselamatan PROPN\n",
      "Jalan PROPN\n",
      "Raya PROPN\n",
      "(Jkjr) PROPN\n",
      "itu DET\n",
      "Dr PROPN\n",
      "Mahathir PUNCT\n",
      "menasihati PROPN\n",
      "mereka PRON\n",
      "supaya SCONJ\n",
      "berhenti VERB\n",
      "berehat PROPN\n",
      "dan CCONJ\n",
      "tidur NOUN\n",
      "sebentar ADV\n",
      "sekiranya PUNCT\n",
      "mengantuk PROPN\n",
      "ketika SCONJ\n",
      "memandu VERB\n",
      "epoch: 1, pass acc: 0.918162, current acc: 0.921442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 1524/1524 [13:27<00:00,  1.90it/s, accuracy=0.945, cost=8.01] \n",
      "test minibatch loop: 100%|██████████| 381/381 [01:35<00:00,  4.06it/s, accuracy=0.933, cost=12.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 903.2434318065643\n",
      "epoch: 2, training loss: 7.451073, training acc: 0.956498, valid loss: 17.149170, valid acc: 0.919412\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "train minibatch loop:   0%|          | 0/1524 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kuala PROPN\n",
      "Lumpur PROPN\n",
      "Sempena PUNCT\n",
      "sambutan NOUN\n",
      "Aidilfitri PROPN\n",
      "minggu VERB\n",
      "depan NOUN\n",
      "Perdana PROPN\n",
      "Menteri PROPN\n",
      "Tun PROPN\n",
      "Dr PROPN\n",
      "Mahathir PROPN\n",
      "Mohamad PROPN\n",
      "dan CCONJ\n",
      "Menteri PROPN\n",
      "Pengangkutan PROPN\n",
      "Anthony PROPN\n",
      "Loke PROPN\n",
      "Siew PROPN\n",
      "Fook PROPN\n",
      "menitipkan PROPN\n",
      "pesanan PROPN\n",
      "khas VERB\n",
      "kepada ADP\n",
      "orang NOUN\n",
      "ramai NOUN\n",
      "yang PRON\n",
      "mahu ADV\n",
      "pulang VERB\n",
      "ke ADP\n",
      "kampung NOUN\n",
      "halaman NOUN\n",
      "masing-masing PROPN\n",
      "Dalam ADP\n",
      "video NOUN\n",
      "pendek ADJ\n",
      "terbitan NOUN\n",
      "Jabatan NOUN\n",
      "Keselamatan PROPN\n",
      "Jalan PROPN\n",
      "Raya PROPN\n",
      "(Jkjr) PROPN\n",
      "itu DET\n",
      "Dr PROPN\n",
      "Mahathir PROPN\n",
      "menasihati PROPN\n",
      "mereka PRON\n",
      "supaya SCONJ\n",
      "berhenti VERB\n",
      "berehat PROPN\n",
      "dan CCONJ\n",
      "tidur NOUN\n",
      "sebentar ADV\n",
      "sekiranya PROPN\n",
      "mengantuk PROPN\n",
      "ketika SCONJ\n",
      "memandu VERB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 1524/1524 [13:27<00:00,  1.91it/s, accuracy=0.969, cost=4.91] \n",
      "test minibatch loop: 100%|██████████| 381/381 [01:35<00:00,  4.09it/s, accuracy=0.921, cost=14.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 902.8044228553772\n",
      "epoch: 3, training loss: 5.376336, training acc: 0.967726, valid loss: 20.922392, valid acc: 0.905358\n",
      "\n",
      "Kuala PROPN\n",
      "Lumpur PROPN\n",
      "Sempena PROPN\n",
      "sambutan NOUN\n",
      "Aidilfitri PROPN\n",
      "minggu NOUN\n",
      "depan ADJ\n",
      "Perdana PROPN\n",
      "Menteri PROPN\n",
      "Tun PROPN\n",
      "Dr PROPN\n",
      "Mahathir PROPN\n",
      "Mohamad PROPN\n",
      "dan CCONJ\n",
      "Menteri PROPN\n",
      "Pengangkutan PROPN\n",
      "Anthony PROPN\n",
      "Loke PROPN\n",
      "Siew PROPN\n",
      "Fook PROPN\n",
      "menitipkan PROPN\n",
      "pesanan PROPN\n",
      "khas VERB\n",
      "kepada ADP\n",
      "orang NOUN\n",
      "ramai NOUN\n",
      "yang PRON\n",
      "mahu ADV\n",
      "pulang VERB\n",
      "ke ADP\n",
      "kampung NOUN\n",
      "halaman NOUN\n",
      "masing-masing NOUN\n",
      "Dalam ADP\n",
      "video NOUN\n",
      "pendek ADJ\n",
      "terbitan NOUN\n",
      "Jabatan NOUN\n",
      "Keselamatan NOUN\n",
      "Jalan NOUN\n",
      "Raya PROPN\n",
      "(Jkjr) NOUN\n",
      "itu DET\n",
      "Dr ADV\n",
      "Mahathir PROPN\n",
      "menasihati PROPN\n",
      "mereka PRON\n",
      "supaya SCONJ\n",
      "berhenti VERB\n",
      "berehat PROPN\n",
      "dan CCONJ\n",
      "tidur NOUN\n",
      "sebentar ADV\n",
      "sekiranya PROPN\n",
      "mengantuk PROPN\n",
      "ketika SCONJ\n",
      "memandu VERB\n",
      "break epoch:4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "EARLY_STOPPING, CURRENT_CHECKPOINT, CURRENT_ACC, EPOCH = 2, 0, 0, 0\n",
    "\n",
    "while True:\n",
    "    lasttime = time.time()\n",
    "    if CURRENT_CHECKPOINT == EARLY_STOPPING:\n",
    "        print('break epoch:%d\\n' % (EPOCH))\n",
    "        break\n",
    "\n",
    "    lasttime = time.time()\n",
    "    train_acc, train_loss, test_acc, test_loss = 0, 0, 0, 0\n",
    "    pbar = tqdm(\n",
    "        range(0, train_X.shape[0], batch_size), desc = 'train minibatch loop'\n",
    "    )\n",
    "    for i in pbar:\n",
    "        index = min(i + batch_size, train_X.shape[0])\n",
    "        batch_x = train_X[i : index]\n",
    "        batch_char = generate_char_seq(batch_x)\n",
    "        batch_y = train_Y[i : index]\n",
    "        acc, cost, _ = sess.run(\n",
    "            [model.accuracy, model.cost, model.optimizer],\n",
    "            feed_dict = {\n",
    "                model.word_ids: batch_x,\n",
    "                model.char_ids: batch_char,\n",
    "                model.labels: batch_y\n",
    "            },\n",
    "        )\n",
    "        assert not np.isnan(cost)\n",
    "        train_loss += cost\n",
    "        train_acc += acc\n",
    "        pbar.set_postfix(cost = cost, accuracy = acc)\n",
    "        \n",
    "    pbar = tqdm(\n",
    "        range(0, test_X.shape[0], batch_size), desc = 'test minibatch loop'\n",
    "    )\n",
    "    for i in pbar:\n",
    "        index = min(i + batch_size, test_X.shape[0])\n",
    "        batch_x = test_X[i : index]\n",
    "        batch_char = generate_char_seq(batch_x)\n",
    "        batch_y = test_Y[i : index]\n",
    "        acc, cost = sess.run(\n",
    "            [model.accuracy, model.cost],\n",
    "            feed_dict = {\n",
    "                model.word_ids: batch_x,\n",
    "                model.char_ids: batch_char,\n",
    "                model.labels: batch_y\n",
    "            },\n",
    "        )\n",
    "        assert not np.isnan(cost)\n",
    "        test_loss += cost\n",
    "        test_acc += acc\n",
    "        pbar.set_postfix(cost = cost, accuracy = acc)\n",
    "    \n",
    "    train_loss /= len(train_X) / batch_size\n",
    "    train_acc /= len(train_X) / batch_size\n",
    "    test_loss /= len(test_X) / batch_size\n",
    "    test_acc /= len(test_X) / batch_size\n",
    "\n",
    "    print('time taken:', time.time() - lasttime)\n",
    "    print(\n",
    "        'epoch: %d, training loss: %f, training acc: %f, valid loss: %f, valid acc: %f\\n'\n",
    "        % (EPOCH, train_loss, train_acc, test_loss, test_acc)\n",
    "    )\n",
    "    \n",
    "    sequence = entities_textcleaning(string)[1]\n",
    "    X_seq = char_str_idx([sequence], word2idx, 2)\n",
    "    X_char_seq = generate_char_seq(X_seq)\n",
    "\n",
    "    predicted = sess.run(model.tags_seq,\n",
    "                feed_dict = {\n",
    "                    model.word_ids: X_seq,\n",
    "                    model.char_ids: X_char_seq,\n",
    "                },\n",
    "        )[0]\n",
    "\n",
    "    for i in range(len(predicted)):\n",
    "        print(sequence[i],idx2tag[predicted[i]])\n",
    "        \n",
    "    if test_acc > CURRENT_ACC:\n",
    "        print(\n",
    "            'epoch: %d, pass acc: %f, current acc: %f'\n",
    "            % (EPOCH, CURRENT_ACC, test_acc)\n",
    "        )\n",
    "        CURRENT_ACC = test_acc\n",
    "        CURRENT_CHECKPOINT = 0\n",
    "    else:\n",
    "        CURRENT_CHECKPOINT += 1\n",
    "    EPOCH += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mahathir PROPN\n",
      "suka VERB\n",
      "Akta PROPN\n",
      "19977 PROPN\n"
     ]
    }
   ],
   "source": [
    "sequence = entities_textcleaning('mahathir suka Akta 19977')[1]\n",
    "X_seq = char_str_idx([sequence], word2idx, 2)\n",
    "X_char_seq = generate_char_seq(X_seq)\n",
    "\n",
    "predicted = sess.run(model.tags_seq,\n",
    "            feed_dict = {\n",
    "                model.word_ids: X_seq,\n",
    "                model.char_ids: X_char_seq,\n",
    "            },\n",
    "    )[0]\n",
    "\n",
    "for i in range(len(predicted)):\n",
    "    print(sequence[i],idx2tag[predicted[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred2label(pred):\n",
    "    out = []\n",
    "    for pred_i in pred:\n",
    "        out_i = []\n",
    "        for p in pred_i:\n",
    "            out_i.append(idx2tag[p])\n",
    "        out.append(out_i)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation minibatch loop: 100%|██████████| 381/381 [01:33<00:00,  4.19it/s]\n"
     ]
    }
   ],
   "source": [
    "real_Y, predict_Y = [], []\n",
    "\n",
    "pbar = tqdm(\n",
    "    range(0, len(test_X), batch_size), desc = 'validation minibatch loop'\n",
    ")\n",
    "for i in pbar:\n",
    "    batch_x = test_X[i : min(i + batch_size, test_X.shape[0])]\n",
    "    batch_char = generate_char_seq(batch_x)\n",
    "    batch_y = test_Y[i : min(i + batch_size, test_X.shape[0])]\n",
    "    predicted = pred2label(sess.run(model.tags_seq,\n",
    "            feed_dict = {\n",
    "                model.word_ids: batch_x,\n",
    "                model.char_ids: batch_char,\n",
    "            },\n",
    "    ))\n",
    "    real = pred2label(batch_y)\n",
    "    predict_Y.extend(predicted)\n",
    "    real_Y.extend(real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ   0.840047  0.621951  0.714731     45666\n",
      "         ADP   0.964556  0.949134  0.956783    119589\n",
      "         ADV   0.807150  0.835846  0.821247     47760\n",
      "         AUX   0.980583  0.999900  0.990147     10000\n",
      "       CCONJ   0.973852  0.910791  0.941266     37171\n",
      "         DET   0.952105  0.917197  0.934325     38839\n",
      "        NOUN   0.789860  0.935113  0.856371    268329\n",
      "         NUM   0.920680  0.936206  0.928378     41211\n",
      "        PART   0.933212  0.835818  0.881834      5500\n",
      "        PRON   0.977711  0.935968  0.956384     48835\n",
      "       PROPN   0.944440  0.816074  0.875577    227608\n",
      "       PUNCT   0.997880  0.999076  0.998478    182824\n",
      "       SCONJ   0.740312  0.796898  0.767563     15150\n",
      "         SYM   0.999425  0.965556  0.982198      3600\n",
      "        VERB   0.931810  0.917996  0.924851    124518\n",
      "           X   0.000000  0.000000  0.000000       150\n",
      "\n",
      "    accuracy                       0.903527   1216750\n",
      "   macro avg   0.859601  0.835845  0.845633   1216750\n",
      "weighted avg   0.909183  0.903527  0.903409   1216750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(np.array(real_Y).ravel(), np.array(predict_Y).ravel(),\n",
    "                           digits = 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Placeholder',\n",
       " 'Placeholder_1',\n",
       " 'Placeholder_2',\n",
       " 'Variable',\n",
       " 'Variable_1',\n",
       " 'bidirectional_rnn_char_0/fw/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_char_0/fw/lstm_cell/bias',\n",
       " 'bidirectional_rnn_char_0/bw/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_char_0/bw/lstm_cell/bias',\n",
       " 'bidirectional_rnn_char_1/fw/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_char_1/fw/lstm_cell/bias',\n",
       " 'bidirectional_rnn_char_1/bw/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_char_1/bw/lstm_cell/bias',\n",
       " 'memory_layer/kernel',\n",
       " 'memory_layer_1/kernel',\n",
       " 'bidirectional_rnn_word_0/fw/attention_wrapper/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_word_0/fw/attention_wrapper/lstm_cell/bias',\n",
       " 'bidirectional_rnn_word_0/fw/attention_wrapper/bahdanau_attention/query_layer/kernel',\n",
       " 'bidirectional_rnn_word_0/fw/attention_wrapper/bahdanau_attention/attention_v',\n",
       " 'bidirectional_rnn_word_0/fw/attention_wrapper/attention_layer/kernel',\n",
       " 'bidirectional_rnn_word_0/bw/attention_wrapper/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_word_0/bw/attention_wrapper/lstm_cell/bias',\n",
       " 'bidirectional_rnn_word_0/bw/attention_wrapper/bahdanau_attention/query_layer/kernel',\n",
       " 'bidirectional_rnn_word_0/bw/attention_wrapper/bahdanau_attention/attention_v',\n",
       " 'bidirectional_rnn_word_0/bw/attention_wrapper/attention_layer/kernel',\n",
       " 'memory_layer_2/kernel',\n",
       " 'memory_layer_3/kernel',\n",
       " 'bidirectional_rnn_word_1/fw/attention_wrapper/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_word_1/fw/attention_wrapper/lstm_cell/bias',\n",
       " 'bidirectional_rnn_word_1/fw/attention_wrapper/bahdanau_attention/query_layer/kernel',\n",
       " 'bidirectional_rnn_word_1/fw/attention_wrapper/bahdanau_attention/attention_v',\n",
       " 'bidirectional_rnn_word_1/fw/attention_wrapper/attention_layer/kernel',\n",
       " 'bidirectional_rnn_word_1/bw/attention_wrapper/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_word_1/bw/attention_wrapper/lstm_cell/bias',\n",
       " 'bidirectional_rnn_word_1/bw/attention_wrapper/bahdanau_attention/query_layer/kernel',\n",
       " 'bidirectional_rnn_word_1/bw/attention_wrapper/bahdanau_attention/attention_v',\n",
       " 'bidirectional_rnn_word_1/bw/attention_wrapper/attention_layer/kernel',\n",
       " 'dense/kernel',\n",
       " 'dense/bias',\n",
       " 'transitions',\n",
       " 'logits']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver(tf.trainable_variables())\n",
    "saver.save(sess, 'bahdanau/model.ckpt')\n",
    "\n",
    "strings = ','.join(\n",
    "    [\n",
    "        n.name\n",
    "        for n in tf.get_default_graph().as_graph_def().node\n",
    "        if ('Variable' in n.op\n",
    "        or 'Placeholder' in n.name\n",
    "        or 'logits' in n.name\n",
    "        or 'alphas' in n.name)\n",
    "        and 'Adam' not in n.name\n",
    "        and 'beta' not in n.name\n",
    "        and 'OptimizeLoss' not in n.name\n",
    "        and 'Global_Step' not in n.name\n",
    "    ]\n",
    ")\n",
    "strings.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_graph(model_dir, output_node_names):\n",
    "\n",
    "    if not tf.gfile.Exists(model_dir):\n",
    "        raise AssertionError(\n",
    "            \"Export directory doesn't exists. Please specify an export \"\n",
    "            'directory: %s' % model_dir\n",
    "        )\n",
    "\n",
    "    checkpoint = tf.train.get_checkpoint_state(model_dir)\n",
    "    input_checkpoint = checkpoint.model_checkpoint_path\n",
    "\n",
    "    absolute_model_dir = '/'.join(input_checkpoint.split('/')[:-1])\n",
    "    output_graph = absolute_model_dir + '/frozen_model.pb'\n",
    "    clear_devices = True\n",
    "    with tf.Session(graph = tf.Graph()) as sess:\n",
    "        saver = tf.train.import_meta_graph(\n",
    "            input_checkpoint + '.meta', clear_devices = clear_devices\n",
    "        )\n",
    "        saver.restore(sess, input_checkpoint)\n",
    "        output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "            sess,\n",
    "            tf.get_default_graph().as_graph_def(),\n",
    "            output_node_names.split(','),\n",
    "        )\n",
    "        with tf.gfile.GFile(output_graph, 'wb') as f:\n",
    "            f.write(output_graph_def.SerializeToString())\n",
    "        print('%d ops in the final graph.' % len(output_graph_def.node))\n",
    "        \n",
    "def load_graph(frozen_graph_filename):\n",
    "    with tf.gfile.GFile(frozen_graph_filename, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def)\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1928 ops in the final graph.\n"
     ]
    }
   ],
   "source": [
    "freeze_graph('bahdanau', strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "bucketName = 'huseinhouse-storage'\n",
    "Key = 'bahdanau/frozen_model.pb'\n",
    "outPutname = \"v27/pos/bahdanau-pos.pb\"\n",
    "\n",
    "s3 = boto3.client('s3',\n",
    "                 aws_access_key_id='',\n",
    "                 aws_secret_access_key='')\n",
    "s3.upload_file(Key,bucketName,outPutname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
