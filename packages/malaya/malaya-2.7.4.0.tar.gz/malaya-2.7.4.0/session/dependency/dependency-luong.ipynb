{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('id_gsd-ud-train.conllu.txt') as fopen:\n",
    "    corpus = fopen.read().split('\\n')\n",
    "    \n",
    "with open('id_gsd-ud-test.conllu.txt') as fopen:\n",
    "    corpus.extend(fopen.read().split('\\n'))\n",
    "    \n",
    "with open('id_gsd-ud-dev.conllu.txt') as fopen:\n",
    "    corpus.extend(fopen.read().split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {'PAD': 0,'NUM':1,'UNK':2}\n",
    "tag2idx = {'PAD': 0}\n",
    "char2idx = {'PAD': 0,'NUM':1,'UNK':2}\n",
    "word_idx = 3\n",
    "tag_idx = 1\n",
    "char_idx = 3\n",
    "\n",
    "def process_string(string):\n",
    "    string = re.sub('[^A-Za-z0-9\\-\\/ ]+', ' ', string).split()\n",
    "    return [to_title(y.strip()) for y in string]\n",
    "\n",
    "def to_title(string):\n",
    "    if string.isupper():\n",
    "        string = string.title()\n",
    "    return string\n",
    "\n",
    "def process_corpus(corpus, until = None):\n",
    "    global word2idx, tag2idx, char2idx, word_idx, tag_idx, char_idx\n",
    "    sentences, words, depends, labels, pos = [], [], [], [], []\n",
    "    temp_sentence, temp_word, temp_depend, temp_label, temp_pos = [], [], [], [], []\n",
    "    for sentence in corpus:\n",
    "        if len(sentence):\n",
    "            if sentence[0] == '#':\n",
    "                continue\n",
    "            sentence = sentence.split('\\t')\n",
    "            temp = process_string(sentence[1])\n",
    "            if not len(temp):\n",
    "                sentence[1] = 'EMPTY'\n",
    "            sentence[1] = process_string(sentence[1])[0]\n",
    "            for c in sentence[1]:\n",
    "                if c not in char2idx:\n",
    "                    char2idx[c] = char_idx\n",
    "                    char_idx += 1\n",
    "            if sentence[7] not in tag2idx:\n",
    "                tag2idx[sentence[7]] = tag_idx\n",
    "                tag_idx += 1\n",
    "            if sentence[1] not in word2idx:\n",
    "                word2idx[sentence[1]] = word_idx\n",
    "                word_idx += 1\n",
    "            temp_word.append(word2idx[sentence[1]])\n",
    "            temp_depend.append(int(sentence[6]) + 1)\n",
    "            temp_label.append(tag2idx[sentence[7]])\n",
    "            temp_sentence.append(sentence[1])\n",
    "            temp_pos.append(sentence[3])\n",
    "        else:\n",
    "            words.append(temp_word)\n",
    "            depends.append(temp_depend)\n",
    "            labels.append(temp_label)\n",
    "            sentences.append(temp_sentence)\n",
    "            pos.append(temp_pos)\n",
    "            temp_word = []\n",
    "            temp_depend = []\n",
    "            temp_label = []\n",
    "            temp_sentence = []\n",
    "            temp_pos = []\n",
    "    return sentences[:-1], words[:-1], depends[:-1], labels[:-1], pos[:-1]\n",
    "        \n",
    "sentences, words, depends, labels, pos = process_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('augmented.json') as fopen:\n",
    "    augmented = json.load(fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_XY(texts):\n",
    "    global word2idx, tag2idx, char2idx, word_idx, tag_idx, char_idx\n",
    "    outside, sentences = [], []\n",
    "    for no, text in enumerate(texts):\n",
    "        s = process_string(text)\n",
    "        sentences.append(s)\n",
    "        inside = []\n",
    "        for w in s:\n",
    "            for c in w:\n",
    "                if c not in char2idx:\n",
    "                    char2idx[c] = char_idx\n",
    "                    char_idx += 1\n",
    "            \n",
    "            if w not in word2idx:\n",
    "                word2idx[w] = word_idx\n",
    "                word_idx += 1\n",
    "                \n",
    "            inside.append(word2idx[w])\n",
    "        outside.append(inside)\n",
    "    return outside, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_augmented = []\n",
    "for a in augmented:\n",
    "    text_augmented.extend(a[0])\n",
    "    depends.extend(a[1])\n",
    "    labels.extend(a[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "outside, new_sentences = parse_XY(text_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.extend(outside)\n",
    "sentences.extend(new_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50365, 50365, 50365, 50365)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words), len(depends), len(labels), len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_char_seq(batch, UNK = 2):\n",
    "    maxlen_c = max([len(k) for k in batch])\n",
    "    x = [[len(i) for i in k] for k in batch]\n",
    "    maxlen = max([j for i in x for j in i])\n",
    "    temp = np.zeros((len(batch),maxlen_c,maxlen),dtype=np.int32)\n",
    "    for i in range(len(batch)):\n",
    "        for k in range(len(batch[i])):\n",
    "            for no, c in enumerate(batch[i][k][:maxlen][::-1]):\n",
    "                temp[i,k,-1-no] = char2idx.get(c, UNK)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {idx: tag for tag, idx in word2idx.items()}\n",
    "idx2tag = {i: w for w, i in tag2idx.items()}\n",
    "char = generate_char_seq(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50365, 189)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = pad_sequences(words,padding='post')\n",
    "depends = pad_sequences(depends,padding='post')\n",
    "labels = pad_sequences(labels,padding='post')\n",
    "words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "train_X, test_X, train_Y, test_Y, train_depends, test_depends, train_char, test_char = train_test_split(\n",
    "                                                                           words,\n",
    "                                                                           labels,\n",
    "                                                                           depends,\n",
    "                                                                           char,\n",
    "                                                                           test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_word,\n",
    "        dim_char,\n",
    "        dropout,\n",
    "        learning_rate,\n",
    "        hidden_size_char,\n",
    "        hidden_size_word,\n",
    "        num_layers,\n",
    "        maxlen\n",
    "    ):\n",
    "        def cells(size, reuse = False):\n",
    "            return tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.nn.rnn_cell.LSTMCell(\n",
    "                    size,\n",
    "                    initializer = tf.orthogonal_initializer(),\n",
    "                    reuse = reuse,\n",
    "                ),\n",
    "                output_keep_prob = dropout,\n",
    "            )\n",
    "\n",
    "        def luong(embedded, size):\n",
    "            attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "                num_units = hidden_size_word, memory = embedded\n",
    "            )\n",
    "            return tf.contrib.seq2seq.AttentionWrapper(\n",
    "                cell = cells(hidden_size_word),\n",
    "                attention_mechanism = attention_mechanism,\n",
    "                attention_layer_size = hidden_size_word,\n",
    "            )\n",
    "        self.word_ids = tf.placeholder(tf.int32, shape = [None, None])\n",
    "        self.char_ids = tf.placeholder(tf.int32, shape = [None, None, None])\n",
    "        self.labels = tf.placeholder(tf.int32, shape = [None, None])\n",
    "        self.depends = tf.placeholder(tf.int32, shape = [None, None])\n",
    "        self.maxlen = tf.shape(self.word_ids)[1]\n",
    "        self.lengths = tf.count_nonzero(self.word_ids, 1)\n",
    "\n",
    "        self.word_embeddings = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [len(word2idx), dim_word], stddev = 1.0 / np.sqrt(dim_word)\n",
    "            )\n",
    "        )\n",
    "        self.char_embeddings = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [len(char2idx), dim_char], stddev = 1.0 / np.sqrt(dim_char)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        word_embedded = tf.nn.embedding_lookup(\n",
    "            self.word_embeddings, self.word_ids\n",
    "        )\n",
    "        char_embedded = tf.nn.embedding_lookup(\n",
    "            self.char_embeddings, self.char_ids\n",
    "        )\n",
    "        s = tf.shape(char_embedded)\n",
    "        char_embedded = tf.reshape(\n",
    "            char_embedded, shape = [s[0] * s[1], s[-2], dim_char]\n",
    "        )\n",
    "        for n in range(num_layers):\n",
    "            (out_fw, out_bw), (\n",
    "                state_fw,\n",
    "                state_bw,\n",
    "            ) = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw = cells(hidden_size_char),\n",
    "                cell_bw = cells(hidden_size_char),\n",
    "                inputs = char_embedded,\n",
    "                dtype = tf.float32,\n",
    "                scope = 'bidirectional_rnn_char_%d' % (n),\n",
    "            )\n",
    "            char_embedded = tf.concat((out_fw, out_bw), 2)\n",
    "        output = tf.reshape(\n",
    "            char_embedded[:, -1], shape = [s[0], s[1], 2 * hidden_size_char]\n",
    "        )\n",
    "        word_embedded = tf.concat([word_embedded, output], axis = -1)\n",
    "\n",
    "        for n in range(num_layers):\n",
    "            (out_fw, out_bw), (\n",
    "                state_fw,\n",
    "                state_bw,\n",
    "            ) = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw = luong(word_embedded, hidden_size_word),\n",
    "                cell_bw = luong(word_embedded, hidden_size_word),\n",
    "                inputs = word_embedded,\n",
    "                dtype = tf.float32,\n",
    "                scope = 'bidirectional_rnn_word_%d' % (n),\n",
    "            )\n",
    "            word_embedded = tf.concat((out_fw, out_bw), 2)\n",
    "\n",
    "        logits = tf.layers.dense(word_embedded, len(idx2tag))\n",
    "        tag_embeddings = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [len(idx2tag), dim_word], stddev = 1.0 / np.sqrt(dim_word)\n",
    "            )\n",
    "        )\n",
    "        logits_max = tf.argmax(logits,axis=2,output_type=tf.int32)\n",
    "        lookup_logits = tf.nn.embedding_lookup(\n",
    "            tag_embeddings, logits_max\n",
    "        )\n",
    "        (out_fw, out_bw), _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw = cells(hidden_size_word),\n",
    "                cell_bw = cells(hidden_size_word),\n",
    "                inputs = word_embedded,\n",
    "                dtype = tf.float32,\n",
    "                scope = 'bidirectional_rnn_word_%d' % (n),\n",
    "            )\n",
    "        \n",
    "        cast_mask = tf.cast(tf.sequence_mask(self.lengths + 1, maxlen = maxlen), dtype = tf.float32)\n",
    "        cast_mask = tf.tile(tf.expand_dims(cast_mask,axis=1),[1,self.maxlen,1]) * 10\n",
    "        \n",
    "        lookup_logits = tf.concat((out_fw, out_bw), 2)\n",
    "        logits_depends = tf.layers.dense(lookup_logits, maxlen)\n",
    "        \n",
    "        logits_depends = tf.multiply(logits_depends, cast_mask)\n",
    "        \n",
    "        log_likelihood, transition_params = tf.contrib.crf.crf_log_likelihood(\n",
    "            logits, self.labels, self.lengths\n",
    "        )\n",
    "        with tf.variable_scope(\"depends\"):\n",
    "            log_likelihood_depends, transition_params_depends = tf.contrib.crf.crf_log_likelihood(\n",
    "                logits_depends, self.depends, self.lengths\n",
    "            )\n",
    "        self.cost = tf.reduce_mean(-log_likelihood) + tf.reduce_mean(-log_likelihood_depends)\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate = learning_rate\n",
    "        ).minimize(self.cost)\n",
    "        \n",
    "        mask = tf.sequence_mask(self.lengths, maxlen = self.maxlen)\n",
    "        \n",
    "        self.tags_seq, _ = tf.contrib.crf.crf_decode(\n",
    "            logits, transition_params, self.lengths\n",
    "        )\n",
    "        self.tags_seq = tf.identity(self.tags_seq, name = 'logits')\n",
    "        \n",
    "        self.tags_seq_depends, _ = tf.contrib.crf.crf_decode(\n",
    "            logits_depends, transition_params_depends, self.lengths\n",
    "        )\n",
    "        self.tags_seq_depends = tf.identity(self.tags_seq_depends, name = 'logits_depends')\n",
    "\n",
    "        self.prediction = tf.boolean_mask(self.tags_seq, mask)\n",
    "        mask_label = tf.boolean_mask(self.labels, mask)\n",
    "        correct_pred = tf.equal(self.prediction, mask_label)\n",
    "        correct_index = tf.cast(correct_pred, tf.float32)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        \n",
    "        self.prediction = tf.boolean_mask(self.tags_seq_depends, mask)\n",
    "        mask_label = tf.boolean_mask(self.depends, mask)\n",
    "        correct_pred = tf.equal(self.prediction, mask_label)\n",
    "        correct_index = tf.cast(correct_pred, tf.float32)\n",
    "        self.accuracy_depends = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "dim_word = 128\n",
    "dim_char = 256\n",
    "dropout = 0.85\n",
    "learning_rate = 1e-3\n",
    "hidden_size_char = 128\n",
    "hidden_size_word = 64\n",
    "num_layers = 2\n",
    "batch_size = 16\n",
    "\n",
    "model = Model(dim_word,dim_char,dropout,learning_rate,hidden_size_char,hidden_size_word,num_layers,\n",
    "             words.shape[1])\n",
    "sess.run(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 2833/2833 [1:12:52<00:00,  1.54s/it, accuracy=0.875, accuracy_depends=0.693, cost=24.4]\n",
      "test minibatch loop: 100%|██████████| 315/315 [03:10<00:00,  1.75it/s, accuracy=0.829, accuracy_depends=0.665, cost=29]  \n",
      "train minibatch loop:   0%|          | 0/2833 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 4563.180447816849\n",
      "epoch: 0, training loss: 61.783961, training acc: 0.686706, training depends: 0.425950, valid loss: 39.344299, valid acc: 0.825150, valid depends: 0.600830\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 2833/2833 [1:12:42<00:00,  1.54s/it, accuracy=0.922, accuracy_depends=0.851, cost=13.2]\n",
      "test minibatch loop: 100%|██████████| 315/315 [03:09<00:00,  1.74it/s, accuracy=0.894, accuracy_depends=0.8, cost=16]    \n",
      "train minibatch loop:   0%|          | 0/2833 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 4551.800651788712\n",
      "epoch: 1, training loss: 28.065757, training acc: 0.868640, training depends: 0.720747, valid loss: 24.269384, valid acc: 0.883178, valid depends: 0.773814\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 2833/2833 [1:12:40<00:00,  1.54s/it, accuracy=0.973, accuracy_depends=0.939, cost=5.88]\n",
      "test minibatch loop: 100%|██████████| 315/315 [03:09<00:00,  1.75it/s, accuracy=0.935, accuracy_depends=0.914, cost=9.33]\n",
      "train minibatch loop:   0%|          | 0/2833 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 4549.277266025543\n",
      "epoch: 2, training loss: 16.749653, training acc: 0.919367, training depends: 0.835569, valid loss: 17.442899, valid acc: 0.911510, valid depends: 0.851503\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 2833/2833 [1:12:38<00:00,  1.54s/it, accuracy=0.983, accuracy_depends=0.949, cost=3.36]\n",
      "test minibatch loop: 100%|██████████| 315/315 [03:09<00:00,  1.74it/s, accuracy=0.939, accuracy_depends=0.935, cost=6.91]\n",
      "train minibatch loop:   0%|          | 0/2833 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 4547.3607811927795\n",
      "epoch: 3, training loss: 11.579865, training acc: 0.945103, training depends: 0.885029, valid loss: 14.599322, valid acc: 0.926866, valid depends: 0.878660\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 2833/2833 [1:12:39<00:00,  1.53s/it, accuracy=0.99, accuracy_depends=0.956, cost=3.35] \n",
      "test minibatch loop: 100%|██████████| 315/315 [03:09<00:00,  1.73it/s, accuracy=0.955, accuracy_depends=0.959, cost=6.04]\n",
      "train minibatch loop:   0%|          | 0/2833 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 4548.768400430679\n",
      "epoch: 4, training loss: 7.585491, training acc: 0.963065, training depends: 0.925449, valid loss: 12.118104, valid acc: 0.938473, valid depends: 0.908994\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 2833/2833 [1:12:38<00:00,  1.54s/it, accuracy=0.997, accuracy_depends=0.959, cost=1.85] \n",
      "test minibatch loop: 100%|██████████| 315/315 [03:09<00:00,  1.75it/s, accuracy=0.935, accuracy_depends=0.943, cost=6.83]\n",
      "train minibatch loop:   0%|          | 0/2833 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 4548.058335542679\n",
      "epoch: 5, training loss: 6.261957, training acc: 0.971101, training depends: 0.938693, valid loss: 11.085619, valid acc: 0.946185, valid depends: 0.923939\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 2833/2833 [1:12:43<00:00,  1.55s/it, accuracy=0.99, accuracy_depends=0.973, cost=1.6]   \n",
      "test minibatch loop: 100%|██████████| 315/315 [03:09<00:00,  1.73it/s, accuracy=0.951, accuracy_depends=0.955, cost=5.28]\n",
      "train minibatch loop:   0%|          | 0/2833 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 4553.394236087799\n",
      "epoch: 6, training loss: 4.545035, training acc: 0.978248, training depends: 0.955501, valid loss: 10.506786, valid acc: 0.949097, valid depends: 0.932812\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 2833/2833 [1:12:42<00:00,  1.54s/it, accuracy=0.997, accuracy_depends=0.976, cost=1.69] \n",
      "test minibatch loop: 100%|██████████| 315/315 [03:09<00:00,  1.73it/s, accuracy=0.951, accuracy_depends=0.959, cost=4.65]\n",
      "train minibatch loop:   0%|          | 0/2833 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 4552.490092277527\n",
      "epoch: 7, training loss: 3.915957, training acc: 0.982050, training depends: 0.961163, valid loss: 10.037659, valid acc: 0.952602, valid depends: 0.937374\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 2833/2833 [1:12:38<00:00,  1.54s/it, accuracy=0.983, accuracy_depends=0.929, cost=4.88] \n",
      "test minibatch loop: 100%|██████████| 315/315 [03:09<00:00,  1.72it/s, accuracy=0.931, accuracy_depends=0.922, cost=9.41]\n",
      "train minibatch loop:   0%|          | 0/2833 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 4547.812601804733\n",
      "epoch: 8, training loss: 3.559482, training acc: 0.984057, training depends: 0.964962, valid loss: 17.300933, valid acc: 0.924944, valid depends: 0.872288\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 2833/2833 [1:12:46<00:00,  1.54s/it, accuracy=1, accuracy_depends=0.99, cost=1.06]      \n",
      "test minibatch loop: 100%|██████████| 315/315 [03:09<00:00,  1.75it/s, accuracy=0.947, accuracy_depends=0.988, cost=2.97]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 4555.395034074783\n",
      "epoch: 9, training loss: 3.126417, training acc: 0.985561, training depends: 0.969058, valid loss: 9.102047, valid acc: 0.957782, valid depends: 0.949280\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for e in range(10):\n",
    "    lasttime = time.time()\n",
    "    train_acc, train_loss, test_acc, test_loss, train_acc_depends, test_acc_depends = 0, 0, 0, 0, 0, 0\n",
    "    pbar = tqdm(\n",
    "        range(0, len(train_X), batch_size), desc = 'train minibatch loop'\n",
    "    )\n",
    "    for i in pbar:\n",
    "        batch_x = train_X[i : min(i + batch_size, train_X.shape[0])]\n",
    "        batch_char = train_char[i : min(i + batch_size, train_X.shape[0])]\n",
    "        batch_y = train_Y[i : min(i + batch_size, train_X.shape[0])]\n",
    "        batch_depends = train_depends[i : min(i + batch_size, train_X.shape[0])]\n",
    "        acc_depends, acc, cost, _ = sess.run(\n",
    "            [model.accuracy_depends, model.accuracy, model.cost, model.optimizer],\n",
    "            feed_dict = {\n",
    "                model.word_ids: batch_x,\n",
    "                model.char_ids: batch_char,\n",
    "                model.labels: batch_y,\n",
    "                model.depends: batch_depends\n",
    "            },\n",
    "        )\n",
    "        assert not np.isnan(cost)\n",
    "        train_loss += cost\n",
    "        train_acc += acc\n",
    "        train_acc_depends += acc_depends\n",
    "        pbar.set_postfix(cost = cost, accuracy = acc, accuracy_depends = acc_depends)\n",
    "    pbar = tqdm(\n",
    "        range(0, len(test_X), batch_size), desc = 'test minibatch loop'\n",
    "    )\n",
    "    for i in pbar:\n",
    "        batch_x = test_X[i : min(i + batch_size, test_X.shape[0])]\n",
    "        batch_char = test_char[i : min(i + batch_size, test_X.shape[0])]\n",
    "        batch_y = test_Y[i : min(i + batch_size, test_X.shape[0])]\n",
    "        batch_depends = test_depends[i : min(i + batch_size, test_X.shape[0])]\n",
    "        acc_depends, acc, cost = sess.run(\n",
    "            [model.accuracy_depends, model.accuracy, model.cost],\n",
    "            feed_dict = {\n",
    "                model.word_ids: batch_x,\n",
    "                model.char_ids: batch_char,\n",
    "                model.labels: batch_y,\n",
    "                model.depends: batch_depends\n",
    "            },\n",
    "        )\n",
    "        assert not np.isnan(cost)\n",
    "        test_loss += cost\n",
    "        test_acc += acc\n",
    "        test_acc_depends += acc_depends\n",
    "        pbar.set_postfix(cost = cost, accuracy = acc, accuracy_depends = acc_depends)\n",
    "    \n",
    "    train_loss /= len(train_X) / batch_size\n",
    "    train_acc /= len(train_X) / batch_size\n",
    "    train_acc_depends /= len(train_X) / batch_size\n",
    "    test_loss /= len(test_X) / batch_size\n",
    "    test_acc /= len(test_X) / batch_size\n",
    "    test_acc_depends /= len(test_X) / batch_size\n",
    "\n",
    "    print('time taken:', time.time() - lasttime)\n",
    "    print(\n",
    "        'epoch: %d, training loss: %f, training acc: %f, training depends: %f, valid loss: %f, valid acc: %f, valid depends: %f\\n'\n",
    "        % (e, train_loss, train_acc, train_acc_depends, test_loss, test_acc, test_acc_depends)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred2label(pred):\n",
    "    out = []\n",
    "    for pred_i in pred:\n",
    "        out_i = []\n",
    "        for p in pred_i:\n",
    "            out_i.append(idx2tag[p])\n",
    "        out.append(out_i)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq, deps = sess.run([model.tags_seq, model.tags_seq_depends],\n",
    "            feed_dict = {\n",
    "                model.word_ids: batch_x,\n",
    "                model.char_ids: batch_char,\n",
    "            },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation minibatch loop: 100%|██████████| 315/315 [03:04<00:00,  1.75it/s]\n"
     ]
    }
   ],
   "source": [
    "real_Y, predict_Y, real_depends, predict_depends = [], [], [], []\n",
    "\n",
    "pbar = tqdm(\n",
    "    range(0, len(test_X), batch_size), desc = 'validation minibatch loop'\n",
    ")\n",
    "for i in pbar:\n",
    "    batch_x = test_X[i : min(i + batch_size, test_X.shape[0])]\n",
    "    batch_char = test_char[i : min(i + batch_size, test_X.shape[0])]\n",
    "    batch_y = test_Y[i : min(i + batch_size, test_X.shape[0])]\n",
    "    batch_depends = test_depends[i : min(i + batch_size, test_X.shape[0])]\n",
    "    seq, deps = sess.run([model.tags_seq, model.tags_seq_depends],\n",
    "            feed_dict = {\n",
    "                model.word_ids: batch_x,\n",
    "                model.char_ids: batch_char,\n",
    "            },\n",
    "    )\n",
    "    predicted = pred2label(seq)\n",
    "    real = pred2label(batch_y)\n",
    "    predict_Y.extend(predicted)\n",
    "    real_Y.extend(real)\n",
    "    \n",
    "    real_depends.extend(batch_depends.tolist())\n",
    "    predict_depends.extend(deps.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "          PAD     1.0000    1.0000    1.0000    840905\n",
      "          acl     0.9249    0.9392    0.9320      3094\n",
      "        advcl     0.8952    0.8478    0.8709      1209\n",
      "       advmod     0.9629    0.9475    0.9551      4952\n",
      "         amod     0.9288    0.9246    0.9267      4218\n",
      "        appos     0.9535    0.9204    0.9367      2426\n",
      "          aux     1.0000    1.0000    1.0000         1\n",
      "         case     0.9796    0.9795    0.9796     10991\n",
      "           cc     0.9686    0.9739    0.9713      3298\n",
      "        ccomp     0.8426    0.8501    0.8463       447\n",
      "     compound     0.9170    0.9477    0.9321      6787\n",
      "compound:plur     0.9428    0.9744    0.9584       626\n",
      "         conj     0.9539    0.9581    0.9560      4251\n",
      "          cop     0.9625    0.9809    0.9716       993\n",
      "        csubj     0.9655    0.8750    0.9180        32\n",
      "   csubj:pass     1.0000    0.9167    0.9565        12\n",
      "          dep     0.8905    0.8320    0.8603       518\n",
      "          det     0.9503    0.9364    0.9433      4088\n",
      "        fixed     0.9113    0.8899    0.9005       554\n",
      "         flat     0.9596    0.9792    0.9693     10272\n",
      "         iobj     1.0000    0.6000    0.7500        15\n",
      "         mark     0.9396    0.9217    0.9305      1417\n",
      "         nmod     0.9086    0.9475    0.9277      4155\n",
      "        nsubj     0.9524    0.9547    0.9535      6483\n",
      "   nsubj:pass     0.9402    0.9108    0.9252      1916\n",
      "       nummod     0.9747    0.9761    0.9754      4022\n",
      "          obj     0.9559    0.9468    0.9513      5337\n",
      "          obl     0.9622    0.9242    0.9428      5727\n",
      "    parataxis     0.8072    0.8910    0.8470       376\n",
      "        punct     0.9972    0.9984    0.9978     16581\n",
      "         root     0.9646    0.9688    0.9667      5037\n",
      "        xcomp     0.9225    0.8364    0.8774      1253\n",
      "\n",
      "  avg / total     0.9950    0.9950    0.9950    951993\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(np.array(real_Y).ravel(), np.array(predict_Y).ravel(), digits = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     1.0000    1.0000    1.0000    840905\n",
      "          1     0.9709    0.9726    0.9717      5037\n",
      "          2     0.9310    0.9534    0.9420      4271\n",
      "          3     0.9543    0.9485    0.9514      5148\n",
      "          4     0.9587    0.9514    0.9551      6220\n",
      "          5     0.9471    0.9631    0.9550      5984\n",
      "          6     0.9593    0.9585    0.9589      5827\n",
      "          7     0.9597    0.9554    0.9576      5789\n",
      "          8     0.9657    0.9527    0.9592      5559\n",
      "          9     0.9548    0.9517    0.9532      5088\n",
      "         10     0.9565    0.9492    0.9528      4427\n",
      "         11     0.9458    0.9631    0.9544      4280\n",
      "         12     0.9584    0.9540    0.9562      3910\n",
      "         13     0.9481    0.9586    0.9533      3791\n",
      "         14     0.9385    0.9563    0.9473      3272\n",
      "         15     0.9577    0.9389    0.9482      3306\n",
      "         16     0.9383    0.9560    0.9471      3023\n",
      "         17     0.9629    0.9417    0.9522      2815\n",
      "         18     0.9384    0.9548    0.9465      2409\n",
      "         19     0.9463    0.9391    0.9427      2103\n",
      "         20     0.9349    0.9617    0.9481      2166\n",
      "         21     0.9712    0.9354    0.9530      2090\n",
      "         22     0.9525    0.9450    0.9487      1763\n",
      "         23     0.9512    0.9512    0.9512      1742\n",
      "         24     0.9624    0.9475    0.9549      1619\n",
      "         25     0.9439    0.9460    0.9449      1333\n",
      "         26     0.9584    0.9333    0.9457      1260\n",
      "         27     0.9443    0.9231    0.9336      1158\n",
      "         28     0.9384    0.9414    0.9399       955\n",
      "         29     0.9313    0.9417    0.9365      1080\n",
      "         30     0.9332    0.9323    0.9327      1004\n",
      "         31     0.9240    0.9404    0.9322       789\n",
      "         32     0.9500    0.9226    0.9361       762\n",
      "         33     0.9292    0.9502    0.9396       843\n",
      "         34     0.9553    0.9468    0.9510       677\n",
      "         35     0.9284    0.9396    0.9339       662\n",
      "         36     0.9238    0.9287    0.9262       561\n",
      "         37     0.9213    0.9152    0.9183       448\n",
      "         38     0.8978    0.9114    0.9045       395\n",
      "         39     0.8991    0.9114    0.9052       440\n",
      "         40     0.9262    0.9446    0.9353       505\n",
      "         41     0.9289    0.9098    0.9193       388\n",
      "         42     0.9544    0.9181    0.9359       342\n",
      "         43     0.9119    0.9308    0.9212       289\n",
      "         44     0.9106    0.9006    0.9056       362\n",
      "         45     0.8525    0.9091    0.8799       286\n",
      "         46     0.9283    0.8859    0.9066       263\n",
      "         47     0.9068    0.8924    0.8995       316\n",
      "         48     0.9282    0.9095    0.9188       199\n",
      "         49     0.9648    0.9202    0.9419       238\n",
      "         50     0.9274    0.9583    0.9426       120\n",
      "         51     0.9167    0.9585    0.9371       241\n",
      "         52     0.9507    0.9415    0.9461       205\n",
      "         53     0.9248    0.9179    0.9213       134\n",
      "         54     0.9200    0.9306    0.9253       173\n",
      "         55     0.9329    0.8910    0.9115       156\n",
      "         56     0.9073    0.8954    0.9013       153\n",
      "         57     0.9304    0.9469    0.9386       113\n",
      "         58     0.9417    0.9576    0.9496       118\n",
      "         59     0.8947    0.8500    0.8718       100\n",
      "         60     0.9770    0.8095    0.8854       105\n",
      "         61     0.8020    0.9576    0.8729       165\n",
      "         62     0.8767    0.8889    0.8828        72\n",
      "         63     0.9355    0.8365    0.8832       104\n",
      "         64     0.8852    0.8308    0.8571        65\n",
      "         65     0.9375    0.8955    0.9160        67\n",
      "         66     0.8690    0.8588    0.8639        85\n",
      "         67     0.9839    0.8472    0.9104        72\n",
      "         68     0.9223    0.9500    0.9360       100\n",
      "         69     0.9367    0.9250    0.9308        80\n",
      "         70     0.8442    0.9701    0.9028        67\n",
      "         71     0.8462    0.8462    0.8462        65\n",
      "         72     0.9200    0.8734    0.8961        79\n",
      "         73     0.8909    0.8596    0.8750        57\n",
      "         74     0.9487    0.8810    0.9136        42\n",
      "         75     0.9296    0.8919    0.9103        74\n",
      "         76     0.8333    0.9677    0.8955        31\n",
      "         77     0.8056    0.9062    0.8529        32\n",
      "         78     0.8750    0.8077    0.8400        26\n",
      "         79     0.7636    0.9333    0.8400        45\n",
      "         80     0.9180    0.8889    0.9032        63\n",
      "         81     0.7188    0.8214    0.7667        28\n",
      "         82     0.8983    0.9298    0.9138        57\n",
      "         83     1.0000    0.8571    0.9231        28\n",
      "         84     0.8605    0.9487    0.9024        39\n",
      "         85     0.9474    0.9474    0.9474        19\n",
      "         86     0.8919    0.9706    0.9296        34\n",
      "         87     0.9231    0.8571    0.8889        14\n",
      "         88     0.9474    0.7826    0.8571        23\n",
      "         89     1.0000    0.8571    0.9231        14\n",
      "         90     0.8929    0.8621    0.8772        29\n",
      "         91     0.8462    0.9429    0.8919        35\n",
      "         92     0.9333    0.7568    0.8358        37\n",
      "         93     0.7895    0.8333    0.8108        18\n",
      "         94     1.0000    0.8000    0.8889        20\n",
      "         95     0.9048    0.9500    0.9268        20\n",
      "         96     0.9412    0.9412    0.9412        17\n",
      "         97     0.9583    1.0000    0.9787        23\n",
      "         98     0.9000    1.0000    0.9474         9\n",
      "         99     1.0000    0.9643    0.9818        28\n",
      "        100     0.8333    1.0000    0.9091         5\n",
      "        101     1.0000    0.9231    0.9600        13\n",
      "        102     1.0000    1.0000    1.0000        13\n",
      "        103     0.8750    1.0000    0.9333        14\n",
      "        104     1.0000    0.9231    0.9600        26\n",
      "        105     1.0000    0.9167    0.9565        12\n",
      "        106     0.9444    0.8500    0.8947        20\n",
      "        107     1.0000    0.8571    0.9231        21\n",
      "        108     1.0000    1.0000    1.0000        20\n",
      "        109     1.0000    1.0000    1.0000         6\n",
      "        110     0.8750    1.0000    0.9333         7\n",
      "        111     1.0000    1.0000    1.0000         4\n",
      "        112     0.9200    0.9583    0.9388        24\n",
      "        113     0.8889    1.0000    0.9412         8\n",
      "        114     1.0000    0.6667    0.8000         3\n",
      "        115     1.0000    1.0000    1.0000         5\n",
      "        116     0.9474    0.8571    0.9000        21\n",
      "        117     0.6667    1.0000    0.8000         2\n",
      "        119     1.0000    1.0000    1.0000         3\n",
      "        120     0.8824    0.9375    0.9091        16\n",
      "        121     1.0000    0.8000    0.8889         5\n",
      "        122     0.8889    1.0000    0.9412         8\n",
      "        123     0.0000    0.0000    0.0000         2\n",
      "        124     1.0000    0.6667    0.8000         3\n",
      "        125     1.0000    1.0000    1.0000         8\n",
      "        126     1.0000    0.8000    0.8889        10\n",
      "        127     1.0000    1.0000    1.0000         3\n",
      "        128     1.0000    1.0000    1.0000         1\n",
      "        129     1.0000    1.0000    1.0000         5\n",
      "        130     1.0000    0.8333    0.9091        12\n",
      "        131     1.0000    1.0000    1.0000         2\n",
      "        132     1.0000    1.0000    1.0000         1\n",
      "        133     1.0000    1.0000    1.0000         9\n",
      "        134     1.0000    1.0000    1.0000         6\n",
      "        136     1.0000    1.0000    1.0000         3\n",
      "        137     1.0000    1.0000    1.0000        10\n",
      "        138     1.0000    1.0000    1.0000        10\n",
      "        140     1.0000    1.0000    1.0000         4\n",
      "        141     1.0000    1.0000    1.0000         2\n",
      "        142     0.4000    0.5000    0.4444         4\n",
      "        144     0.5714    1.0000    0.7273         4\n",
      "        146     0.7500    0.7500    0.7500         4\n",
      "        147     1.0000    0.6000    0.7500         5\n",
      "        149     1.0000    1.0000    1.0000         2\n",
      "        150     0.6667    0.6667    0.6667         3\n",
      "        151     0.5000    0.5000    0.5000         2\n",
      "        152     0.5000    0.5000    0.5000         2\n",
      "        153     1.0000    0.5000    0.6667         2\n",
      "        156     1.0000    1.0000    1.0000         2\n",
      "        158     0.8889    1.0000    0.9412         8\n",
      "        160     0.8000    1.0000    0.8889         4\n",
      "        164     1.0000    1.0000    1.0000         4\n",
      "\n",
      "avg / total     0.9941    0.9941    0.9941    951993\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(np.array(real_depends).ravel(), \n",
    "                            np.array(predict_depends).ravel(), digits = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tolong', 'tangkap', 'gambar', 'kami']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = 'tolong tangkap gambar kami'\n",
    "\n",
    "def char_str_idx(corpus, dic, UNK = 0):\n",
    "    maxlen = max([len(i) for i in corpus])\n",
    "    X = np.zeros((len(corpus), maxlen))\n",
    "    for i in range(len(corpus)):\n",
    "        for no, k in enumerate(corpus[i][:maxlen]):\n",
    "            val = dic[k] if k in dic else UNK\n",
    "            X[i, no] = val\n",
    "    return X\n",
    "\n",
    "def generate_char_seq(batch, UNK = 2):\n",
    "    maxlen_c = max([len(k) for k in batch])\n",
    "    x = [[len(i) for i in k] for k in batch]\n",
    "    maxlen = max([j for i in x for j in i])\n",
    "    temp = np.zeros((len(batch),maxlen_c,maxlen),dtype=np.int32)\n",
    "    for i in range(len(batch)):\n",
    "        for k in range(len(batch[i])):\n",
    "            for no, c in enumerate(batch[i][k][::-1]):\n",
    "                temp[i,k,-1-no] = char2idx.get(c, UNK)\n",
    "    return temp\n",
    "\n",
    "sequence = process_string(string)\n",
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_seq = char_str_idx([sequence], word2idx, 2)\n",
    "X_char_seq = generate_char_seq([sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4, 7)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_char_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq, deps = sess.run([model.tags_seq, model.tags_seq_depends],\n",
    "        feed_dict={model.word_ids:X_seq,\n",
    "                  model.char_ids:X_char_seq})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 2, 1], dtype=int32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deps[0] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['parataxis', 'compound', 'compound', 'det']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[idx2tag[i] for i in seq[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Placeholder',\n",
       " 'Placeholder_1',\n",
       " 'Placeholder_2',\n",
       " 'Placeholder_3',\n",
       " 'Variable',\n",
       " 'Variable_1',\n",
       " 'bidirectional_rnn_char_0/fw/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_char_0/fw/lstm_cell/bias',\n",
       " 'bidirectional_rnn_char_0/bw/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_char_0/bw/lstm_cell/bias',\n",
       " 'bidirectional_rnn_char_1/fw/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_char_1/fw/lstm_cell/bias',\n",
       " 'bidirectional_rnn_char_1/bw/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_char_1/bw/lstm_cell/bias',\n",
       " 'memory_layer/kernel',\n",
       " 'memory_layer_1/kernel',\n",
       " 'bidirectional_rnn_word_0/fw/attention_wrapper/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_word_0/fw/attention_wrapper/lstm_cell/bias',\n",
       " 'bidirectional_rnn_word_0/fw/attention_wrapper/attention_layer/kernel',\n",
       " 'bidirectional_rnn_word_0/bw/attention_wrapper/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_word_0/bw/attention_wrapper/lstm_cell/bias',\n",
       " 'bidirectional_rnn_word_0/bw/attention_wrapper/attention_layer/kernel',\n",
       " 'memory_layer_2/kernel',\n",
       " 'memory_layer_3/kernel',\n",
       " 'bidirectional_rnn_word_1/fw/attention_wrapper/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_word_1/fw/attention_wrapper/lstm_cell/bias',\n",
       " 'bidirectional_rnn_word_1/fw/attention_wrapper/attention_layer/kernel',\n",
       " 'bidirectional_rnn_word_1/bw/attention_wrapper/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_word_1/bw/attention_wrapper/lstm_cell/bias',\n",
       " 'bidirectional_rnn_word_1/bw/attention_wrapper/attention_layer/kernel',\n",
       " 'dense/kernel',\n",
       " 'dense/bias',\n",
       " 'Variable_2',\n",
       " 'bidirectional_rnn_word_1/fw/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_word_1/fw/lstm_cell/bias',\n",
       " 'bidirectional_rnn_word_1/bw/lstm_cell/kernel',\n",
       " 'bidirectional_rnn_word_1/bw/lstm_cell/bias',\n",
       " 'dense_1/kernel',\n",
       " 'dense_1/bias',\n",
       " 'transitions',\n",
       " 'depends/transitions',\n",
       " 'logits',\n",
       " 'logits_depends']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver(tf.trainable_variables())\n",
    "saver.save(sess, 'luong-dependency/model.ckpt')\n",
    "\n",
    "strings = ','.join(\n",
    "    [\n",
    "        n.name\n",
    "        for n in tf.get_default_graph().as_graph_def().node\n",
    "        if ('Variable' in n.op\n",
    "        or 'Placeholder' in n.name\n",
    "        or 'logits' in n.name\n",
    "        or 'logits_depends' in n.name\n",
    "        or 'alphas' in n.name)\n",
    "        and 'Adam' not in n.name\n",
    "        and 'beta' not in n.name\n",
    "        and 'OptimizeLoss' not in n.name\n",
    "        and 'Global_Step' not in n.name\n",
    "        and 'Epoch_Step' not in n.name\n",
    "        and 'learning_rate' not in n.name\n",
    "    ]\n",
    ")\n",
    "strings.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('luong-dependency.json','w') as fopen:\n",
    "    fopen.write(json.dumps({'idx2tag':idx2tag,'idx2word':idx2word,\n",
    "           'word2idx':word2idx,'tag2idx':tag2idx,'char2idx':char2idx}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_graph(model_dir, output_node_names):\n",
    "\n",
    "    if not tf.gfile.Exists(model_dir):\n",
    "        raise AssertionError(\n",
    "            \"Export directory doesn't exists. Please specify an export \"\n",
    "            'directory: %s' % model_dir\n",
    "        )\n",
    "\n",
    "    checkpoint = tf.train.get_checkpoint_state(model_dir)\n",
    "    input_checkpoint = checkpoint.model_checkpoint_path\n",
    "\n",
    "    absolute_model_dir = '/'.join(input_checkpoint.split('/')[:-1])\n",
    "    output_graph = absolute_model_dir + '/frozen_model.pb'\n",
    "    clear_devices = True\n",
    "    with tf.Session(graph = tf.Graph()) as sess:\n",
    "        saver = tf.train.import_meta_graph(\n",
    "            input_checkpoint + '.meta', clear_devices = clear_devices\n",
    "        )\n",
    "        saver.restore(sess, input_checkpoint)\n",
    "        output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "            sess,\n",
    "            tf.get_default_graph().as_graph_def(),\n",
    "            output_node_names.split(','),\n",
    "        )\n",
    "        with tf.gfile.GFile(output_graph, 'wb') as f:\n",
    "            f.write(output_graph_def.SerializeToString())\n",
    "        print('%d ops in the final graph.' % len(output_graph_def.node))\n",
    "        \n",
    "def load_graph(frozen_graph_filename):\n",
    "    with tf.gfile.GFile(frozen_graph_filename, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from luong-dependency/model.ckpt\n",
      "INFO:tensorflow:Froze 37 variables.\n",
      "INFO:tensorflow:Converted 37 variables to const ops.\n",
      "2491 ops in the final graph.\n"
     ]
    }
   ],
   "source": [
    "freeze_graph('luong-dependency', strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = load_graph('luong-dependency/frozen_model.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1 13 13  3]] [[3 1 0 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['nsubj', 'compound', 'compound', 'det']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids = g.get_tensor_by_name('import/Placeholder:0')\n",
    "char_ids = g.get_tensor_by_name('import/Placeholder_1:0')\n",
    "tags_seq = g.get_tensor_by_name('import/logits:0')\n",
    "depends_seq = g.get_tensor_by_name('import/logits_depends:0')\n",
    "test_sess = tf.InteractiveSession(graph = g)\n",
    "seq, deps = test_sess.run([tags_seq, depends_seq],\n",
    "            feed_dict = {\n",
    "                word_ids: X_seq,\n",
    "                char_ids: X_char_seq,\n",
    "            })\n",
    "\n",
    "print(seq,deps - 1)\n",
    "[idx2tag[i] for i in seq[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
