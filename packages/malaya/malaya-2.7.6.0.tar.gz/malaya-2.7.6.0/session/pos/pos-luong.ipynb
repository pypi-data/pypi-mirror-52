{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!wget https://huseinhouse-storage.s3-ap-southeast-1.amazonaws.com/bert-bahasa/dictionary-pos.json\n!wget https://huseinhouse-storage.s3-ap-southeast-1.amazonaws.com/bert-bahasa/session-pos.pkl","execution_count":1,"outputs":[{"output_type":"stream","text":"--2019-08-04 12:32:24--  https://huseinhouse-storage.s3-ap-southeast-1.amazonaws.com/bert-bahasa/dictionary-pos.json\nResolving huseinhouse-storage.s3-ap-southeast-1.amazonaws.com (huseinhouse-storage.s3-ap-southeast-1.amazonaws.com)... 52.219.32.155\nConnecting to huseinhouse-storage.s3-ap-southeast-1.amazonaws.com (huseinhouse-storage.s3-ap-southeast-1.amazonaws.com)|52.219.32.155|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 825070 (806K) [binary/octet-stream]\nSaving to: ‘dictionary-pos.json’\n\ndictionary-pos.json 100%[===================>] 805.73K   556KB/s    in 1.5s    \n\n2019-08-04 12:32:27 (556 KB/s) - ‘dictionary-pos.json’ saved [825070/825070]\n\n--2019-08-04 12:32:28--  https://huseinhouse-storage.s3-ap-southeast-1.amazonaws.com/bert-bahasa/session-pos.pkl\nResolving huseinhouse-storage.s3-ap-southeast-1.amazonaws.com (huseinhouse-storage.s3-ap-southeast-1.amazonaws.com)... 52.219.36.231\nConnecting to huseinhouse-storage.s3-ap-southeast-1.amazonaws.com (huseinhouse-storage.s3-ap-southeast-1.amazonaws.com)|52.219.36.231|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 97458750 (93M) [binary/octet-stream]\nSaving to: ‘session-pos.pkl’\n\nsession-pos.pkl     100%[===================>]  92.94M  10.2MB/s    in 11s     \n\n2019-08-04 12:32:40 (8.32 MB/s) - ‘session-pos.pkl’ saved [97458750/97458750]\n\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pickle\nimport json\nimport tensorflow as tf\nimport numpy as np","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('session-pos.pkl', 'rb') as fopen:\n    data = pickle.load(fopen)\ndata.keys()","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"dict_keys(['train_X', 'test_X', 'train_Y', 'test_Y'])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X = data['train_X']\ntest_X = data['test_X']\ntrain_Y = data['train_Y']\ntest_Y = data['test_Y']","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('dictionary-pos.json') as fopen:\n    dictionary = json.load(fopen)\ndictionary.keys()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"dict_keys(['word2idx', 'idx2word', 'tag2idx', 'idx2tag', 'char2idx'])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"word2idx = dictionary['word2idx']\nidx2word = {int(k): v for k, v in dictionary['idx2word'].items()}\ntag2idx = dictionary['tag2idx']\nidx2tag = {int(k): v for k, v in dictionary['idx2tag'].items()}\nchar2idx = dictionary['char2idx']","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(zip([idx2word[d] for d in train_X[-1]], [idx2tag[d] for d in train_Y[-1]]))","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"[('-', 'PUNCT'),\n ('film', 'NOUN'),\n ('yang', 'PRON'),\n ('dibuatnya', 'VERB'),\n ('akan', 'ADV'),\n ('segera', 'ADV'),\n ('tayang', 'VERB'),\n ('.', 'PUNCT'),\n ('Jadi', 'ADV'),\n ('dicoba', 'VERB'),\n ('untuk', 'ADP'),\n ('menjawab', 'VERB'),\n ('pertanyaan-pertanyaan', 'NOUN'),\n ('seperti', 'ADP'),\n ('kebutuhan', 'NOUN'),\n ('apa', 'PRON'),\n ('yang', 'PRON'),\n ('dicoba', 'VERB'),\n ('dipuaskan', 'VERB'),\n ('oleh', 'ADP'),\n ('seseorang', 'NOUN'),\n ('?', 'PUNCT'),\n ('Kamu', 'PRON'),\n ('selalu', 'ADV'),\n ('bertanya', 'VERB'),\n ('apa', 'PRON'),\n ('itu', 'DET'),\n ('Pi', 'PROPN'),\n ('?', 'PUNCT'),\n ('Bagaimana', 'PRON'),\n ('di', 'ADP'),\n ('Indonesia', 'PROPN'),\n ('?', 'PUNCT'),\n ('Grimes', 'PROPN'),\n ('merupakan', 'VERB'),\n ('sebuah', 'DET'),\n ('di', 'ADP'),\n ('Dale', 'PROPN'),\n (',', 'PUNCT'),\n ('Alabama', 'PROPN'),\n (',', 'PUNCT'),\n ('Amerika', 'PROPN'),\n ('Serikat', 'PROPN'),\n ('.', 'PUNCT'),\n ('Sampul', 'NOUN'),\n ('dari', 'ADP'),\n ('dua', 'NUM'),\n ('singel', 'NOUN'),\n ('pertama', 'NUM'),\n ('difoto', 'VERB')]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_char_seq(batch):\n    x = [[len(idx2word[i]) for i in k] for k in batch]\n    maxlen = max([j for i in x for j in i])\n    temp = np.zeros((batch.shape[0],batch.shape[1],maxlen),dtype=np.int32)\n    for i in range(batch.shape[0]):\n        for k in range(batch.shape[1]):\n            for no, c in enumerate(idx2word[batch[i,k]]):\n                temp[i,k,-1-no] = char2idx[c]\n    return temp","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generate_char_seq(data['train_X'][:10]).shape","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"(10, 50, 12)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model:\n    def __init__(\n        self,\n        dim_word,\n        dim_char,\n        dropout,\n        learning_rate,\n        hidden_size_char,\n        hidden_size_word,\n        num_layers,\n    ):\n        def cells(size, reuse = False):\n            return tf.contrib.rnn.DropoutWrapper(\n                tf.nn.rnn_cell.LSTMCell(\n                    size,\n                    initializer = tf.orthogonal_initializer(),\n                    reuse = reuse,\n                ),\n                output_keep_prob = dropout,\n            )\n\n        def luong(embedded, size):\n            attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n                num_units = hidden_size_word, memory = embedded\n            )\n            return tf.contrib.seq2seq.AttentionWrapper(\n                cell = cells(hidden_size_word),\n                attention_mechanism = attention_mechanism,\n                attention_layer_size = hidden_size_word,\n            )\n\n        self.word_ids = tf.placeholder(tf.int32, shape = [None, None])\n        self.char_ids = tf.placeholder(tf.int32, shape = [None, None, None])\n        self.labels = tf.placeholder(tf.int32, shape = [None, None])\n        self.maxlen = tf.shape(self.word_ids)[1]\n        self.lengths = tf.count_nonzero(self.word_ids, 1)\n        \n        self.word_embeddings = tf.Variable(\n            tf.truncated_normal(\n                [len(word2idx), dim_word], stddev = 1.0 / np.sqrt(dim_word)\n            )\n        )\n        self.char_embeddings = tf.Variable(\n            tf.truncated_normal(\n                [len(char2idx), dim_char], stddev = 1.0 / np.sqrt(dim_char)\n            )\n        )\n\n        word_embedded = tf.nn.embedding_lookup(\n            self.word_embeddings, self.word_ids\n        )\n        char_embedded = tf.nn.embedding_lookup(\n            self.char_embeddings, self.char_ids\n        )\n        s = tf.shape(char_embedded)\n        char_embedded = tf.reshape(\n            char_embedded, shape = [s[0] * s[1], s[-2], dim_char]\n        )\n        \n        for n in range(num_layers):\n            (out_fw, out_bw), (\n                state_fw,\n                state_bw,\n            ) = tf.nn.bidirectional_dynamic_rnn(\n                cell_fw = cells(hidden_size_char),\n                cell_bw = cells(hidden_size_char),\n                inputs = char_embedded,\n                dtype = tf.float32,\n                scope = 'bidirectional_rnn_char_%d' % (n),\n            )\n            char_embedded = tf.concat((out_fw, out_bw), 2)\n        output = tf.reshape(\n            char_embedded[:, -1], shape = [s[0], s[1], 2 * hidden_size_char]\n        )\n        word_embedded = tf.concat([word_embedded, output], axis = -1)\n\n        for n in range(num_layers):\n            (out_fw, out_bw), (\n                state_fw,\n                state_bw,\n            ) = tf.nn.bidirectional_dynamic_rnn(\n                cell_fw = luong(word_embedded, hidden_size_word),\n                cell_bw = luong(word_embedded, hidden_size_word),\n                inputs = word_embedded,\n                dtype = tf.float32,\n                scope = 'bidirectional_rnn_word_%d' % (n),\n            )\n            word_embedded = tf.concat((out_fw, out_bw), 2)\n\n        logits = tf.layers.dense(word_embedded, len(idx2tag))\n        y_t = self.labels\n        log_likelihood, transition_params = tf.contrib.crf.crf_log_likelihood(\n            logits, y_t, self.lengths\n        )\n        self.cost = tf.reduce_mean(-log_likelihood)\n        self.optimizer = tf.train.AdamOptimizer(\n            learning_rate = learning_rate\n        ).minimize(self.cost)\n        mask = tf.sequence_mask(self.lengths, maxlen = self.maxlen)\n        self.tags_seq, tags_score = tf.contrib.crf.crf_decode(\n            logits, transition_params, self.lengths\n        )\n        self.tags_seq = tf.identity(self.tags_seq, name = 'logits')\n\n        y_t = tf.cast(y_t, tf.int32)\n        self.prediction = tf.boolean_mask(self.tags_seq, mask)\n        mask_label = tf.boolean_mask(y_t, mask)\n        correct_pred = tf.equal(self.prediction, mask_label)\n        correct_index = tf.cast(correct_pred, tf.float32)\n        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model:\n    def __init__(\n        self,\n        dim_word,\n        dim_char,\n        dropout,\n        learning_rate,\n        hidden_size_char,\n        hidden_size_word,\n        num_layers,\n    ):\n        def cells(size, reuse = False):\n            return tf.contrib.rnn.DropoutWrapper(\n                tf.nn.rnn_cell.LSTMCell(\n                    size,\n                    initializer = tf.orthogonal_initializer(),\n                    reuse = reuse,\n                ),\n                output_keep_prob = dropout,\n            )\n\n        def luong(embedded, size):\n            attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n                num_units = hidden_size_word, memory = embedded\n            )\n            return tf.contrib.seq2seq.AttentionWrapper(\n                cell = cells(hidden_size_word),\n                attention_mechanism = attention_mechanism,\n                attention_layer_size = hidden_size_word,\n            )\n\n        self.word_ids = tf.placeholder(tf.int32, shape = [None, None])\n        self.char_ids = tf.placeholder(tf.int32, shape = [None, None, None])\n        self.labels = tf.placeholder(tf.int32, shape = [None, None])\n        self.maxlen = tf.shape(self.word_ids)[1]\n        self.lengths = tf.count_nonzero(self.word_ids, 1)\n        \n        self.word_embeddings = tf.Variable(\n            tf.truncated_normal(\n                [len(word2idx), dim_word], stddev = 1.0 / np.sqrt(dim_word)\n            )\n        )\n        self.char_embeddings = tf.Variable(\n            tf.truncated_normal(\n                [len(char2idx), dim_char], stddev = 1.0 / np.sqrt(dim_char)\n            )\n        )\n\n        word_embedded = tf.nn.embedding_lookup(\n            self.word_embeddings, self.word_ids\n        )\n        char_embedded = tf.nn.embedding_lookup(\n            self.char_embeddings, self.char_ids\n        )\n        s = tf.shape(char_embedded)\n        char_embedded = tf.reshape(\n            char_embedded, shape = [s[0] * s[1], s[-2], dim_char]\n        )\n        \n        for n in range(num_layers):\n            (out_fw, out_bw), (\n                state_fw,\n                state_bw,\n            ) = tf.nn.bidirectional_dynamic_rnn(\n                cell_fw = cells(hidden_size_char),\n                cell_bw = cells(hidden_size_char),\n                inputs = char_embedded,\n                dtype = tf.float32,\n                scope = 'bidirectional_rnn_char_%d' % (n),\n            )\n            char_embedded = tf.concat((out_fw, out_bw), 2)\n        output = tf.reshape(\n            char_embedded[:, -1], shape = [s[0], s[1], 2 * hidden_size_char]\n        )\n        word_embedded = tf.concat([word_embedded, output], axis = -1)\n\n        for n in range(num_layers):\n            (out_fw, out_bw), (\n                state_fw,\n                state_bw,\n            ) = tf.nn.bidirectional_dynamic_rnn(\n                cell_fw = luong(word_embedded, hidden_size_word),\n                cell_bw = luong(word_embedded, hidden_size_word),\n                inputs = word_embedded,\n                dtype = tf.float32,\n                scope = 'bidirectional_rnn_word_%d' % (n),\n            )\n            word_embedded = tf.concat((out_fw, out_bw), 2)\n\n        logits = tf.layers.dense(word_embedded, len(idx2tag))\n        y_t = self.labels\n        log_likelihood, transition_params = tf.contrib.crf.crf_log_likelihood(\n            logits, y_t, self.lengths\n        )\n        self.cost = tf.reduce_mean(-log_likelihood)\n        self.optimizer = tf.train.AdamOptimizer(\n            learning_rate = learning_rate\n        ).minimize(self.cost)\n        mask = tf.sequence_mask(self.lengths, maxlen = self.maxlen)\n        self.tags_seq, tags_score = tf.contrib.crf.crf_decode(\n            logits, transition_params, self.lengths\n        )\n        self.tags_seq = tf.identity(self.tags_seq, name = 'logits')\n\n        y_t = tf.cast(y_t, tf.int32)\n        self.prediction = tf.boolean_mask(self.tags_seq, mask)\n        mask_label = tf.boolean_mask(y_t, mask)\n        correct_pred = tf.equal(self.prediction, mask_label)\n        correct_index = tf.cast(correct_pred, tf.float32)\n        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.reset_default_graph()\nsess = tf.InteractiveSession()\n\ndim_word = 128\ndim_char = 256\ndropout = 0.8\nlearning_rate = 1e-3\nhidden_size_char = 128\nhidden_size_word = 128\nnum_layers = 2\nbatch_size = 64\n\nmodel = Model(dim_word,dim_char,dropout,learning_rate,hidden_size_char,hidden_size_word,num_layers)\nsess.run(tf.global_variables_initializer())","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"string = 'KUALA LUMPUR: Sempena sambutan Aidilfitri minggu depan, Perdana Menteri Tun Dr Mahathir Mohamad dan Menteri Pengangkutan Anthony Loke Siew Fook menitipkan pesanan khas kepada orang ramai yang mahu pulang ke kampung halaman masing-masing. Dalam video pendek terbitan Jabatan Keselamatan Jalan Raya (JKJR) itu, Dr Mahathir menasihati mereka supaya berhenti berehat dan tidur sebentar  sekiranya mengantuk ketika memandu.'\n\nimport re\n\ndef entities_textcleaning(string, lowering = False):\n    \"\"\"\n    use by entities recognition, pos recognition and dependency parsing\n    \"\"\"\n    string = re.sub('[^A-Za-z0-9\\-\\/() ]+', ' ', string)\n    string = re.sub(r'[ ]+', ' ', string).strip()\n    original_string = string.split()\n    if lowering:\n        string = string.lower()\n    string = [\n        (original_string[no], word.title() if word.isupper() else word)\n        for no, word in enumerate(string.split())\n        if len(word)\n    ]\n    return [s[0] for s in string], [s[1] for s in string]\n\ndef char_str_idx(corpus, dic, UNK = 0):\n    maxlen = max([len(i) for i in corpus])\n    X = np.zeros((len(corpus), maxlen))\n    for i in range(len(corpus)):\n        for no, k in enumerate(corpus[i][:maxlen][::-1]):\n            val = dic[k] if k in dic else UNK\n            X[i, -1 - no] = val\n    return X","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nimport time\n\nEARLY_STOPPING, CURRENT_CHECKPOINT, CURRENT_ACC, EPOCH = 2, 0, 0, 0\n\nwhile True:\n    lasttime = time.time()\n    if CURRENT_CHECKPOINT == EARLY_STOPPING:\n        print('break epoch:%d\\n' % (EPOCH))\n        break\n\n    lasttime = time.time()\n    train_acc, train_loss, test_acc, test_loss = 0, 0, 0, 0\n    pbar = tqdm(\n        range(0, train_X.shape[0], batch_size), desc = 'train minibatch loop'\n    )\n    for i in pbar:\n        index = min(i + batch_size, train_X.shape[0])\n        batch_x = train_X[i : index]\n        batch_char = generate_char_seq(batch_x)\n        batch_y = train_Y[i : index]\n        acc, cost, _ = sess.run(\n            [model.accuracy, model.cost, model.optimizer],\n            feed_dict = {\n                model.word_ids: batch_x,\n                model.char_ids: batch_char,\n                model.labels: batch_y\n            },\n        )\n        assert not np.isnan(cost)\n        train_loss += cost\n        train_acc += acc\n        pbar.set_postfix(cost = cost, accuracy = acc)\n        \n    pbar = tqdm(\n        range(0, test_X.shape[0], batch_size), desc = 'test minibatch loop'\n    )\n    for i in pbar:\n        index = min(i + batch_size, test_X.shape[0])\n        batch_x = test_X[i : index]\n        batch_char = generate_char_seq(batch_x)\n        batch_y = test_Y[i : index]\n        acc, cost = sess.run(\n            [model.accuracy, model.cost],\n            feed_dict = {\n                model.word_ids: batch_x,\n                model.char_ids: batch_char,\n                model.labels: batch_y\n            },\n        )\n        assert not np.isnan(cost)\n        test_loss += cost\n        test_acc += acc\n        pbar.set_postfix(cost = cost, accuracy = acc)\n    \n    train_loss /= len(train_X) / batch_size\n    train_acc /= len(train_X) / batch_size\n    test_loss /= len(test_X) / batch_size\n    test_acc /= len(test_X) / batch_size\n\n    print('time taken:', time.time() - lasttime)\n    print(\n        'epoch: %d, training loss: %f, training acc: %f, valid loss: %f, valid acc: %f\\n'\n        % (EPOCH, train_loss, train_acc, test_loss, test_acc)\n    )\n    \n    sequence = entities_textcleaning(string)[1]\n    X_seq = char_str_idx([sequence], word2idx, 2)\n    X_char_seq = generate_char_seq(X_seq)\n\n    predicted = sess.run(model.tags_seq,\n                feed_dict = {\n                    model.word_ids: X_seq,\n                    model.char_ids: X_char_seq,\n                },\n        )[0]\n\n    for i in range(len(predicted)):\n        print(sequence[i],idx2tag[predicted[i]])\n        \n    if test_acc > CURRENT_ACC:\n        print(\n            'epoch: %d, pass acc: %f, current acc: %f'\n            % (EPOCH, CURRENT_ACC, test_acc)\n        )\n        CURRENT_ACC = test_acc\n        CURRENT_CHECKPOINT = 0\n    else:\n        CURRENT_CHECKPOINT += 1\n    EPOCH += 1","execution_count":14,"outputs":[{"output_type":"stream","text":"train minibatch loop: 100%|██████████| 1524/1524 [12:37<00:00,  2.05it/s, accuracy=0.908, cost=16.2]\ntest minibatch loop: 100%|██████████| 381/381 [01:36<00:00,  3.88it/s, accuracy=0.937, cost=11.7]\n","name":"stderr"},{"output_type":"stream","text":"time taken: 854.3211750984192\nepoch: 0, training loss: 30.581068, training acc: 0.815311, valid loss: 15.356339, valid acc: 0.916838\n\n","name":"stdout"},{"output_type":"stream","text":"\rtrain minibatch loop:   0%|          | 0/1524 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"Kuala PROPN\nLumpur PROPN\nSempena PROPN\nsambutan NOUN\nAidilfitri PROPN\nminggu VERB\ndepan ADJ\nPerdana PROPN\nMenteri PROPN\nTun PROPN\nDr PROPN\nMahathir PROPN\nMohamad PROPN\ndan CCONJ\nMenteri PROPN\nPengangkutan PROPN\nAnthony PROPN\nLoke PROPN\nSiew PROPN\nFook PROPN\nmenitipkan PROPN\npesanan PROPN\nkhas NOUN\nkepada ADP\norang NOUN\nramai NOUN\nyang PRON\nmahu ADJ\npulang VERB\nke ADP\nkampung NOUN\nhalaman NOUN\nmasing-masing PROPN\nDalam ADP\nvideo NOUN\npendek ADJ\nterbitan NOUN\nJabatan NOUN\nKeselamatan PROPN\nJalan PROPN\nRaya PROPN\n(Jkjr) PROPN\nitu DET\nDr PROPN\nMahathir PROPN\nmenasihati PROPN\nmereka PRON\nsupaya SCONJ\nberhenti VERB\nberehat PROPN\ndan CCONJ\ntidur NOUN\nsebentar ADV\nsekiranya PROPN\nmengantuk PROPN\nketika SCONJ\nmemandu VERB\nepoch: 0, pass acc: 0.000000, current acc: 0.916838\n","name":"stdout"},{"output_type":"stream","text":"train minibatch loop: 100%|██████████| 1524/1524 [12:23<00:00,  2.08it/s, accuracy=0.952, cost=8.15]\ntest minibatch loop: 100%|██████████| 381/381 [01:35<00:00,  4.09it/s, accuracy=0.939, cost=9.44]\n","name":"stderr"},{"output_type":"stream","text":"time taken: 839.4308767318726\nepoch: 1, training loss: 10.645715, training acc: 0.939483, valid loss: 15.366538, valid acc: 0.919690\n\n","name":"stdout"},{"output_type":"stream","text":"\rtrain minibatch loop:   0%|          | 0/1524 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"Kuala PROPN\nLumpur PROPN\nSempena PROPN\nsambutan NOUN\nAidilfitri PROPN\nminggu NOUN\ndepan ADJ\nPerdana PROPN\nMenteri PROPN\nTun PROPN\nDr PROPN\nMahathir PROPN\nMohamad PROPN\ndan CCONJ\nMenteri PROPN\nPengangkutan PROPN\nAnthony PROPN\nLoke PROPN\nSiew PROPN\nFook PROPN\nmenitipkan PROPN\npesanan PROPN\nkhas VERB\nkepada ADP\norang NOUN\nramai NOUN\nyang PRON\nmahu ADV\npulang VERB\nke ADP\nkampung NOUN\nhalaman NOUN\nmasing-masing PROPN\nDalam ADP\nvideo NOUN\npendek ADJ\nterbitan NOUN\nJabatan NOUN\nKeselamatan NOUN\nJalan PROPN\nRaya PROPN\n(Jkjr) PROPN\nitu DET\nDr PROPN\nMahathir PROPN\nmenasihati PROPN\nmereka PRON\nsupaya SCONJ\nberhenti VERB\nberehat PROPN\ndan CCONJ\ntidur NOUN\nsebentar ADV\nsekiranya PROPN\nmengantuk PROPN\nketika SCONJ\nmemandu VERB\nepoch: 1, pass acc: 0.916838, current acc: 0.919690\n","name":"stdout"},{"output_type":"stream","text":"train minibatch loop: 100%|██████████| 1524/1524 [12:26<00:00,  2.03it/s, accuracy=0.962, cost=7.43] \ntest minibatch loop: 100%|██████████| 381/381 [01:35<00:00,  4.07it/s, accuracy=0.924, cost=9.78]\n","name":"stderr"},{"output_type":"stream","text":"time taken: 842.8556673526764\nepoch: 2, training loss: 7.352831, training acc: 0.957632, valid loss: 17.453034, valid acc: 0.914130\n\n","name":"stdout"},{"output_type":"stream","text":"\rtrain minibatch loop:   0%|          | 0/1524 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"Kuala PROPN\nLumpur PROPN\nSempena PROPN\nsambutan NOUN\nAidilfitri PROPN\nminggu ADP\ndepan NOUN\nPerdana PROPN\nMenteri PROPN\nTun PROPN\nDr PROPN\nMahathir PROPN\nMohamad PROPN\ndan CCONJ\nMenteri PROPN\nPengangkutan PROPN\nAnthony PROPN\nLoke PROPN\nSiew PROPN\nFook PROPN\nmenitipkan PROPN\npesanan PROPN\nkhas VERB\nkepada ADP\norang NOUN\nramai NOUN\nyang PRON\nmahu ADV\npulang VERB\nke ADP\nkampung NOUN\nhalaman NOUN\nmasing-masing PROPN\nDalam ADP\nvideo NOUN\npendek ADJ\nterbitan NOUN\nJabatan NOUN\nKeselamatan NOUN\nJalan PROPN\nRaya PROPN\n(Jkjr) PROPN\nitu DET\nDr PROPN\nMahathir PROPN\nmenasihati PROPN\nmereka PRON\nsupaya SCONJ\nberhenti VERB\nberehat PROPN\ndan CCONJ\ntidur NOUN\nsebentar ADV\nsekiranya PROPN\nmengantuk PROPN\nketika SCONJ\nmemandu VERB\n","name":"stdout"},{"output_type":"stream","text":"train minibatch loop: 100%|██████████| 1524/1524 [12:32<00:00,  2.05it/s, accuracy=0.976, cost=2.71] \ntest minibatch loop: 100%|██████████| 381/381 [01:36<00:00,  4.06it/s, accuracy=0.911, cost=18.5]\n","name":"stderr"},{"output_type":"stream","text":"time taken: 848.7655079364777\nepoch: 3, training loss: 5.340113, training acc: 0.968100, valid loss: 20.376155, valid acc: 0.903500\n\nKuala PROPN\nLumpur PROPN\nSempena PROPN\nsambutan NOUN\nAidilfitri PROPN\nminggu ADV\ndepan ADJ\nPerdana PROPN\nMenteri PROPN\nTun PROPN\nDr PROPN\nMahathir PROPN\nMohamad PROPN\ndan CCONJ\nMenteri PROPN\nPengangkutan PROPN\nAnthony PROPN\nLoke PROPN\nSiew PROPN\nFook PROPN\nmenitipkan PROPN\npesanan PROPN\nkhas ADV\nkepada ADP\norang NOUN\nramai NOUN\nyang PRON\nmahu ADV\npulang VERB\nke ADP\nkampung NOUN\nhalaman NOUN\nmasing-masing PROPN\nDalam ADP\nvideo NOUN\npendek ADJ\nterbitan NOUN\nJabatan NOUN\nKeselamatan NOUN\nJalan PROPN\nRaya PROPN\n(Jkjr) PROPN\nitu DET\nDr PROPN\nMahathir PROPN\nmenasihati PROPN\nmereka PRON\nsupaya SCONJ\nberhenti VERB\nberehat PROPN\ndan CCONJ\ntidur NOUN\nsebentar ADV\nsekiranya PROPN\nmengantuk PROPN\nketika SCONJ\nmemandu VERB\nbreak epoch:4\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequence = entities_textcleaning('Mahathir suka Akta 19977')[1]\nX_seq = char_str_idx([sequence], word2idx, 2)\nX_char_seq = generate_char_seq(X_seq)\n\npredicted = sess.run(model.tags_seq,\n            feed_dict = {\n                model.word_ids: X_seq,\n                model.char_ids: X_char_seq,\n            },\n    )[0]\n\nfor i in range(len(predicted)):\n    print(sequence[i],idx2tag[predicted[i]])","execution_count":15,"outputs":[{"output_type":"stream","text":"Mahathir ADV\nsuka VERB\nAkta PROPN\n19977 PROPN\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pred2label(pred):\n    out = []\n    for pred_i in pred:\n        out_i = []\n        for p in pred_i:\n            out_i.append(idx2tag[p])\n        out.append(out_i)\n    return out","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"real_Y, predict_Y = [], []\n\npbar = tqdm(\n    range(0, len(test_X), batch_size), desc = 'validation minibatch loop'\n)\nfor i in pbar:\n    batch_x = test_X[i : min(i + batch_size, test_X.shape[0])]\n    batch_char = generate_char_seq(batch_x)\n    batch_y = test_Y[i : min(i + batch_size, test_X.shape[0])]\n    predicted = pred2label(sess.run(model.tags_seq,\n            feed_dict = {\n                model.word_ids: batch_x,\n                model.char_ids: batch_char,\n            },\n    ))\n    real = pred2label(batch_y)\n    predict_Y.extend(predicted)\n    real_Y.extend(real)","execution_count":17,"outputs":[{"output_type":"stream","text":"validation minibatch loop: 100%|██████████| 381/381 [01:32<00:00,  4.24it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(np.array(real_Y).ravel(), np.array(predict_Y).ravel(),\n                           digits = 6))","execution_count":18,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n         ADJ   0.831224  0.617002  0.708269     45666\n         ADP   0.968751  0.943582  0.956001    119589\n         ADV   0.797609  0.804690  0.801134     47760\n         AUX   0.990094  0.999500  0.994775     10000\n       CCONJ   0.961630  0.924377  0.942635     37171\n         DET   0.928473  0.923119  0.925788     38839\n        NOUN   0.808552  0.923039  0.862010    268329\n         NUM   0.959908  0.846473  0.899629     41211\n        PART   0.875569  0.873818  0.874693      5500\n        PRON   0.954275  0.938896  0.946523     48835\n       PROPN   0.935033  0.833543  0.881376    227608\n       PUNCT   0.998556  0.998425  0.998490    182824\n       SCONJ   0.608801  0.862046  0.713622     15150\n         SYM   0.976771  0.946111  0.961197      3600\n        VERB   0.908930  0.926011  0.917391    124518\n           X   0.000000  0.000000  0.000000       150\n\n    accuracy                       0.901504   1216750\n   macro avg   0.844011  0.835039  0.836471   1216750\nweighted avg   0.906385  0.901504  0.901539   1216750\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"saver = tf.train.Saver(tf.trainable_variables())\nsaver.save(sess, 'luong/model.ckpt')\n\nstrings = ','.join(\n    [\n        n.name\n        for n in tf.get_default_graph().as_graph_def().node\n        if ('Variable' in n.op\n        or 'Placeholder' in n.name\n        or 'logits' in n.name\n        or 'alphas' in n.name)\n        and 'Adam' not in n.name\n        and 'beta' not in n.name\n        and 'OptimizeLoss' not in n.name\n        and 'Global_Step' not in n.name\n    ]\n)\nstrings.split(',')","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"['Placeholder',\n 'Placeholder_1',\n 'Placeholder_2',\n 'Variable',\n 'Variable_1',\n 'bidirectional_rnn_char_0/fw/lstm_cell/kernel',\n 'bidirectional_rnn_char_0/fw/lstm_cell/bias',\n 'bidirectional_rnn_char_0/bw/lstm_cell/kernel',\n 'bidirectional_rnn_char_0/bw/lstm_cell/bias',\n 'bidirectional_rnn_char_1/fw/lstm_cell/kernel',\n 'bidirectional_rnn_char_1/fw/lstm_cell/bias',\n 'bidirectional_rnn_char_1/bw/lstm_cell/kernel',\n 'bidirectional_rnn_char_1/bw/lstm_cell/bias',\n 'memory_layer/kernel',\n 'memory_layer_1/kernel',\n 'bidirectional_rnn_word_0/fw/attention_wrapper/lstm_cell/kernel',\n 'bidirectional_rnn_word_0/fw/attention_wrapper/lstm_cell/bias',\n 'bidirectional_rnn_word_0/fw/attention_wrapper/attention_layer/kernel',\n 'bidirectional_rnn_word_0/bw/attention_wrapper/lstm_cell/kernel',\n 'bidirectional_rnn_word_0/bw/attention_wrapper/lstm_cell/bias',\n 'bidirectional_rnn_word_0/bw/attention_wrapper/attention_layer/kernel',\n 'memory_layer_2/kernel',\n 'memory_layer_3/kernel',\n 'bidirectional_rnn_word_1/fw/attention_wrapper/lstm_cell/kernel',\n 'bidirectional_rnn_word_1/fw/attention_wrapper/lstm_cell/bias',\n 'bidirectional_rnn_word_1/fw/attention_wrapper/attention_layer/kernel',\n 'bidirectional_rnn_word_1/bw/attention_wrapper/lstm_cell/kernel',\n 'bidirectional_rnn_word_1/bw/attention_wrapper/lstm_cell/bias',\n 'bidirectional_rnn_word_1/bw/attention_wrapper/attention_layer/kernel',\n 'dense/kernel',\n 'dense/bias',\n 'transitions',\n 'logits']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def freeze_graph(model_dir, output_node_names):\n\n    if not tf.gfile.Exists(model_dir):\n        raise AssertionError(\n            \"Export directory doesn't exists. Please specify an export \"\n            'directory: %s' % model_dir\n        )\n\n    checkpoint = tf.train.get_checkpoint_state(model_dir)\n    input_checkpoint = checkpoint.model_checkpoint_path\n\n    absolute_model_dir = '/'.join(input_checkpoint.split('/')[:-1])\n    output_graph = absolute_model_dir + '/frozen_model.pb'\n    clear_devices = True\n    with tf.Session(graph = tf.Graph()) as sess:\n        saver = tf.train.import_meta_graph(\n            input_checkpoint + '.meta', clear_devices = clear_devices\n        )\n        saver.restore(sess, input_checkpoint)\n        output_graph_def = tf.graph_util.convert_variables_to_constants(\n            sess,\n            tf.get_default_graph().as_graph_def(),\n            output_node_names.split(','),\n        )\n        with tf.gfile.GFile(output_graph, 'wb') as f:\n            f.write(output_graph_def.SerializeToString())\n        print('%d ops in the final graph.' % len(output_graph_def.node))\n        \ndef load_graph(frozen_graph_filename):\n    with tf.gfile.GFile(frozen_graph_filename, 'rb') as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n    with tf.Graph().as_default() as graph:\n        tf.import_graph_def(graph_def)\n    return graph\n","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freeze_graph('luong', strings)","execution_count":21,"outputs":[{"output_type":"stream","text":"1888 ops in the final graph.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import boto3\n\nbucketName = 'huseinhouse-storage'\nKey = 'luong/frozen_model.pb'\noutPutname = \"v27/pos/luong-pos.pb\"\n\ns3 = boto3.client('s3',\n                 aws_access_key_id='',\n                 aws_secret_access_key='')\ns3.upload_file(Key,bucketName,outPutname)","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}